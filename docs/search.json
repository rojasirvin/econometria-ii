[
  {
    "objectID": "tareas/tarea-4.html",
    "href": "tareas/tarea-4.html",
    "title": "Tarea 4",
    "section": "",
    "text": "Fecha de entrega: viernes 29 de noviembre a las 20:00 en Teams\nLa tarea deberá entregarse en Teams. Deberá incluir dos documentos:\nUn primer documento de respuestas donde se incluyan las respuestas a las preguntas teóricas y conceptuales. Este documento debe estar en formato pdf y debe ser generado usando un software de procesamiento de textos científicos, por ejemplo, usando los lenguajes LaTeX o Markdown. En este documento también se deben incluir las respuestas a preguntas sobre conclusiones que se desprenden de las secciones prácticas. Por ejemplo, si una pregunta pide obtener la media de la variable x en cierta base de datos, entonces el documento de respuestas debe incluir la pregunta y respuesta correspondiente: “la media de la variable x es 32.6”. En este documento también deberán incluirse las tablas y gráficas que se soliciten.\nUn segundo archivo deberá contener el código replicable usado para generar los resultados de la sección práctica. El código debe también crear las tablas y gráficas solicitadas. Los archivos de código se verificarán para comprobar su replicabilidad."
  },
  {
    "objectID": "tareas/tarea-4.html#preguntas",
    "href": "tareas/tarea-4.html#preguntas",
    "title": "Tarea 4",
    "section": "",
    "text": "Fecha de entrega: viernes 29 de noviembre a las 20:00 en Teams\nLa tarea deberá entregarse en Teams. Deberá incluir dos documentos:\nUn primer documento de respuestas donde se incluyan las respuestas a las preguntas teóricas y conceptuales. Este documento debe estar en formato pdf y debe ser generado usando un software de procesamiento de textos científicos, por ejemplo, usando los lenguajes LaTeX o Markdown. En este documento también se deben incluir las respuestas a preguntas sobre conclusiones que se desprenden de las secciones prácticas. Por ejemplo, si una pregunta pide obtener la media de la variable x en cierta base de datos, entonces el documento de respuestas debe incluir la pregunta y respuesta correspondiente: “la media de la variable x es 32.6”. En este documento también deberán incluirse las tablas y gráficas que se soliciten.\nUn segundo archivo deberá contener el código replicable usado para generar los resultados de la sección práctica. El código debe también crear las tablas y gráficas solicitadas. Los archivos de código se verificarán para comprobar su replicabilidad."
  },
  {
    "objectID": "tareas/tarea-4.html#pregunta-1",
    "href": "tareas/tarea-4.html#pregunta-1",
    "title": "Tarea 4",
    "section": "Pregunta 1",
    "text": "Pregunta 1\nConsidere los datos en el archivo capital_trabajo.csv. Con una función de producción Cobb-Douglas las participaciones del capital y el trabajo en el valor de la producción se pueden estimar usando una regresión lineal. En algunas aplicaciones es de interés conocer el cociente de las participaciones estimadas.\n\n[10 puntos] Usando 500 repeticiones bootstrap estime el error estándar del cociente capital-trabajo. Para ello realice el siguiente procedimiento:\n\nGenere una matriz vacía de 500 filas para coleccionar sus relaciones estimadas.\nEn cada una de las repeticiones obtenga una muestra con remplazo a partir de la muestra original.\nEstime por MCO los coeficientes sobre el log del capital y el log del trabajo. La variable dependiente es el log del valor de la producción. Calcule el cociente de los coeficientes estimados. Guarde el cociente en la matriz.\nRepita ii. y iii. 500 veces.\nCalcule la desviación estándar de los cocientes estimados.\n\n[10 puntos] Calcule ahora el error estándar jackknife, para lo que realizará \\(N\\) estimaciones de la ecuación del valor de la producción y en cada una de ellas calculará el cociente de interés. En cada una de las \\(i=1,\\ldots,N\\) repeticiones, eliminará de la muestra la observación \\(i\\), por lo que cada regresión será estimada con \\(N-1\\) observaciones. Obtenga la desviación estándar de los \\(N\\) cocientes estimados.\n[10 puntos] Compruebe que sus cálculos aproximan el error estándar obtenido con el Método Delta. Para ello, después de estimar la ecuación del valor de la producción con la muestra original, use la función deltaMethod del paquete car."
  },
  {
    "objectID": "tareas/tarea-4.html#pregunta-2",
    "href": "tareas/tarea-4.html#pregunta-2",
    "title": "Tarea 4",
    "section": "Pregunta 2",
    "text": "Pregunta 2\nConsidere los datos en MunichRent.rda. Estos archivos contienen información sobre rentas en la ciudad de Munich, rent. Se desea explicar la renta en función de la antiguedad de los edificios en renta, controlando por el área, area. La variable yearc indica cuándo fue construido el edificio. Construya la antiguedad como antiguedad=2023-yearc. Para leer los datos basta con ejecutar load(“MunichRent.rda”).\n\n[10 puntos] Estime por MCO la relación entre la renta, rent y la antiguedad del edificio, controlando por area y efectos fijos de bath y kitchen. Interprete el coeficiente sobre la antiguedad.\n[10 puntos] Estime la misma relación que en la parte a., pero con una regresión mediana. Interprete el coeficiente sobre la antiguedad.\n[10 puntos] Estime ahora una regresión cuantil para cada uno de los deciles de la distribución condicional de la renta y represente en una gráfica los coeficientes por regresión cuantil junto con el coeficiente de MCO para las variables del área y la antiguedad. ¿Concluye que vale la pena modelar la relación de las rentas en función del área y la antiguedad usando regresión cuantil?\n[10 puntos] Suponga que no está dispuesto a imponer una relación lineal entre la antiguedad y la renta. Considere entonces el siguiente modelo:\n\\[rent_i=\\beta_0+\\beta_1 area + \\lambda(antiguedad_i)+\\varepsilon_i\\]\nUse el estimador de Robinson (1988) para estimar este modelo parcialmente lineal. Grafique sus resultados e interprételos."
  },
  {
    "objectID": "tareas/tarea-4.html#pregunta-3",
    "href": "tareas/tarea-4.html#pregunta-3",
    "title": "Tarea 4",
    "section": "Pregunta 3",
    "text": "Pregunta 3\nStevenson & Wolfers (2006) estudian los efectos de la introducción de leyes que permiten el divorcio unilateral. La librería bacondecomp incluye los datos usados en dicho artículo. Usaremos los datos de 1964 a 1996 para mostrar cómo impactan las leyes de divorcio express (unilateral) a la tasa de suicidios en mujeres. Comience llamando los datos:\n\nwd &lt;- divorce %&gt;% \n  filter(year&gt;1964 & year&lt;1996 & sex==2) %&gt;% \n  mutate(suicide_rate=suicide*1000000/(stpop*fshare),\n         year=as.numeric(year))\n\n\n[5 puntos] Estime el efecto de diferencia en diferencias asumiendo tendencias paralelas y el estimador de two-way fixed effects. Obtenga los errores estándar primero asumiendo errores clásicos y luego usando una matriz agrupada a nivel estado.\n[5 puntos] Realice la descomposición de Goodman-Bacon (2021). Construya un gráfico donde muestre en el eje el peso otorgado a cada comparación 2x2 que el estimador de TWFE realiza mecánicamente y en el eje el efecto estimado correspondiente a cada comparación. Interprete el gráfico obtenido.\n[10 puntos] Implemente el estimador de Callaway & Sant’Anna (2021) para estimar los efectos del tratamiento específicos para cada cohorte, usando el paquete did. Utilice como grupo de comparación los estados que nunca son tratados. La columna stid es un identificador numérico de los estados (lo requerirá cuando use att_gt del paquete did).\n[10 puntos] Reporte los resultados agregados obtenidos a partir del estimador Callaway & Sant’Anna (2021), usando una agregación dinámica que muestre los efectos promedio para cada periodo antes y después del tratamiento. Grafique e interprete los resultados."
  },
  {
    "objectID": "tareas/tarea-3.html",
    "href": "tareas/tarea-3.html",
    "title": "Tarea 3",
    "section": "",
    "text": "Fecha de entrega: viernes 8 de noviembre a las 20:00 en Teams\nLa tarea deberá entregarse en Teams. Deberá incluir dos documentos:\nUn primer documento de respuestas donde se incluyan las respuestas a las preguntas teóricas y conceptuales. Este documento debe estar en formato pdf y debe ser generado usando un software de procesamiento de textos científicos, por ejemplo, usando los lenguajes LaTeX o Markdown. En este documento también se deben incluir las respuestas a preguntas sobre conclusiones que se desprenden de las secciones prácticas. Por ejemplo, si una pregunta pide obtener la media de la variable x en cierta base de datos, entonces el documento de respuestas debe incluir la pregunta y respuesta correspondiente: “la media de la variable x es 32.6”. En este documento también deberán incluirse las tablas y gráficas que se soliciten.\nUn segundo archivo deberá contener el código replicable usado para generar los resultados de la sección práctica. El código debe también crear las tablas y gráficas solicitadas. Los archivos de código se verificarán para comprobar su replicabilidad."
  },
  {
    "objectID": "tareas/tarea-3.html#preguntas",
    "href": "tareas/tarea-3.html#preguntas",
    "title": "Tarea 3",
    "section": "",
    "text": "Fecha de entrega: viernes 8 de noviembre a las 20:00 en Teams\nLa tarea deberá entregarse en Teams. Deberá incluir dos documentos:\nUn primer documento de respuestas donde se incluyan las respuestas a las preguntas teóricas y conceptuales. Este documento debe estar en formato pdf y debe ser generado usando un software de procesamiento de textos científicos, por ejemplo, usando los lenguajes LaTeX o Markdown. En este documento también se deben incluir las respuestas a preguntas sobre conclusiones que se desprenden de las secciones prácticas. Por ejemplo, si una pregunta pide obtener la media de la variable x en cierta base de datos, entonces el documento de respuestas debe incluir la pregunta y respuesta correspondiente: “la media de la variable x es 32.6”. En este documento también deberán incluirse las tablas y gráficas que se soliciten.\nUn segundo archivo deberá contener el código replicable usado para generar los resultados de la sección práctica. El código debe también crear las tablas y gráficas solicitadas. Los archivos de código se verificarán para comprobar su replicabilidad."
  },
  {
    "objectID": "tareas/tarea-3.html#datos",
    "href": "tareas/tarea-3.html#datos",
    "title": "Tarea 3",
    "section": "Datos",
    "text": "Datos\ncomportamiento_wide.csv\nmlbook1.csv"
  },
  {
    "objectID": "tareas/tarea-3.html#pregunta-1",
    "href": "tareas/tarea-3.html#pregunta-1",
    "title": "Tarea 3",
    "section": "Pregunta 1",
    "text": "Pregunta 1\nEn este ejercicio usaremos los datos de Blackburn & Neumark (1992), que pueden accederse instalando el paquete wooldridge y luego llamando el objeto wooldridge::wage2. La pregunta de investigación de este trabajo es la misma que la del estudio de Card (1993) visto en clase, pero con otra estrategia de identificación. El propósito de este ejercicio es estimar los retornos a la educación.\n\n[3 puntos] Estime una regresión por MCO para explicar el log del salario (lwage) en función de la educación educ y los siguientes controles: experiencia (exper), tiempo en el empleo actual (tenure), edad (age), casado (married), número de hermanos (sibs) y urbano (urban). Reporte errores clásicos y errores robustos. ¿Qué problema encuentra en la estimación de esta relación? ¿El coeficiente sobre educ tiene una interpretación causal del efecto de la educación en el salario?\n[3 puntos] Se propone usar como instrumento de los años de educación a la educación del padre. ¿Qué condiciones debe cumplir la variable propuesta para funcionar como instrumento válido?\n[4 puntos] ¿Cómo juzga la propuesta de usar la variable antes descrita como instrumento?\n[3 puntos] Estime la relación entre el logaritmo del salario y la educación usando la educación del padre, feduc, como instrumento. Emplee las mismas variables de control que en el modelo de MCO. Reporte errores clásicos y errores robustos.\n[4 puntos] Interprete la primera etapa en términos del coeficiente sobre el instrumento. Obtenga el estadístico \\(F\\) del instrumento excluido e interprete su magnitud.\n[3 puntos] Interprete el coeficiente sobre la variable de educación en el modelo estructural. Compare la magnitud del efecto estimado con el resultado de MCO.\n[3 puntos] Realice ahora el siguiente procedimiento. Primero, estime la primera etapa usando una regresión por MCO. Obtenga los valores ajustados de educación y llámelos educ_hat. Luego, estime la segunda etapa empleando educ_hat como variable independiente, además del resto de variables de control. ¿Cómo cambian sus resultados en comparación con la parte d.?\n[3 puntos] ¿A qué se deben las discrepancias que encuentra? ¿Cuál de las dos estrategias prefiere para estimar el modelo de variables instrumentales?\n[3 puntos] Reestime el modelo de variables instrumentales añadiendo un segundo instrumento, la educación de la madre, meduc, y reporte errores robustos (no es necesario usar gmm, puede seguir con ivreg, por lo que no estaría obteniendo el estimador de MGM óptimo). ¿Cómo cambian sus resultados para la ecuación estructural con respecto al caso exactamente identificado?\n[3 puntos] Con el objeto que resulta de la estimación del modelo sobreidentificado, realice summary(OBJETO, vcov = sandwich, diagnostics = TRUE) para obtener las tres pruebas diagnóstico más usadas en variables instrumentales: prueba de instrumentos débiles, prueba de Hausman y prueba de Sargan. Interprete cada una de las pruebas.\n[4 puntos] Considere la primera etapa del modelo sobreidentificado. Compruebe que si realiza una prueba de significancia conjunta para los instrumentos obtiene la prueba de instrumentos débiles que se reporta en el resumen que obtuvo con summary.\n[4 puntos] Compruebe que si realiza el procedimiento de regresión auxiliar para la prueba de Hausman obtiene el mismo valor \\(p\\) que se reporta en el resumen que obtuvo con summary."
  },
  {
    "objectID": "tareas/tarea-3.html#pregunta-2",
    "href": "tareas/tarea-3.html#pregunta-2",
    "title": "Tarea 3",
    "section": "Pregunta 2",
    "text": "Pregunta 2\nConsidere los datos comportamiento_wide.csv, que contienen información individual de niñas y niños, incluyendo su género, edad, raza e información de sus madres. Además, se incluye una medida auto reportada de autoestima (self) y una evaluación de comportamiento antisocial (anti). Se quiere conocer cómo influye la autoestima en el comportamiento antisocial. Para cada niño o niña hay tres observaciones en el tiempo. Se busca explicar el comportamiento antisocial en función de la autoestima y la condición de pobreza (pov):\n\\[anti_{it}=\\alpha_i+\\beta_1 self_{it}+\\beta_2 pov_{it}+\\varepsilon_{it}\\]\n\n[2 puntos] La base se encuentra en formato wide. Ponga la base en formato long, donde haya una columna para cada variable y donde las filas representen a un individuo en un periodo.\n[2 puntos] Estime la ecuación de comportamiento antisocial empleando MCO pooled. ¿Cuáles son los supuestos que se deben cumplir para que \\(\\hat{\\beta}_1^{MCO}\\) sea consistente?\n[3 puntos] Estime la ecuación de comportamiento antisocial empleando el estimador within. ¿Cuáles son los supuestos que se deben cumplir para que \\(\\hat{\\beta}_1^{FE}\\) sea consistente?\n[3 puntos] Estime la ecuación de comportamiento antisocial empleando efectos aleatorios. ¿Cuáles son los supuestos que se deben cumplir para que \\(\\hat{\\beta}_1^{RE}\\) sea consistente?\n[3 puntos] Se desea incorporar en el análisis el género (gender) y una variable dicotómica para los hispanos (hispanic). Indique qué modelo usaría y estime dicho modelo.\n[2 puntos] Regrese al modelo que incluye solo la autoestima y el estado de pobreza como covariables. Realice una prueba de Hausman para determinar si se prefiere un modelo de efectos fijos o uno de efectos aleatorios."
  },
  {
    "objectID": "tareas/tarea-3.html#pregunta-3",
    "href": "tareas/tarea-3.html#pregunta-3",
    "title": "Tarea 3",
    "section": "Pregunta 3",
    "text": "Pregunta 3\nRetome los datos de la pregunta 2 y el modelo del comportamiento antisocial en función de la autoestima y la pobreza. En esta pregunta mostraremos la equivalencia del estimador within con otros estimadores.\n\n[3 puntos] Compruebe que el estimador de efectos fijos es equivalente a MCO con dummies de individuos.\n[2 puntos] Compruebe que en un modelo de efectos fijos las características que no varían en el tiempo no pueden ser identificadas. Añada la variable black para comprobarlo.\n[5 puntos] Compruebe que el estimador de efectos fijos es equivalente a MCO sobre el modelo en diferencias con respecto a la media. Para esto, conserve dos periodos consecutivos de datos y solo observaciones que tengan datos para las variables dependientes e independientes en los dos años que elija. Luego estime por MCO el modelo con variables transformadas.\n[5 puntos] Compruebe que el estimador de efectos fijos es equivalente a MCO sobre el modelo en primeras diferencias. Parta de la muestra con dos años de la parte d. para estimar por MCO el modelo con variables transformadas."
  },
  {
    "objectID": "tareas/tarea-3.html#pregunta-4",
    "href": "tareas/tarea-3.html#pregunta-4",
    "title": "Tarea 3",
    "section": "Pregunta 4",
    "text": "Pregunta 4\nConsidere los datos mlbook1.csv con información sobre 2287 estudiantes en 131 escuelas. Nos interesa la relación entre una medida de aptitud verbal, (iq_vert) y el resultado de un examen de inglés (langpost). Las variables schoolnr y pupilnr identifican a las escuelas y los estudiantes, respectivamente. El modelo a estimar es el siguiente:\n\\[langpost_{i}=\\alpha+\\beta iqvert_{i}+BX_{i}+\\varepsilon_{i}\\]\ndonde \\(i\\) indexa y \\(X_i\\) son tres características usadas como control: el sexo, sex, si el estudiante es de una población minoritaria, minority y el número de años repetidos, repeatgr.\n\n[5 puntos] ¿Por qué es posible que estemos frente a una situación de errores agrupados?\n[5 puntos] Estime la ecuación de calificación usando MCO ignorando la agrupación de datos. ¿Qué concluye respecto a la relación entre la aptitud verbal y la prueba de inglés?\n[5 puntos] Estime ahora los errores robustos a heteroscedasticidad del tipo HC1. ¿Qué cambia y por qué en la interpretación de la relación entre la prueba de aptitud y el examen?\n[5 puntos] Estime la ecuación de calificación usando MCO y efectos fijos de escuela. ¿Qué resuelve este procedimiento?\n[5 puntos] Estime la ecuación de calificación usando MCO y con errores agrupados a nivel escuela (sin efectos fijos de escuela). ¿Qué resuelve este procedimiento?\n[5 puntos] Estime la ecuación de calificación usando MCO, efectos fijos de escuela y con errores agrupados a nivel escuela. ¿Qué resuelve este procedimiento?"
  },
  {
    "objectID": "tareas/tarea-2.html",
    "href": "tareas/tarea-2.html",
    "title": "Tarea 2",
    "section": "",
    "text": "Fecha de entrega: viernes 4 de octubre a las 20:00 en Teams\nLa tarea deberá entregarse en Teams. Deberá incluir dos documentos:\nUn primer documento de respuestas donde se incluyan las respuestas a las preguntas teóricas y conceptuales. Este documento debe estar en formato pdf y debe ser generado usando un software de procesamiento de textos científicos, por ejemplo, usando los lenguajes LaTeX o Markdown. En este documento también se deben incluir las respuestas a preguntas sobre conclusiones que se desprenden de las secciones prácticas. Por ejemplo, si una pregunta pide obtener la media de la variable x en cierta base de datos, entonces el documento de respuestas debe incluir la pregunta y respuesta correspondiente: “la media de la variable x es 32.6”. En este documento también deberán incluirse las tablas y gráficas que se soliciten.\nUn segundo archivo deberá contener el código replicable usado para generar los resultados de la sección práctica. El código debe también crear las tablas y gráficas solicitadas. Los archivos de código se verificarán para comprobar su replicabilidad."
  },
  {
    "objectID": "tareas/tarea-2.html#preguntas",
    "href": "tareas/tarea-2.html#preguntas",
    "title": "Tarea 2",
    "section": "",
    "text": "Fecha de entrega: viernes 4 de octubre a las 20:00 en Teams\nLa tarea deberá entregarse en Teams. Deberá incluir dos documentos:\nUn primer documento de respuestas donde se incluyan las respuestas a las preguntas teóricas y conceptuales. Este documento debe estar en formato pdf y debe ser generado usando un software de procesamiento de textos científicos, por ejemplo, usando los lenguajes LaTeX o Markdown. En este documento también se deben incluir las respuestas a preguntas sobre conclusiones que se desprenden de las secciones prácticas. Por ejemplo, si una pregunta pide obtener la media de la variable x en cierta base de datos, entonces el documento de respuestas debe incluir la pregunta y respuesta correspondiente: “la media de la variable x es 32.6”. En este documento también deberán incluirse las tablas y gráficas que se soliciten.\nUn segundo archivo deberá contener el código replicable usado para generar los resultados de la sección práctica. El código debe también crear las tablas y gráficas solicitadas. Los archivos de código se verificarán para comprobar su replicabilidad."
  },
  {
    "objectID": "tareas/tarea-2.html#datos",
    "href": "tareas/tarea-2.html#datos",
    "title": "Tarea 2",
    "section": "Datos",
    "text": "Datos\nmotral2012.csv\nphd_articulos.csv"
  },
  {
    "objectID": "tareas/tarea-2.html#pregunta-1",
    "href": "tareas/tarea-2.html#pregunta-1",
    "title": "Tarea 2",
    "section": "Pregunta 1",
    "text": "Pregunta 1\nUse los datos en el archivo motral2012.csv, que incluye una muestra de individuos con sus características socioeconómicas. Nos interesa conocer los factores que afectan la probabilidad de que los individuos tengan ahorros formales. Considere lo siguiente sobre las opciones de ahorro de los entrevistados, contenida en la variable p14:\n\np14 igual a 1 significa cuentas de ahorro bancarias\np14 igual a 2 significa cuenta de inversión bancaria\np14 igual a 3 significa inversiones en bienes raíces\np14 igual a 4 significa caja de ahorro en su trabajo\np14 igual a 5 significa caja de ahorro con sus amigos\np14 igual a 6 significa tandas\np14 igual a 7 significa que ahorra en su casa o alcancías\np14 igual a 8 significa otro lugar\np14 NA significa que no ahorra\n\n\n[2 puntos] Comience generando una variable binaria ahorra_inf que tome el valor de 1 para las personas que ahorran en instrumentos informales y 0 en otro caso. Se consideran instrumentos informales las cajas de ahorro en el trabajo o amigos, las tandas y el ahorro en casa o alcancías . Construya también la variable mujer que tome el valor de 1 cuando sex toma el valor de 2 y 0 en otro caso. Luego, estime un modelo de probabilidad lineal que relacione ahorra_inf como variable dependiente con eda (edad), anios_esc (años de escolaridad) y mujer. Reporte los errores que asumen homocedasticidad y los errores robustos a heteroscedasticidad. ¿Qué observa respecto a los errores y por qué sucede?\n[3 puntos] ¿Cuál es el efecto en la probabilidad de ahorrar informalmente si los años de educación se incrementan en una unidad, pasando de 4 a 5 años de educación?\n[2 puntos] Realice una prueba de significancia conjunta de eda y anios_esc. ¿Qué concluye?\n[3 puntos] Estime un modelo logit relacionando las mismas variables. Use la función avg_slopes del paquete marginaleffects para obtener los efectos marginales promedio de un cambio en cada uno de los regresores. ¿Por qué difiere la magnitud de este efecto marginal con respecto a la parte b.?\n[2 puntos] Ahora estime el efecto marginal en la media para eda y anios_esc y para los hombres, usando la función slopes. ¿Por qué difiere la magnitud de este efecto marginal respecto a la parte b. y la d.?\n[3 puntos] Provea una expresión para la maginitud de:\n\n\\[\\frac{\\frac{\\partial P(y=1)}{\\partial \\; anios\\_esc}}{\\frac{\\partial P(y=1)}{\\partial \\; eda}}\\]"
  },
  {
    "objectID": "tareas/tarea-2.html#pregunta-2",
    "href": "tareas/tarea-2.html#pregunta-2",
    "title": "Tarea 2",
    "section": "Pregunta 2",
    "text": "Pregunta 2\nAhora estimará un modelo multinomial empleando los mismos datos en motral2012.csv. El propósito será ahora estudiar los factores relevantes para predecir la forma de ahorro que tienen las personas que ahorran.\n\n[2 punto] Genere una variable categórica llamada ahorro que sea igual a 1 cuando p14 sea igual a 1 o 2, igual a 2 cuando p14 sea igual a 7, e igual a 3 cuando p14 sea igual a 3, 4, 5, 6 u 8. Haga que esa variable sea missing cuando p14 sea missing. Posteriormente, convierta esta nueva variable en una de factores de forma que el valor 1 tenga la etiqueta “Banco”, el valor 2 tenga la etiqueta “Casa” y el valor 3 tenga la etiqueta “Otro”.\n[4 puntos] Estime un modelo logit multinomial (regresores invariantes a la alternativa) con la opción de ahorro como variable dependiente y los mismos regresores de la pregunta 1. Hay varios paquetes para hacer esto, pero recomiendo usar la función multinom del paquete nnet. ¿Qué puede decir sobre el coeficiente de años de educación en la alternativa “Casa”?\n[4 puntos] Calcule los efectos marginales promedio sobre la probabilidad de ahorrar en el banco. Al considerar el cambio en la probabilidad para el caso de las mujeres (cuando la variable mujer pasa de 0 a 1), ¿de qué tamaño es el efecto predicho en la probabilidad de ahorrar en el banco?\n[3 puntos] Calcule los cocientes de riesgo relativo (relative risk ratios o RRR). ¿Qué significa el hecho de que el RRR asociado a ser mujer sea mayor que 1 en la alternativa “Casa”?\n[2 puntos] Estime nuevamente el modelo, pero ahora, especifique que la alternativa “Casa” sea la alternativa base. ¿Cómo es el RRR de la edad en la alternativa “Banco”? ¿Es esto congruente con lo que obtuvo en la parte d. de esta pregunta?"
  },
  {
    "objectID": "tareas/tarea-2.html#pregunta-3-modelo-poisson-inflado-en-cero",
    "href": "tareas/tarea-2.html#pregunta-3-modelo-poisson-inflado-en-cero",
    "title": "Tarea 2",
    "section": "Pregunta 3: modelo Poisson inflado en cero",
    "text": "Pregunta 3: modelo Poisson inflado en cero\nOtra manera de resolver el problema del exceso de ceros que a veces nos molesta en los modelos Poisson es usar un modelo Poisson inflado en cero (CT, p. 681). La idea es introducir un proceso binario con densidad \\(f_1(\\cdot)\\) para modelar la probabilidad de que \\(y=0\\) y luego una densidad de conteo \\(f_2(\\cdot)\\). Si el proceso binario toma el valor de 0, con probabilidad \\(f_1(0)\\), entonces \\(y=0\\), pero si el proceso binario toma el valor de 1, entonces \\(y={0,1,2,\\ldots}\\). Note que podemos entonces observar ceros por dos razones, por el proceso binomial o por el conteo.\nUn modelo inflado en cero tendrá como densidad:\n\\[\ng(y)=\n\\begin{cases}\nf_1(0)+(1-f_1(0))f_2(0) & \\text{si }y=0 \\\\\n(1-f_1(0))f_2(y)& \\text{si }y\\geq 1\n\\end{cases}\n\\] Considere la variable aleatoria \\(Y\\) con observaciones iid que sigue una distribución Poisson con parámetro \\(\\lambda\\). Y considere una variable un proceso binomial tal que \\(\\pi\\) es la probabilidad de que el conteo no se realice. Entonces:\n\\[\ng(y)=\n\\begin{cases}\n\\pi+(1-\\pi)f_2(0) & \\text{si }y=0 \\\\\n(1-\\pi)f_2(y)& \\text{si }y\\geq 1\n\\end{cases}\n\\]\n\n[4 puntos] Termine de especializar la expresión anterior unsando la distribución Poisson para \\(f_2(\\cdot)\\) para obtener la función de masa de probabilidad del modelo Poisson inflado en cero \\(g(y|\\lambda, \\pi)\\).\n[5 puntos] Provea una expresión para la función de verosimilitud \\(L(\\lambda,\\pi)=\\prod_{i=1}^N g(y_i|\\lambda, \\pi)\\). Una sugerencia para simplificar sus cálculos es definir una variable \\(X\\) igual al numero de veces que \\(Y_i\\) que toma el valor de cero.\n[3 puntos] Provea una expresión para la log verosimilitud del problema, \\(\\mathcal{L}(\\lambda,\\pi)\\).\n[3 puntos] Obtenga las condiciones de primer orden que caracterizan la solución del problema de máxima verosimilitud, derivando la log verosimilitud con respecto a \\(\\lambda\\) y a \\(\\pi\\)."
  },
  {
    "objectID": "tareas/tarea-2.html#pregunta-4",
    "href": "tareas/tarea-2.html#pregunta-4",
    "title": "Tarea 2",
    "section": "Pregunta 4",
    "text": "Pregunta 4\nUse los datos phd_articulos.csv, los cuales contienen información sobre el número de artículos publicados para una muestra de entonces estudiantes de doctorado. Nuestra variable de interés será el número de artículos art.\n\n[4 puntos] Estime un modelo Poisson que incluya variables dicotómicas para estudiantes mujeres (female) y para estudiantes casadas o casados (married), el número de hijos mejores de cinco años (kid5), el ranking de prestigio del doctorado (phd) y el número de artículos publicados por su mentor (**mentor*). Realice la estimación de la matriz de varianzas primero a partir de la varianza teórica que resulta de la igualdad de la matriz de información y luego usando una matriz de sándwich. Interprete los coeficientes estimados.\n[3 puntos] Obtenga la razón de tasas de incidencia (IRR) para los coeficientes e interprete los resultados.\n[2 puntos] Considere ahora que las mujeres han tenido carreras profesionales más cortas que los hombres, es decir, han estado menos expuestas a la ocurrencia de los eventos publicar. Incorpore esto al análisis y reinterprete los resultados. Pista: explore la opción offeset en glm de R. La columna profage mide la duración efectiva de las carreras profesionales de cada individuo.\n[2 puntos] Implemente la prueba de dispersión de Cameron y Trivedi (1990) usando una regresión auxiliar y los coeficientes estimados en la parte a. ¿Qué concluye?\n[4 puntos] Emplee ahora un modelo negativo binomial con sobredispersión cuadrática en la media para estimar la relación entre el número de artículos publicados y las variables explicativas antes enumeradas. Interprete el coeficiente asociado al número de hijos y a la variable dicotómica para estudiantes mujeres. ¿Qué puede decir sobre la significancia del \\(\\alpha\\) estimado?"
  },
  {
    "objectID": "tareas/tarea-2.html#pregunta-5",
    "href": "tareas/tarea-2.html#pregunta-5",
    "title": "Tarea 2",
    "section": "Pregunta 5",
    "text": "Pregunta 5\nRetome los datos del archivo motral2012.csv. Estimará un modelo Tobit para explicar los factores que afectan la oferta laboral femenina. En este archivo de datos la variable hrsocup registra las horas trabajadas a la semana.\n\n[2 punto] ¿Qué proporción de la muestra femenina reporta horas trabajadas iguales a cero?\n[3 puntos] Se desea estimar el efecto de los años de educación (anios_esc) sobre la oferta laboral femenina controlando por el estado marital (casada), la edad (eda) y el número de hijos (n_hij) como una variable continua. En la base, e_con toma el valor de 5 para las personas casadas. Genere la variable dummy casada que tome el valor de 1 para las mujeres casadas y cero en otro caso. Estime un modelo de MCO para hrsocup mayor que cero, usando solo la población femenina. Reporte errores robustos. ¿Cuál es la interpretación sobre el coeficiente de los años de escolaridad?\n[3 puntos] ¿Qué problema existe con el modelo planteado en el punto anterior en términos de la selección? ¿Explique si se trata de un caso de censura o de truncamiento?\n[8 puntos] Estime un modelo Tobit de datos censurados. ¿Qué resuelve el modelo Tobit en este caso? Interprete nuevamente el coeficiente sobre los años de escolaridad.\n[4 puntos] ¿Cuál es el efecto marginal de un incremento de un año de educación en la oferta laboral? ¿Cómo cambia su respuesta si, en lugar de considerar la variable latente, considera la variable censurada?"
  },
  {
    "objectID": "tareas/tarea-2.html#pregunta-6",
    "href": "tareas/tarea-2.html#pregunta-6",
    "title": "Tarea 2",
    "section": "Pregunta 6",
    "text": "Pregunta 6\nUsando los mismos datos del archivo motral2012.csv implementará un ejercicio en el mismo espíritu del famoso estudio de Mroz (1987)1 sobre la oferta laboral femenina. El propósito es estimar la relación entre el salario y el número de horas trabajadas, concentrándonos en la muestra de mujeres.\n\n[5 puntos] El primer problema al que nos enfrentamos es que el salario no se observa para las mujeres que no trabajan. Estime un modelo lineal para el log del salario por hora, ing_x_hrs, usando las variables anios_esc, eda, n_hij, el cuadrado de n_hij, busqueda y casada, usando la submuestra de mujeres con salario por hora positivo. Dichas variables representan los años de escolaridad, la edad, el número de hijos, el cuadrado del número de hijos, si la persona buscó trabajo recientemente y si la persona es casada, respectivamente. Use los coeficientes estimados para imputar el ingreso por hora, faltante para las mujeres que reportan 0 en las horas trabajadas.\n[5 puntos] Use heckit de la librería sampleSelection para estimar por máxima verosimilitud un heckit para las horas trabajadas hrsocup. En la ecuación de selección (si la persona trabaja o no) incluya como variable explicativa el salario por hora (imputado para las mujeres que no trabajan), además de anios_esc, eda, n_hij, el cuadrado de n_hij, casada y busqueda (esta última es un indicador de si se buscó trabajo en la última semana). En la ecuación de horas, incluya los mismos regresores, excepto n_hij, su cuadrado y busqueda.\n[10 puntos] Estime ahora el heckit en dos pasos, a mano. Es decir, siga los siguientes pasos: i) estime un probit para la ecuación de selección y obtenga el índice \\(x_i'\\hat{\\beta}\\); ii) calcule el inverso de la razón de Mills \\(\\lambda_i(x_i'\\hat{\\beta})\\); y iii) estime por MCO la ecuación para las horas trabajadas con la submuestra que tiene horas trabajadas positivas, incluyendo como regresor el inverso de la razón de Mills estimado y el resto de los regresores. Compare los coeficientes y los errores estándar obtenidos en esta parte con los de la parte b. ¿Por qué son iguales o por qué difieren?"
  },
  {
    "objectID": "tareas/tarea-2.html#footnotes",
    "href": "tareas/tarea-2.html#footnotes",
    "title": "Tarea 2",
    "section": "Notas",
    "text": "Notas\n\n\nMroz, T. A. (1987). The sensitivity of an empirical model of married women’s hours of work to economic and statistical assumptions. Econometrica: Journal of the econometric society, 765-799.↩︎"
  },
  {
    "objectID": "tareas/tarea-1.html",
    "href": "tareas/tarea-1.html",
    "title": "Tarea 1",
    "section": "",
    "text": "Fecha de entrega: viernes 13 de septiembre a las 20:00 en Teams\nLa tarea deberá entregarse en Teams. Deberá incluir dos documentos:\nUn primer documento de respuestas donde se incluyan las respuestas a las preguntas teóricas y conceptuales. Este documento debe estar en formato pdf y debe ser generado usando un software de procesamiento de textos científicos, por ejemplo, usando los lenguajes LaTeX o Markdown. En este documento también se deben incluir las respuestas a preguntas sobre conclusiones que se desprenden de las secciones prácticas. Por ejemplo, si una pregunta pide obtener la media de la variable x en cierta base de datos, entonces el documento de respuestas debe incluir la pregunta y respuesta correspondiente: “la media de la variable x es 32.6”. En este documento también deberán incluirse las tablas y gráficas que se soliciten.\nUn segundo archivo deberá contener el código replicable usado para generar los resultados de la sección práctica. El código debe también crear las tablas y gráficas solicitadas. Los archivos de código se verificarán para comprobar su replicabilidad."
  },
  {
    "objectID": "tareas/tarea-1.html#preguntas",
    "href": "tareas/tarea-1.html#preguntas",
    "title": "Tarea 1",
    "section": "",
    "text": "Fecha de entrega: viernes 13 de septiembre a las 20:00 en Teams\nLa tarea deberá entregarse en Teams. Deberá incluir dos documentos:\nUn primer documento de respuestas donde se incluyan las respuestas a las preguntas teóricas y conceptuales. Este documento debe estar en formato pdf y debe ser generado usando un software de procesamiento de textos científicos, por ejemplo, usando los lenguajes LaTeX o Markdown. En este documento también se deben incluir las respuestas a preguntas sobre conclusiones que se desprenden de las secciones prácticas. Por ejemplo, si una pregunta pide obtener la media de la variable x en cierta base de datos, entonces el documento de respuestas debe incluir la pregunta y respuesta correspondiente: “la media de la variable x es 32.6”. En este documento también deberán incluirse las tablas y gráficas que se soliciten.\nUn segundo archivo deberá contener el código replicable usado para generar los resultados de la sección práctica. El código debe también crear las tablas y gráficas solicitadas. Los archivos de código se verificarán para comprobar su replicabilidad."
  },
  {
    "objectID": "tareas/tarea-1.html#pregunta-1",
    "href": "tareas/tarea-1.html#pregunta-1",
    "title": "Tarea 1",
    "section": "Pregunta 1",
    "text": "Pregunta 1\nConsidere el problema de regresión no lineal en el que la variable dependiente escalar \\(y\\) tiene una media condicional \\(E(y_i)=g(x_i,\\beta)\\), siendo \\(g(\\cdot)\\) una función no lineal. Suponga que:\n\nEl proceso generador de datos es \\(y_i=g(x_i,\\beta_0)+u_i\\).\nEn el proceso generador de datos \\(E(u_i|x_i)=0\\) y \\(E(uu'|X)=\\Omega\\), donde \\(\\Omega_{0,ij}=\\sigma_{ij}\\).\nLa función \\(g(\\cdot)\\) satisface \\(g(x, \\beta^{(1)})=g(x, \\beta^{(2)})\\) si y solo si \\(\\beta^{(1)}=\\beta^{(2)}\\).\nLa matriz \\(A_0\\) existe y es finita y no singular y donde \\(A_0=p\\lim\\frac{1}{N}\\sum_{i=1}^{N}\\left.\\frac{\\partial g_i}{\\partial \\beta}\\right|_{\\beta_0}\\left.\\frac{\\partial g_i}{\\partial \\beta'}\\right|_{\\beta_0}=p\\lim\\frac{1}{N}\\left.\\frac{\\partial g'}{\\partial \\beta}\\frac{\\partial g'}{\\partial \\beta'}\\right|_{\\beta_0}\\)\n\\(N^{-1/2}\\sum_{i=1}^N \\left.\\frac{\\partial g_i}{\\partial \\beta}u_i \\right|_{\\beta_0}\\xrightarrow{d}\\mathcal{N}(0,B_0)\\), donde \\(B_0=p\\lim \\frac{1}{N}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\sigma_{ij}\\left.\\frac{\\partial g_i}{\\partial \\beta}\\frac{\\partial g_i}{\\partial \\beta'}\\right|_{\\beta_0}=p\\lim\\frac{1}{N}\\left.\\frac{\\partial g'}{\\partial \\beta}\\Omega_0 \\frac{\\partial g}{\\partial \\beta'}\\right|_{\\beta_0}\\).\n\n\n[5 puntos] Plantee el problema de optimización para la minimización de la suma de los errores cuadráticos y obtenga las condiciones de primer orden.\n[10 puntos] Pruebe que \\(\\hat{\\beta}_{MCNL}\\), el estimador de mínimos cuadrados no lineales (MCNL) y definido como una raíz de las condiciones de primer orden, es consistente para \\(\\beta_0\\).\n[10 puntos] Derive una expresión para \\(\\sqrt{N}(\\hat{\\beta}_{MCNL}-\\beta_0)\\) y pruebe que \\(\\sqrt{N}(\\hat{\\beta}_{MCNL}-\\beta_0)\\xrightarrow{d}\\mathcal{N}(0,A_0^{-1}B_0A_0^{-1})\\). Tip: utilice una expansión de Taylor exacta de primer orden.\n[5 puntos] ¿Cómo estimaría \\(V(\\hat{\\beta}_{MCNL})\\)?"
  },
  {
    "objectID": "tareas/tarea-1.html#pregunta-2",
    "href": "tareas/tarea-1.html#pregunta-2",
    "title": "Tarea 1",
    "section": "Pregunta 2",
    "text": "Pregunta 2\nSuponga que está interesado en una variable aleatoria que tiene una distribución Bernoulli con parámetro \\(p\\). La función de densidad está definida como:\n\\[f(x_;p)=\\left\\{\\begin{array} .1 & \\text{con probabilidad } p \\\\ 0 & \\text{con probabilidad } 1-p \\end{array} \\right.\\] Suponga que tiene una muestra de \\(N\\) observaciones independientes e idénticamente distribuidas.\n\n[4 puntos] Plantee la función de log verosimilitud del problema.\n[4 puntos] Obtenga las condiciones de primer orden y resuelva para \\(\\hat{p}\\).\n[2 puntos] ¿Cuál es la media y la varianza del estimador de máxima verosimilitud que ha encontrado?"
  },
  {
    "objectID": "tareas/tarea-1.html#pregunta-3",
    "href": "tareas/tarea-1.html#pregunta-3",
    "title": "Tarea 1",
    "section": "Pregunta 3",
    "text": "Pregunta 3\nConsidere el modelo logit:\n\\[f(y_i|x_i;\\theta_0)=\\left\\{ \\begin{array} .1 & \\frac{\\exp\\{x_i'\\theta_0\\}}{1+\\exp\\{x_i'\\theta_0\\}}  \\\\ 0 &  \\frac{1}{1+\\exp\\{x_i'\\theta_0\\}} \\end{array} \\right.\\] donde \\(x_i\\) es un vector de variables explicativas, \\(\\theta_0\\) y es el vector de parámetros poblacional. Asuma que dispone de observaciones \\((y_i,x_i)\\) que son iid.\n\n[5 puntos] Escriba la función de log verosimilitud condicional para la observación \\(i\\).\n[5 puntos] Encuentre el vector score para la observación \\(i\\).\n[5 puntos] Encuentre la hesiana de la función de log verosimilitud con respecto a \\(\\mathbf{\\theta}\\).\n[5 puntos] Obtenga la matriz de información para la observación \\(i\\)."
  },
  {
    "objectID": "tareas/tarea-1.html#pregunta-4",
    "href": "tareas/tarea-1.html#pregunta-4",
    "title": "Tarea 1",
    "section": "Pregunta 4",
    "text": "Pregunta 4\nSuponga una variable aleatoria \\(X_i\\) con distribución desconocida. Sin embargo, sí conocemos que \\(E(X)=\\mu=54\\) y que \\(\\sqrt{V(X)}=\\sigma=6\\). Suponga que se recolecta una muestra de 50 observaciones.\n\n[2 punto] ¿Cuál es la distribución asintótica de la media muestral \\(\\bar{X}\\)?\n[4 punto] ¿Cuál es la probabilidad de que \\(\\bar{X}&gt;58\\)?\n[2 punto] ¿Cuál es la probabilidad de que una observación elegida al azar sea tal que \\(X_i&gt;58\\)?\n[2 punto] Provea un intervalo de confianza de 99% para la media muestral."
  },
  {
    "objectID": "tareas/tarea-1.html#pregunta-5",
    "href": "tareas/tarea-1.html#pregunta-5",
    "title": "Tarea 1",
    "section": "Pregunta 5",
    "text": "Pregunta 5\nEn esta pregunta mostraremos los alcances de los teoremas del límite central. Para esto, generaremos muchas muestras de tamaño \\(N\\) con una distribución \\(Bernoulli\\) con probabilidad de éxito \\(p=0.7\\). Recuerde que cuando realice simulaciones, siempre debe fijar una semilla al inicio para poder replicar su trabajo.\n\n[2 puntos] ¿Cuál es la media y la varianza de una variable aleatoria \\(y_i \\sim Bernoulli(0.7)\\)?\n[2 puntos] Si \\(y_i\\) son iid y podemos aplicar un teorema de límite central, ¿cuál es la distribución teórica de \\(\\bar{y}\\) cuando \\(N\\to\\infty\\)?\n[5 puntos] Realice el siguiente procedimiento \\(J=1,000\\) veces. Obtenga una muestra de tamaño \\(N=3\\) a partir de la distribución \\(Bernoulli(0.7)\\) y calcule la media muestral \\(\\bar{y}\\). Coleccione las \\(J\\) medias muestrales y luego grafique un histograma de las medias muestrales obtenidas junto con una curva teórica normal con la media y varianza obtenida en la parte b. Comente sobre lo que observa.\n[3 puntos] Repita lo realizado en la parte b., ahora con \\(N=15\\). Comente sobre lo que observa.\n[3 puntos] Repita lo realizado en la parte b., ahora con \\(N=1,500\\). Comente sobre lo que observa.\n[5 puntos] ¿Cómo usaría este ejercicio con palabras simples para explicar a una persona que no sabe mucho de estadística sobre la importancia de los teoremas de límite central?"
  },
  {
    "objectID": "tareas/tarea-1.html#pregunta-6",
    "href": "tareas/tarea-1.html#pregunta-6",
    "title": "Tarea 1",
    "section": "Pregunta 6",
    "text": "Pregunta 6\nSea \\(x_1\\) un vector de variables continuas, \\(x_2\\) una variable continua y \\(d_1\\) una variable dicotómica. Considere el siguiente modelo probit: \\[P(y=1│x_1,x_2 )=\\Phi(x_1'\\alpha+\\beta x_2+\\gamma x_2^2 )\\]\n\n[5 punto] Provea una expresión para el efecto marginal de \\(x_2\\) en la probabilidad. ¿Cómo estimaría este efecto marginal?\n[3 punto] Considere ahora el modelo: \\[P(y=1│x_1  ,x_2 ,d_1)=\\Phi(x_1 '\\delta+\\pi x_2+\\rho d_1+\\nu x_2 d_1 )\\] Provea la nueva expresión para el efecto marginal de \\(x_2\\).\n[2 punto] En el modelo de la parte b., ¿cómo evaluaría el efecto de un cambio en \\(d_1\\) en la probabilidad? Provea una expresión para este efecto."
  },
  {
    "objectID": "tareas/index.html",
    "href": "tareas/index.html",
    "title": "Tareas",
    "section": "",
    "text": "Tarea 1\n\nFecha de entrega: viernes 13 de septiembre a las 20:00 en Teams\nPreguntas\nRespuestas\n\n\n\n\nTarea 2\n\nFecha de entrega: viernes 4 de octubre a las 20:00 en Teams\nPreguntas\nRespuestas\n\n\n\n\nTarea 3\n\nFecha de entrega: viernes 8 de noviembre a las 20:00 en Teams\nPreguntas\nRespuestas\n\n\n\n\nTarea 4\n\nFecha de entrega: viernes 29 de noviembre a las 20:00 en Teams\nPreguntas\nRespuestas",
    "crumbs": [
      "Tareas"
    ]
  },
  {
    "objectID": "reglas.html",
    "href": "reglas.html",
    "title": "Reglas",
    "section": "",
    "text": "No se tolerarán actos de discriminación. Se procura un ambiente de respeto entre todos los miembros de la clase.\nToda la comunicación relativa al curso se dará por medio del correo institucional del CIDE.\nLas tareas y exámenes se entregarán a través de Teams.\nLos participantes en la sesión deberán procurar que haya un ambiente silencioso para el desarrollo de la clase.\nSe aplicarán estrictamente los lineamientos generales contenidos en el código de ética del CIDE en términos de plagio y fraude en tareas y exámenes.",
    "crumbs": [
      "Reglas"
    ]
  },
  {
    "objectID": "presentaciones.html",
    "href": "presentaciones.html",
    "title": "Presentaciones",
    "section": "",
    "text": "Por favor, seleccione una de las lecturas marcadas con una “+” de la lista de lecturas y envíe un correo al profesor para que se le asigne una fecha de presentación.\n\n\n\n\n\n\n\n\n\nAutores\nTema\nPresentador o presentadora\nFecha de exposición\n\n\n\n\nAvila-Foucat & Pérez-Campuzano (2015)\nBinaria\nLuis Ángel Ruiz\nMiércoles 2 de octubre\n\n\nKveder & Flahaux (2013)\nMultinomial\nAlejandro Altair\nMiércoles 2 de octubre\n\n\nZou & Luo (2019)\nTobit\nDavid Eugenio\nMiércoles 2 de octubre\n\n\nHackett & Marquez-Padilla (2019)\nVI\nMoisés Barranco\nJueves 17 de octubre\n\n\nLópez-Feldman & Chávez (2017)\nVI\nDante Rito\nJueves 17 de octubre\n\n\nAmare t al. (2021)\nPanel\nEmiliano Alejo\nJueves 31 de octubre\n\n\nKagin et al. (2016)\nPanel\nMarcial Castillo\nJueves 31 de octubre\n\n\nLi & Maddala (1999)\nBootstrap\nVictor Rosas\nJueves 21 de noviembre\n\n\nEngelhardt & Kumar (2011)\nCuantil\nMoisés Pelayo\nJueves 21 de noviembre",
    "crumbs": [
      "Presentaciones"
    ]
  },
  {
    "objectID": "lecturas.html",
    "href": "lecturas.html",
    "title": "Lecturas",
    "section": "",
    "text": "Todas las lecturas de capítulos de libro son obligatorias pues permiten una discusión informada en la clase. Las lecturas marcadas con “*” no serán cubiertas en clase, pero son ampliamente recomendables. En las sesiones de exposiciones cada alumno presentará uno de los artículos enlistados con negritas, por lo que se espera que el resto de la clase tenga el conocimiento suficiente para participar en la discusión.",
    "crumbs": [
      "Lecturas"
    ]
  },
  {
    "objectID": "lecturas.html#prerrequisitos",
    "href": "lecturas.html#prerrequisitos",
    "title": "Lecturas",
    "section": "Prerrequisitos",
    "text": "Prerrequisitos\n\nModelos lineales\n\nW, Capítulos 1-4",
    "crumbs": [
      "Lecturas"
    ]
  },
  {
    "objectID": "lecturas.html#semana-1",
    "href": "lecturas.html#semana-1",
    "title": "Lecturas",
    "section": "Semana 1",
    "text": "Semana 1\n\nIntroducción\n\nAngrist, J. D., & Pischke, J. S. (2017). Undergraduate Econometrics Instruction: Through Our Classes, Darkly. Journal of Economic Perspectives,31(2), 125-44.\n* Nakamura, E., & Steinsson, J. (2018). Identification in macroeconomics. Journal of Economic Perspectives, 32(3), 59-86.\n\nMCO\n\nCT, Capítulo 4 (secciones 4.1 a 4.3)",
    "crumbs": [
      "Lecturas"
    ]
  },
  {
    "objectID": "lecturas.html#semana-2",
    "href": "lecturas.html#semana-2",
    "title": "Lecturas",
    "section": "Semana 2",
    "text": "Semana 2\n\nTeoría asintótica\n\nCT, Capítulo 4, (sección 4.4)\n\nEstimadores extremos\n\nCT, Capítulo 5",
    "crumbs": [
      "Lecturas"
    ]
  },
  {
    "objectID": "lecturas.html#semana-3",
    "href": "lecturas.html#semana-3",
    "title": "Lecturas",
    "section": "Semana 3",
    "text": "Semana 3\n\nPrueba de hipótesis\n\nCT, Capítulo 7",
    "crumbs": [
      "Lecturas"
    ]
  },
  {
    "objectID": "lecturas.html#semana-4",
    "href": "lecturas.html#semana-4",
    "title": "Lecturas",
    "section": "Semana 4",
    "text": "Semana 4\n\nVariable dependiente binaria\n\nCT, Capítulo 14 (secciones 14.1 - 14.4)\nBinaria: Avila-Foucat, V. S., & Pérez-Campuzano, E. (2015). Municipality socioeconomic characteristics and the probability of occurrence of Wildlife Management Units in Mexico. Environmental Science & Policy, 45, 146-153.\n* Ordenados: Conigliani, C., Manca, A., & Tancredi, A. (2015). Prediction of patient-reported outcome measures via multivariate ordered probit models. Journal of the Royal Statistical Society. Series A (Statistics in Society), 567-591.\n\nVariable dependiente categórica\n\nCT, Capítulo 15 (secciones 15.1 - 15.4)\nMultinomial: Kveder, C. L. M., & Flahaux, M. L. (2013). Returning to Dakar: A mixed methods analysis of the role of migration experience for occupational status. World Development, 45, 223-238.",
    "crumbs": [
      "Lecturas"
    ]
  },
  {
    "objectID": "lecturas.html#semana-5",
    "href": "lecturas.html#semana-5",
    "title": "Lecturas",
    "section": "Semana 5",
    "text": "Semana 5\n\nModelos de conteo\n\nCT, Capítulo 20 (secciones 20.1 - 20.4).\n* Poisson: White, K., & Buckley, C. J. (2011). Exposure to international migration and its effect on childbearing in Turkey. International Migration Review, 45(1), 123-147.\n* Negativo binomial: Antón, J. I., & De Bustillo, R. M. (2010). Health care utilisation and immigration in Spain. The European Journal of Health Economics, 11(5), 487-498.\n* Inflado en cero: Young, J. D., Anderson, N. M., Naughton, H. T., & Mullan, K. (2018). Economic and policy factors driving adoption of institutional woody biomass heating systems in the US. Energy Economics, 69, 456-470.\n* Dos partes: Colchero, M. A., Molina, M., & Guerrero-López, C. M. (2017). After Mexico implemented a tax, purchases of sugar-sweetened beverages decreased and water increased: difference by place of residence, household composition, and income level. The Journal of nutrition, 147(8), 1552-1557.",
    "crumbs": [
      "Lecturas"
    ]
  },
  {
    "objectID": "lecturas.html#semana-6",
    "href": "lecturas.html#semana-6",
    "title": "Lecturas",
    "section": "Semana 6",
    "text": "Semana 6\n\nModelos de selección\n\nCT, Capítulo 16 (secciones 16.1 – 16.6)\nTobit: Zou, B., & Luo, B. (2019). Rural Household Energy Consumption Characteristics and Determinants in China. Energy.\n* Heckman: Parey, M., Ruhose, J., Waldinger, F., & Netz, N. (2017). The selection of high-skilled emigrants. Review of Economics and Statistics, 99(5), 776-792.\n* Ordenado + selección: Alemi, F., Circella, G., Mokhtarian, P., & Handy, S. (2019). What drives the use of ridehailing in California? Ordered probit models of the usage frequency of Uber and Lyft. Transportation Research Part C: Emerging Technologies, 102, 233-248.",
    "crumbs": [
      "Lecturas"
    ]
  },
  {
    "objectID": "lecturas.html#semana-7",
    "href": "lecturas.html#semana-7",
    "title": "Lecturas",
    "section": "Semana 7",
    "text": "Semana 7\n\nVariables instrumentales\n\nW, Capítulo 5.\nCT, Capítulo 4 (secciones 4.8 y 4.9)",
    "crumbs": [
      "Lecturas"
    ]
  },
  {
    "objectID": "lecturas.html#semana-8",
    "href": "lecturas.html#semana-8",
    "title": "Lecturas",
    "section": "Semana 8",
    "text": "Semana 8\n\nMétodo generalizado de momentos\n\nCT, Capítulo 6 (secciones 6.1 - 6.4)",
    "crumbs": [
      "Lecturas"
    ]
  },
  {
    "objectID": "lecturas.html#semana-9",
    "href": "lecturas.html#semana-9",
    "title": "Lecturas",
    "section": "Semana 9",
    "text": "Semana 9\n\nVariables instrumentales en la práctica\n\nVI: Hackett, L., & Marquez-Padilla, F. (2019). Working for Change: the Effect of Female Labor Force Participation on Fertility. SSRN Working Paper 3354753.\nVI: López-Feldman, A., & Chávez, E. (2017). Remittances and natural resource extraction: Evidence from Mexico. Ecological Economics, 132, 69-79.\n\nOtras aplicaciones de VI\n\n* VI: Campos-Vazquez, R. M., & Nuñez, R. (2019). Obesity and labor market outcomes in Mexico/Obesidad y el mercado de trabajo en México. Estudios Económicos, 34(2), 159-196.\n* MC2E2M: Mocetti, S. (2007). Intergenerational earnings mobility in Italy. The BE Journal of Economic Analysis & Policy, 7(2).\n\n* Poisson + VI: Hirvonen, K., & Hoddinott, J. (2017). Agricultural production and children’s diets: Evidence from rural Ethiopia. Agricultural Economics, 48(4), 469-480.",
    "crumbs": [
      "Lecturas"
    ]
  },
  {
    "objectID": "lecturas.html#semana-10",
    "href": "lecturas.html#semana-10",
    "title": "Lecturas",
    "section": "Semana 10",
    "text": "Semana 10\n\nModelos y estimadores de panel\n\nCT, Capítulo 21\nPanel: Amare, M., Abay, K. A., Tiberti, L., & Chamberlin, J. (2021). COVID-19 and food security: Panel data evidence from Nigeria. Food policy, 101, 102099.\nPanel: Kagin, J., Taylor, J. E., & Yúnez-Naude, A. (2016). Inverse productivity or inverse efficiency? Evidence from Mexico. The Journal of Development Studies, 52(3), 396-411.\n* Panel: Bwalya, S. M. (2006). Foreign direct investment and technology spillovers: Evidence from panel data analysis of manufacturing firms in Zambia. Journal of development economics, 81(2), 514-526.\n* Panel + DID: Estrada, R. (2019). Rules versus discretion in public service: Teacher hiring in Mexico. Journal of Labor Economics, 37(2), 545-579.",
    "crumbs": [
      "Lecturas"
    ]
  },
  {
    "objectID": "lecturas.html#semana-11",
    "href": "lecturas.html#semana-11",
    "title": "Lecturas",
    "section": "Semana 11",
    "text": "Semana 11\n\nTemas de errores estándar\n\nMHE, Capítulo 8",
    "crumbs": [
      "Lecturas"
    ]
  },
  {
    "objectID": "lecturas.html#semana-12",
    "href": "lecturas.html#semana-12",
    "title": "Lecturas",
    "section": "Semana 12",
    "text": "Semana 12\n\nBootstrap\n\nCT, Capítulo 11\nBootstrap: Li, H., & Maddala, G. S. (1999). Bootstrap variance estimation of nonlinear functions of parameters: an application to long-run elasticities of energy demand. Review of Economics and Statistics, 81(4), 728-733.\n\nRegresión cuantil\n\nCT, Capítulo 4 (sección 4.6)\nCuantil: Engelhardt, G. V., & Kumar, A. (2011). Pensions and household wealth accumulation. Journal of Human Resources, 46(1), 203-236.",
    "crumbs": [
      "Lecturas"
    ]
  },
  {
    "objectID": "lecturas.html#semana-13",
    "href": "lecturas.html#semana-13",
    "title": "Lecturas",
    "section": "Semana 13",
    "text": "Semana 13\n\nMétodos semiparamétricos\n\nCT, Capítulo 9\nSemiparamétrico: Hussinger, K. (2008). R&D and subsidies at the firm level: An application of parametric and semiparametric two‐step selection models. Journal of applied econometrics, 23(6), 729-747.",
    "crumbs": [
      "Lecturas"
    ]
  },
  {
    "objectID": "lecturas.html#extensiones",
    "href": "lecturas.html#extensiones",
    "title": "Lecturas",
    "section": "Extensiones",
    "text": "Extensiones\n\nPanel no lineal\n\nCT, Capítulo 23\n* Panel Poisson: Castillo, J. C., Mejía, D., & Restrepo, P. (2020). Scarcity without leviathan: The violent effects of cocaine supply shortages in the mexican drug war. Review of Economics and Statistics, 102(2), 269-286.\n\nPanel con endogeneidad\n\nCT, Capítulo 22 (secciones 22.1 - 22.5)\n* Panel + VI: Antman, F. M. (2011). The intergenerational effects of paternal migration on schooling and work: What can we learn from children’s time allocations?. Journal of Development Economics, 96(2), 200-208.\n* Wooldridge, J. (2012). Panel data models with heterogeneity and endogeneity. Institute for Fiscal Studies.\n* Semykina, A., & Wooldridge, J. M. (2010). Estimating panel data models in the presence of endogeneity and selection. Journal of Econometrics, 157(2), 375-380.\n\nModelos de riesgo y sobrevivencia\n\nCT, Capítulo 17 (secciones 17.1 – 17.4 y 17.6-17.11)\n* Riesgo: De Uña-Alvarez, J., Otero-Giráldez, M. S., & Alvarez-Llorente, G. (2003). Estimation under length-bias and right-censoring: an application to unemployment duration analysis for married women. Journal of Applied Statistics, 30(3), 283-291.",
    "crumbs": [
      "Lecturas"
    ]
  },
  {
    "objectID": "diapositivas/variables-instrumentales.html#qué-sucede-si-se-violan-los-supuestos-de-mco",
    "href": "diapositivas/variables-instrumentales.html#qué-sucede-si-se-violan-los-supuestos-de-mco",
    "title": "Variables instrumentales",
    "section": "¿Qué sucede si se violan los supuestos de MCO?",
    "text": "¿Qué sucede si se violan los supuestos de MCO?\nSupongamos que el el proceso de salarios verdadero está dado por\n\\[\\ln(w_i)=\\beta_0+\\beta_1 educ_i+\\beta_2 habilidad_i+e_i\\]\nY asumamos que la habilidad es no observada y decidimos estimar\n\\[\\ln(w_i)=\\beta_0+\\beta_1 educ_i+u_i\\]\n¿Dónde queda la habilidad?\nEl \\(\\hat{\\beta}_1\\) estimado con esta regresión corta es inconsistente porque \\(u_i\\) incluye la habilidad, que afecta tanto el desempeño en el mercado laboral como el desempeño en la escuela"
  },
  {
    "objectID": "diapositivas/variables-instrumentales.html#instrumentos",
    "href": "diapositivas/variables-instrumentales.html#instrumentos",
    "title": "Variables instrumentales",
    "section": "Instrumentos",
    "text": "Instrumentos\nConsideremos el siguiente modelo\n\\[y=\\beta_0+\\beta_1 x+u\\]\ndonde \\(cov(x,u)\\neq 0\\)\nSuponga que existe una varible \\(z\\) que cumple con:\n\nExogeneidad: \\(z\\) no está correlacionada con \\(u\\)\n\n\\[cov(z,u)=0\\]\n\nRelevancia: \\(z\\) está correlacionado con \\(x\\)\n\n\\[cov(z,x)\\neq 0\\]\nEntonces \\(z\\) es un instrumento de \\(x\\)\nLa exogeneidad implica que \\(z\\) no debe estar correlacionado con factores omitidos (por ejemplo, la habilidad)"
  },
  {
    "objectID": "diapositivas/variables-instrumentales.html#estimador-de-vi",
    "href": "diapositivas/variables-instrumentales.html#estimador-de-vi",
    "title": "Variables instrumentales",
    "section": "Estimador de VI",
    "text": "Estimador de VI\nCalculando la covarianza con \\(z\\) de \\(y=\\beta_0+\\beta_1 x+u\\) obtenemos:\n\\[cov(y,z)=\\beta_1 cov(x,z)+cov(u,z)\\]\nY, si \\(cov(u,z)\\), resolviendo para \\(\\hat{\\beta}_1\\)\n\\[\\hat{\\beta}_1=\\frac{cov(y,z)}{cov(x,z)}\\]\nsiempre y cuando \\(cov(x,z)\\neq 0\\)\nPor una LGN se puede mostrar que \\(\\hat{\\beta}_1\\) es consistente\nSin embargo, como profundizaremos más adelante, \\(\\hat{\\beta}_1\\) siempre es sesgagado\nEl sesgo puede ser sustancial en muestras pequeñas, por lo que se recomienda tener precaución con el tamaño de la muestra"
  },
  {
    "objectID": "diapositivas/variables-instrumentales.html#ejemplo-rendimientos-a-la-educación",
    "href": "diapositivas/variables-instrumentales.html#ejemplo-rendimientos-a-la-educación",
    "title": "Variables instrumentales",
    "section": "Ejemplo: rendimientos a la educación",
    "text": "Ejemplo: rendimientos a la educación\nCard (1995) estudia el problema de retornos a la educación\nTenemos una muestra de 5,525 hombres de entre 14 y 24 años\nNos interesa la relación entre educación e ingreso, pero sabemos que no observamos la habilidad\nSabemos que si estimamos \\(\\ln(w_i)=\\beta_0+\\beta_1 educ_i+u_i\\), el coeficiente \\(\\beta_1\\) estará sesgado\nCard emplea como instrumento una variable \\(z_i\\) que indica si en el municipio de la persona \\(i\\) hay una universidad\nLa intución es que la presencia de la universidad baja el costo de ir a la universidad, pero esto no afecta directamente el ingreso\nEncontrar un instrumento casi nunca es tarea sencilla: se trata de enteder cómo los mecanismos, las instituciones y los contextos"
  },
  {
    "objectID": "diapositivas/variables-instrumentales.html#lenguaje-de-vi",
    "href": "diapositivas/variables-instrumentales.html#lenguaje-de-vi",
    "title": "Variables instrumentales",
    "section": "Lenguaje de VI",
    "text": "Lenguaje de VI\nDe forma más general, partimos del la siguiente ecuación estructural para \\(y_i\\):\n\\[y_i=\\beta_0+\\beta_1 x_{1i}+\\beta_2 x_{2i}+u_i\\]\ndonde \\(cov(x_{1i},u_i)\\neq 0\\)\nA \\(x_{1i}\\) se le llama la variable endógena\nSe incluye también una o más variables exógenas como \\(x_{2i}\\) que no están correlacionadas con \\(u_i\\)\nA una regresión de la variable de interés en función del instrumento y las variables exógenas se le conoce como forma reducida\n\\[y_i=\\beta_0+\\beta_1 z_{1i}+\\beta_2 x_{2i}+u_i\\]"
  },
  {
    "objectID": "diapositivas/variables-instrumentales.html#primera-etapa",
    "href": "diapositivas/variables-instrumentales.html#primera-etapa",
    "title": "Variables instrumentales",
    "section": "Primera etapa",
    "text": "Primera etapa\nLa primera etapa especifica la relación entre la variable endógena y el instrumento:\n\\[x_{1i}=\\pi_0+\\pi_1 z_i+\\pi_2x_{2i}+\\nu_i\\]\nDonde se cumple que \\(cov(z_i,\\nu_i=0)\\) y \\(cov(x_{2i},\\nu_i)=0\\)\nEntonces, la condición de relevancia puede escribirse también como \\(\\pi_1\\neq 0\\)\nNoten que la primera etapa también implica que, descontando el efecto de \\(z_i\\), todavía \\(x_{1i}\\) y \\(x_{2i}\\) están correlacionadas\nLa primera etapa puede y debe probarse empíricamente\nEn cambio, no es posible probar la restricción de exclusión, que debe estar respaldada sobre todo por la teoría económica, el conocimiento de las instituciones, la exogeneidad de experimentos naturales, etc."
  },
  {
    "objectID": "diapositivas/variables-instrumentales.html#más-de-un-instrumento",
    "href": "diapositivas/variables-instrumentales.html#más-de-un-instrumento",
    "title": "Variables instrumentales",
    "section": "Más de un instrumento",
    "text": "Más de un instrumento\nEs posible que haya \\(J\\) variables \\(z_{ij}\\) que puedan funcionar como instrumento\nSe debe cumplir que \\(cov(u_i,z_{ij})=0\\) y que cada una se correlacione con \\(x_{i1}\\)\nCon dos instrumentos, podemos escribir la primera etapa como\n\\[x_{1i}=\\pi_0+\\pi_1 z_{1i}+ \\pi_2 z_{2i} +\\pi_3x_{2i}+\\nu_i\\]\nAhora, debe cumplirse que \\(cov(z_{1i},\\nu_i)=cov(z_{2i},\\nu_i)=cov(x_{2i},\\nu_i)=0\\)\nPara lograr identificación, se requiere que \\(\\pi_1\\neq 0\\) o \\(\\pi_2\\neq 0\\)\nPodemos usar una prueba \\(F\\) para probar que \\(\\pi_1=\\pi_2=0\\)"
  },
  {
    "objectID": "diapositivas/variables-instrumentales.html#mínimos-cuadrados-en-dos-etapas",
    "href": "diapositivas/variables-instrumentales.html#mínimos-cuadrados-en-dos-etapas",
    "title": "Variables instrumentales",
    "section": "Mínimos cuadrados en dos etapas",
    "text": "Mínimos cuadrados en dos etapas\nEl modelo presentado anteriormente sugiere que podemos estimar \\(\\beta_1\\) con un procedimiento de dos etapas\n\nRegresión de \\(x_{1i}\\) sobre los instrumentos y las variables exógenas para obtener \\(\\hat{x}_{1i}\\)\nRegresión de \\(y_i\\) sobre las variables exógenas y \\(\\hat{x}_{1i}\\)\n\nEs como si purgáramos a \\(x_{1i}\\) de su correlación con \\(u_i\\)\nNunca hacemos esto a mano\n\nCuando tenemos tantos instrumentos como endógenas, usamos el estimador de variables instrumentales\nCuando tenemos más instrumentos que endógenas, recurrimos al método generalizado de momentos"
  },
  {
    "objectID": "diapositivas/variables-instrumentales.html#más-sobre-rendimientos-a-la-educación",
    "href": "diapositivas/variables-instrumentales.html#más-sobre-rendimientos-a-la-educación",
    "title": "Variables instrumentales",
    "section": "Más sobre rendimientos a la educación",
    "text": "Más sobre rendimientos a la educación\nEn el problema de Card (1995), la primera etapa es\n\\[esc_i=\\pi_0+\\pi_1 X_i+ \\phi unicerca_i +\\nu_i\\] donde \\(unicerca_i= \\begin{cases} 1 \\quad\\text{había una universidad en el municipio} \\\\ 0 \\quad\\text{otro caso}\\\\ \\end{cases}\\)\nY la forma reducida es\n\\[\\ln(w_i)=\\gamma_0+\\gamma_1 X_i+  \\delta unicerca_i+\\varepsilon_i\\]\nSabemos que el salario estará correlacionado con la presencia de la universidad, pero estas diferencias ocurren por la vía de la escolaridad"
  },
  {
    "objectID": "diapositivas/variables-instrumentales.html#ejemplo-card-1995",
    "href": "diapositivas/variables-instrumentales.html#ejemplo-card-1995",
    "title": "Variables instrumentales",
    "section": "Ejemplo: Card (1995)",
    "text": "Ejemplo: Card (1995)\nUsamos los datos en ingresos_iv.csv, del estudio de Card que hemos mencionado como ejemplo\nLa librería AER, que ya hemos usado, tiene la función ivreg\nTambién usaremos una nueva librería, gmm\nEstimemos la relación entre el log del salario y la educación\n\ndata.ingresos &lt;- read_csv(\"../files/ingresos_iv.csv\",\n                          locale = locale(encoding = \"latin1\"))\n#MCO\nmco &lt;- lm(lwage ~ educ + exper + black + south + married + smsa,\n          data = data.ingresos)\n\n#Variables instrumentales (asume homocedasticidad)\nvi &lt;- ivreg(lwage ~  educ + exper + black + south + married + smsa |\n               . - educ + nearc4, data = data.ingresos)"
  },
  {
    "objectID": "diapositivas/variables-instrumentales.html#ejemplo-card-1995-1",
    "href": "diapositivas/variables-instrumentales.html#ejemplo-card-1995-1",
    "title": "Variables instrumentales",
    "section": "Ejemplo: Card (1995)",
    "text": "Ejemplo: Card (1995)\nSabemos que \\(\\beta_1\\) estimado por MCO es inconsistente\n\n\nmodelsummary(list(\"MCO\"=mco, \"VI\"=vi),\n             coef_map = c(\"educ\", \"exper\"),\n             gof_map = c(\"nobs\", \"F\"))\n\n\n \n\n  \n    \n    \n    tinytable_ympncd7jkdp6tzco8jr8\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                MCO\n                VI\n              \n        \n        \n        \n                \n                  educ    \n                  0.071  \n                  0.124  \n                \n                \n                          \n                  (0.003)\n                  (0.050)\n                \n                \n                  exper   \n                  0.034  \n                  0.056  \n                \n                \n                          \n                  (0.002)\n                  (0.020)\n                \n                \n                  Num.Obs.\n                  3003   \n                  3003   \n                \n                \n                  F       \n                  219.153"
  },
  {
    "objectID": "diapositivas/variables-instrumentales.html#ejemplo-card-1995-2",
    "href": "diapositivas/variables-instrumentales.html#ejemplo-card-1995-2",
    "title": "Variables instrumentales",
    "section": "Ejemplo: Card (1995)",
    "text": "Ejemplo: Card (1995)\nLa primera etapa de este ejercicio es\n\n# Primera etapa\npe_vi &lt;- lm(educ ~  nearc4 + exper + black + south + married + smsa,\n            data = data.ingresos)\n\n\n\nmodelsummary(list(\"MCO\"=mco, \"VI\"=vi, \"Primera etapa\"=pe_vi),\n             coef_map = c(\"educ\", \"exper\", \"nearc4\"),\n             gof_map = c(\"nobs\", \"F\"))\n\n\n \n\n  \n    \n    \n    tinytable_gtofodh1puvh3lhmsn4n\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                MCO\n                VI\n                Primera etapa\n              \n        \n        \n        \n                \n                  educ    \n                  0.071  \n                  0.124  \n                         \n                \n                \n                          \n                  (0.003)\n                  (0.050)\n                         \n                \n                \n                  exper   \n                  0.034  \n                  0.056  \n                  -0.404 \n                \n                \n                          \n                  (0.002)\n                  (0.020)\n                  (0.009)\n                \n                \n                  nearc4  \n                         \n                         \n                  0.327  \n                \n                \n                          \n                         \n                         \n                  (0.082)\n                \n                \n                  Num.Obs.\n                  3003   \n                  3003   \n                  3003   \n                \n                \n                  F       \n                  219.153\n                         \n                  456.140"
  },
  {
    "objectID": "diapositivas/variables-instrumentales.html#ejemplo-con-oferta-y-demanda",
    "href": "diapositivas/variables-instrumentales.html#ejemplo-con-oferta-y-demanda",
    "title": "Variables instrumentales",
    "section": "Ejemplo con oferta y demanda",
    "text": "Ejemplo con oferta y demanda\nSupongamos que nos interesa estimar la elasticidad del consumo de mantequilla\n\\[\\ln(q_{_i})=\\alpha + \\beta \\ln(p_i) + \\varepsilon_i\\] Supongamos que tenemos datos del precio y el consumo en una muestra grande de localidades \\(i\\)\nSi estimamos la ecuación anterior por MCO, el coeficiente estimado \\(\\hat{\\beta}\\) será inconsistente por un problema de simulateneidad\nEl precio y la cantidad se determinan en equilibrio por la interacción de la oferta y la demanda\nUn choque a la oferta o la demanda afectará tanto la cantidad como el precio de equilibrio"
  },
  {
    "objectID": "diapositivas/variables-instrumentales.html#ejemplo-con-oferta-y-demanda-1",
    "href": "diapositivas/variables-instrumentales.html#ejemplo-con-oferta-y-demanda-1",
    "title": "Variables instrumentales",
    "section": "Ejemplo con oferta y demanda",
    "text": "Ejemplo con oferta y demanda\nPodemos usar variables instrumentales para estimar la elasticidad de la demanda\nNecesitamos una variable \\(z\\) que afecte solo la oferta, pero que no afecte directamente la demanda\n\\(z\\) puede ser precipitación o temperatura\nAl desplazar la oferta, manteniendo la demanda fija, se revela la forma de la curva de demanda\nPodemos estimar la elasticidad de la demanda"
  },
  {
    "objectID": "diapositivas/variables-instrumentales.html#ejemplo-con-oferta-y-demanda-2",
    "href": "diapositivas/variables-instrumentales.html#ejemplo-con-oferta-y-demanda-2",
    "title": "Variables instrumentales",
    "section": "Ejemplo con oferta y demanda",
    "text": "Ejemplo con oferta y demanda\n\n# Add custom curves\ndemand1 &lt;- data.frame(Hmisc::bezier(c(1, 3, 9),\n                                    c(9, 3, 1))) \n\nsupply1 &lt;- data.frame(Hmisc::bezier(c(1, 8, 9),\n                                    c(1, 5, 9))) \n\nsupply2 &lt;- data.frame(Hmisc::bezier(c(1, 8, 9),\n                                    c(3, 8, 12))) \n\nsupply3 &lt;- data.frame(Hmisc::bezier(c(1, 8, 9),\n                                    c(5, 10, 14)))"
  },
  {
    "objectID": "diapositivas/variables-instrumentales.html#ejemplo-con-oferta-y-demanda-3",
    "href": "diapositivas/variables-instrumentales.html#ejemplo-con-oferta-y-demanda-3",
    "title": "Variables instrumentales",
    "section": "Ejemplo con oferta y demanda",
    "text": "Ejemplo con oferta y demanda\n\neconocharts::sdcurve(supply1, demand1, supply2, demand1, supply3, demand1,\n        names = c(\"S[1]\", \"D[1]\",\"S[2]\", \"D[1]\", \"S[3]\", \"D[1]\"),\n        xmax = 15, ymax=10)\n\n# A tibble: 3 × 2\n      x     y\n  &lt;dbl&gt; &lt;dbl&gt;\n1  4.65  3.40\n2  3.31  4.76\n3  2.39  6.03"
  },
  {
    "objectID": "diapositivas/panel.html#estructura-de-datos-en-panel",
    "href": "diapositivas/panel.html#estructura-de-datos-en-panel",
    "title": "Modelos y datos de panel",
    "section": "Estructura de datos en panel",
    "text": "Estructura de datos en panel\n\nConsideremos el siguiente modelo para unidades indexadas como \\(i\\) observadas en varios periodos indexados con \\(t\\)\n\n\\[y_{it}=\\alpha_{it}+x_{it}'\\beta_{it}+u_{it}\\]\ncon \\(i=1,\\ldots,N\\) y \\(t=1,\\ldots,T\\)\n\nEste modelo así escrito es muy general pues tiene un intercepto y una pendiente para cada \\(i\\) y \\(t\\)\nSin embargo, este modelo es imposible de estimar pues hay más parámetros que las \\(T\\times N\\) observaciones"
  },
  {
    "objectID": "diapositivas/panel.html#modelo-de-efectos-fijos",
    "href": "diapositivas/panel.html#modelo-de-efectos-fijos",
    "title": "Modelos y datos de panel",
    "section": "Modelo de efectos fijos",
    "text": "Modelo de efectos fijos\n\nSi asumimos que los efectos individuales están potencialmente correlacionados con \\(x_{it}\\), nuestro modelo se conoce como de efectos fijos\n\\(\\alpha_i\\) es la heterogeneidad no observada\nCon paneles cortos no podemos estimar de forma consistente las \\(\\alpha_i\\)\nRecurriremos a métodos para deshacernos de las \\(\\alpha_i\\)"
  },
  {
    "objectID": "diapositivas/panel.html#modelo-de-efectos-aleatorios",
    "href": "diapositivas/panel.html#modelo-de-efectos-aleatorios",
    "title": "Modelos y datos de panel",
    "section": "Modelo de efectos aleatorios",
    "text": "Modelo de efectos aleatorios\n\nPor otro lado, si asumimos que la heterogeneidad no observada es una variable aleatoria independiente de \\(x_{it}\\), podemos hacer algunos supuestos sobre su distribución y especificar un modelo de efectos aleatorios\nSuponemos que tanto la heterogeneidad no observada como el error son iid\n\n\\(\\alpha_i \\sim (\\alpha,\\sigma^2_{\\alpha})\\)\n\\(\\varepsilon_{it} \\sim (0,\\sigma^2_{\\varepsilon})\\)\n\nPodemos ver al modelo como una transformación del original con un error compuesto \\(u_{it}=\\alpha_i+\\varepsilon_{it}\\):\n\n\\[y_{it}=x_{it}'\\beta+u_{it}\\]\n\nPodemos mostrar que la correlación de los errores compuestos de dos observaciones en distintos momentos del tiempo es \\(cor(u_{it},u_{is})=\\frac{\\sigma_{\\alpha}^2}{\\sigma_{\\alpha}^2+\\sigma_{\\varepsilon}^2}\\)\nTambién se conoce como modelo equicorrelacionado"
  },
  {
    "objectID": "diapositivas/panel.html#nota-sobre-la-terminología",
    "href": "diapositivas/panel.html#nota-sobre-la-terminología",
    "title": "Modelos y datos de panel",
    "section": "Nota sobre la terminología",
    "text": "Nota sobre la terminología\n\nQuizás un mejor nombre para el modelo de efectos aleatorios es el a veces usado modelo de intercepto aleatorio o de componentes aleatorios\nPara el modelo de efectos fijos, Lee (2002) usa el nombre de efecto relacionado y para el de efectos aleatorios el de efecto no relacionado\nLo más importante es tener en cuenta que \\(\\alpha_i\\) es una variable aleatoria, lo que cambia es lo que asumimos sobre su correlación con los regresores"
  },
  {
    "objectID": "diapositivas/panel.html#estimador-de-efectos-fijos",
    "href": "diapositivas/panel.html#estimador-de-efectos-fijos",
    "title": "Modelos y datos de panel",
    "section": "Estimador de efectos fijos",
    "text": "Estimador de efectos fijos\n\nTambién conocido como estimador within\nTomando la media sobre \\(i\\)\n\n\\[\n\\begin{aligned}\n\\frac{1}{T}\\sum_iy_{it}&=\\frac{1}{T}\\sum_i\\alpha_i+\\frac{1}{T}\\sum_ix_{it}'\\beta+\\frac{1}{T}\\sum_i\\varepsilon_{it} \\\\\n\\bar{y}_{i}&=\\alpha_i+ \\bar{x}_i\\beta+\\bar{\\varepsilon}_i\n\\end{aligned}\n\\]\n\nY luego restando del modelo original estas medias\n\n\\[y_{it}-\\bar{y}_{i}=(x_{it}-\\bar{x}_i)'\\beta+(\\varepsilon_{it}-\\bar{\\varepsilon}_i)\\]\n\nEl estimador within es el estimador de MCO a esta ecuación modificada\n\\(\\hat{\\beta}\\) es consistente si los \\(\\alpha_i\\) son efectos fijos y \\(\\varepsilon_{it}\\) es iid\nUna desventaja del modelo de efectos fijos es que no se pueden identificar el parámetros sobre regresores que son fijos en el tiempo, auque sean observables (sexo, IQ, lugar de nacimiento, raza, entre muchos otros)"
  },
  {
    "objectID": "diapositivas/panel.html#estimador-de-efectos-fijos-1",
    "href": "diapositivas/panel.html#estimador-de-efectos-fijos-1",
    "title": "Modelos y datos de panel",
    "section": "Estimador de efectos fijos",
    "text": "Estimador de efectos fijos\n\nFrecuentemente no estamos interesados en estimar los efectos fijos, es decir, los consideremos parámetros incómodos\nSi quisiéramos estimarlos directamente, podríamos incluir dummies individuales\nSe requieren páneles largos para que los efectos estimados sean consistentes\nEn el resto del curso nos enfocaremos en casos donde podemos ignorar estos parámetros incómodos"
  },
  {
    "objectID": "diapositivas/panel.html#estimador-de-primeras-diferencias",
    "href": "diapositivas/panel.html#estimador-de-primeras-diferencias",
    "title": "Modelos y datos de panel",
    "section": "Estimador de primeras diferencias",
    "text": "Estimador de primeras diferencias\n\nConsideremos el modelo\n\n\\[y_{it}-y_{it-1}=(x_{it}-x_{it-1})'\\beta+(\\varepsilon_{it}-\\varepsilon_{it-1})\\]\n\nEl estimador de primeras diferencias es el estimador de MCO de esta ecuación"
  },
  {
    "objectID": "diapositivas/panel.html#equivalencia-de-estimadores",
    "href": "diapositivas/panel.html#equivalencia-de-estimadores",
    "title": "Modelos y datos de panel",
    "section": "Equivalencia de estimadores",
    "text": "Equivalencia de estimadores\n\nSupongamos que existen una serie de características que son fijas en el tiempo\nEl estimador de primeras diferencias es consistente\nEl estimador de primeras diferencias y el within son iguales cuando \\(T=2\\)\nCon \\(T&gt;2\\) el estimador de primeras diferencias es menos eficiente\nLos paquetes estadísticos usan un estimador within"
  },
  {
    "objectID": "diapositivas/panel.html#propiedades-del-estimador-within",
    "href": "diapositivas/panel.html#propiedades-del-estimador-within",
    "title": "Modelos y datos de panel",
    "section": "Propiedades del estimador within",
    "text": "Propiedades del estimador within\n\nEl estimador within puede escribirse como\n\n\\[\\hat{\\beta}_W=\\left(\\sum_i\\sum_t(x_{it}-\\bar{x}_i)(x_{it}-\\bar{x}_i)'\\right)^{-1}\\sum_i\\sum_y(x_{it}-\\bar{x}_i)(y_{it}-\\bar{y}_i)\\]\n\nLa consistencia del estimador depende de que \\(p\\lim \\frac{1}{NT}\\sum_i\\sum_y(x_{it}-\\bar{x}_i)(y_{it}-\\bar{y}_i)=0\\) cuando \\(N\\to\\infty\\) o \\(T\\to\\infty\\)\nFrecuentemente tenemos pocos periodos (páneles cortos), por lo que esperamos que haya muchos individuos para probar consistencia\nLa condición de exogeneidad fuerte es suficiente para que \\(p\\lim \\frac{1}{NT}\\sum_i\\sum_y(x_{it}-\\bar{x}_i)(y_{it}-\\bar{y}_i)=0\\) cuando \\(N\\to\\infty\\) o \\(T\\to\\infty\\)"
  },
  {
    "objectID": "diapositivas/panel.html#propiedades-del-estimador-within-1",
    "href": "diapositivas/panel.html#propiedades-del-estimador-within-1",
    "title": "Modelos y datos de panel",
    "section": "Propiedades del estimador within",
    "text": "Propiedades del estimador within\n\nPodemos asumir que los errores \\(\\varepsilon_{it}\\) son iid y obtener\n\n\\[\n\\begin{aligned}\n\\hat{V}(\\hat{\\beta}_{W,h})&= \\sigma^2_{\\varepsilon}\\left(\\sum_i\\sum_t (x_{it}-\\bar{x}_i)(x_{it}-\\bar{x}_i)'\\right)^{-1}\\\\\n&=\\sigma^2_{\\varepsilon}\\left(\\sum_i\\sum_t \\ddot{x}_{it}\\ddot{x}_{it}'\\right)^{-1}\n\\end{aligned}\n\\] donde se puede estimar \\(\\hat{\\sigma}^2_{\\varepsilon}=\\frac{1}{N(T-1)-K}\\sum_i\\sum_t\\hat{\\varepsilon}_{it}\\)\n\nSin embargo, en panel esto es un supuesto fuerte pues esperamos que, auque las observaciones sean independientes entre individuos, haya correlación serial"
  },
  {
    "objectID": "diapositivas/panel.html#propiedades-del-estimador-within-2",
    "href": "diapositivas/panel.html#propiedades-del-estimador-within-2",
    "title": "Modelos y datos de panel",
    "section": "Propiedades del estimador within",
    "text": "Propiedades del estimador within\n\nGeneralmente usamos una versión robusta, que considera la correlación serial y permite heterocedasticidad (ver ecuación 21.28 en CT)\n\n\\[\n\\begin{aligned}\n\\hat{V}(\\hat{\\beta}_{W})&=\\left(\\sum_i\\sum_t \\ddot{x}_{it}\\ddot{x}_{it}'\\right)^{-1}\\left(\\sum_i \\sum_t \\sum_s \\ddot{x}_{it}\\ddot{x}_{is}'\\hat{\\ddot{e}}_{it}\\hat{\\ddot{e}}_{is}\\right)\\left(\\sum_i\\sum_t \\ddot{x}_{it}\\ddot{x}_{it}'\\right)^{-1}\n\\end{aligned}\n\\]\n\nEsta versión con la estructura de la matríz de White es robusta a la heterocedasticidad y a la correlación serial\nEs lo que se conoce como matriz de varianzas agrupada"
  },
  {
    "objectID": "diapositivas/panel.html#breve-desviación-a-mínimos-cuadrados-generalizados",
    "href": "diapositivas/panel.html#breve-desviación-a-mínimos-cuadrados-generalizados",
    "title": "Modelos y datos de panel",
    "section": "Breve desviación a mínimos cuadrados generalizados",
    "text": "Breve desviación a mínimos cuadrados generalizados\n\nEn ECNI aprendimos que el estimador de MCO es el estimador de varianza mínima entre los estimadores lineales insesgados cuando los errores son iid\nAhora consideremos el modelo \\(y=X\\beta+u\\)\nAasumamos que la varianza de los errores es \\(\\Omega\\neq\\sigma^2I\\)\nAsumamos que conocemos \\(\\Omega\\), una matriz no singular\nSi premultiplicamos el modelo lineal por \\(\\Omega^{-1/2}\\)\n\n\\[\\Omega^{-1/2}y=\\Omega^{-1/2}X\\beta+\\Omega^{-1/2}u\\] se puede mostrar con algo de álgebra que\n\\[V(\\Omega^{-1/2}u)=E((\\Omega^{-1/2}u)(\\Omega^{-1/2}u)'|X)=I\\]"
  },
  {
    "objectID": "diapositivas/panel.html#breve-desviación-a-mínimos-cuadrados-generalizados-1",
    "href": "diapositivas/panel.html#breve-desviación-a-mínimos-cuadrados-generalizados-1",
    "title": "Modelos y datos de panel",
    "section": "Breve desviación a mínimos cuadrados generalizados",
    "text": "Breve desviación a mínimos cuadrados generalizados\n\nEs decir, los errores del modelo transformado son iid\nAsí que podemos estimar \\(\\beta\\) eficientemente por MCO al modelo transformado\nEl estimador mínimos cuadrados generalizados (MCG) es\n\n\\[\\hat{\\beta}_{MCG}=(X'\\Omega^{-1}X)^{-1}X'\\Omega^{-1}y\\]\n\nEn la práctica, no conocemos \\(\\Omega\\)\nPodemos proponer un modelo para \\(V(u|x)\\) y obtener un estimador consistente para \\(\\Omega\\)\nEsto da lugar al estimador de mínimos cuadrados generalizados feasibles\n\n\\[\\hat{\\beta}_{MCGF}=(X'\\hat{\\Omega}^{-1}X)^{-1}X'\\hat{\\Omega}^{-1}y\\] # Estimador between"
  },
  {
    "objectID": "diapositivas/panel.html#estimador-between",
    "href": "diapositivas/panel.html#estimador-between",
    "title": "Modelos y datos de panel",
    "section": "Estimador between",
    "text": "Estimador between\n\nAunque es de poca utilidad práctica, el estimador between nos servirá de auxiliar más adelante\nExplota la variación entre unidades\nAl promediar sobre el tiempo recordemos que obtenemos\n\n\\[\n\\begin{aligned}\n\\bar{y}_{i}&=\\alpha_i+ \\bar{x}_i\\beta+\\bar{\\varepsilon}_i\n\\end{aligned}\n\\] - Sumando 0 obtenemos:\n\\[\n\\begin{aligned}\n\\bar{y}_{i}&=\\alpha + \\bar{x}_i\\beta+(\\alpha_i - \\alpha + \\bar{\\varepsilon}_i)\n\\end{aligned}\n\\]\n\nEl estimador between es MCO a la ecuación transformada (con intercepto)\nSolo es consistente si \\(\\bar{x}_i\\) es independiente de el error compuesto \\(\\alpha_i - \\alpha + \\bar{\\varepsilon}_i\\)\nEs equivalente a una regresión de sección cruzada con \\(T=1\\)"
  },
  {
    "objectID": "diapositivas/panel.html#estimador-de-efectos-aleatorios",
    "href": "diapositivas/panel.html#estimador-de-efectos-aleatorios",
    "title": "Modelos y datos de panel",
    "section": "Estimador de efectos aleatorios",
    "text": "Estimador de efectos aleatorios\n\nPodemos escribir el modelo general añadiendo un intercepto no aleatorio \\(\\mu\\) para que los efectos aleatorios puedan tener media cero:\n\n\\[y_{it}=\\mu+x_{it}'\\beta+\\alpha_i+\\varepsilon_{it}=w_{it}'\\delta+\\alpha_i+\\varepsilon_{it}\\] donde \\(w_{it}=[1\\;x_{it}]\\) y \\(\\delta=[\\mu\\;\\beta']'\\)\n\nAsumimos que \\(\\alpha_i\\) y \\(\\varepsilon_i\\) son iid\n\\(\\mu\\) simplemente garantiza que los errores tengan media cero"
  },
  {
    "objectID": "diapositivas/panel.html#estimador-de-efectos-aleatorios-1",
    "href": "diapositivas/panel.html#estimador-de-efectos-aleatorios-1",
    "title": "Modelos y datos de panel",
    "section": "Estimador de efectos aleatorios",
    "text": "Estimador de efectos aleatorios\n\nEl estimador de MCG de \\(\\mu\\) y \\(\\beta\\) puede implementarse como MCO a la ecuación transformada\n\n\\[y_{it}-\\lambda\\bar{y}_i=(1-\\lambda)\\mu+(x_{it}-\\lambda\\bar{x}_i)'\\beta+\\nu_{it}\\] donde \\(\\nu_{it}=(1-\\lambda)\\alpha_i+(\\varepsilon_{it}-\\lambda\\bar{\\varepsilon}_i)\\)\n\nConstruimos un estimador consistente de \\(\\lambda\\) usando estimadores consistentes de \\(\\sigma^2_{\\varepsilon}\\) y \\(\\hat{\\alpha}^2\\)\n\n\\[\\hat{\\lambda}=1-\\frac{\\hat{\\sigma}_{\\varepsilon}}{\\sqrt{T\\hat{\\sigma}_{\\alpha}^2+\\hat{\\sigma}_{\\varepsilon}^2}}\\]"
  },
  {
    "objectID": "diapositivas/panel.html#estimación-de-hatlambda",
    "href": "diapositivas/panel.html#estimación-de-hatlambda",
    "title": "Modelos y datos de panel",
    "section": "Estimación de \\(\\hat{\\lambda}\\)",
    "text": "Estimación de \\(\\hat{\\lambda}\\)\n\nEl software estadístico hace estas estimaciones basadas en las ecuaciones 21.48 y 21.49 en CT\nPara \\(\\sigma_{\\varepsilon}^2\\) usamos el estimador within \\(\\hat{\\beta}_W\\) para obtener:\n\n\\[\\hat{\\sigma_{\\varepsilon}}^2=\\frac{1}{N(T-1)-k}\\sum_i\\sum_t\\left((y_{it}-\\bar{y}_i)-(x_{it}-\\bar{x}_i)'\\hat{\\beta}_W\\right)^2\\] - Para \\(\\sigma_{\\alpha}^2\\), usamos el estimador between \\(\\hat{\\beta}_B\\) para obtener\n\\[\\hat{\\sigma_{\\alpha}}^2=\\frac{1}{N-(k-1)}\\sum_i\\left(\\bar{y}_i-\\hat{\\alpha}_B-\\bar{x}_i'\\hat{\\beta}_B\\right)^2-\\frac{1}{T}\\hat{\\sigma_{\\varepsilon}}^2\\]"
  },
  {
    "objectID": "diapositivas/panel.html#estimador-de-efectos-aleatorios-2",
    "href": "diapositivas/panel.html#estimador-de-efectos-aleatorios-2",
    "title": "Modelos y datos de panel",
    "section": "Estimador de efectos aleatorios",
    "text": "Estimador de efectos aleatorios\n\nEl estimador de efectos aleatorios se logra premultiplicando el modelo \\(y_{it}=w_{it}'\\delta+\\alpha_i+\\varepsilon_{it}\\) por \\(\\Omega^{-1/2}\\) y con algo de álgebra\nSe asume que \\(\\alpha_i\\) y \\(\\varepsilon_{it}\\) son independientes\nHacemos \\(\\Omega_i\\) con entradas \\(\\sigma_{\\alpha}^2+\\sigma_{\\varepsilon}^2\\) en la diagonal principal\nFuera de la diagonal tiene \\(\\sigma_{\\alpha}^2\\)\n\n\\[\\Omega=\\sigma_{\\varepsilon}^2 I_T+\\sigma_{\\alpha}^2ee'\\] donde \\(e=(1,1,\\ldots,1)'\\) es un vector de unos de dimensión \\(T\\times 1\\)\n\nPremultiplicamos entonces el modelo con efectos aleatorios para obtener uno que tenga como matriz de varianzas \\(\\sigma^2_{\\varepsilon}I_T\\), es decir, que sea homoscedástico"
  },
  {
    "objectID": "diapositivas/panel.html#propiedades-del-estimador-de-efectos-aleatorios",
    "href": "diapositivas/panel.html#propiedades-del-estimador-de-efectos-aleatorios",
    "title": "Modelos y datos de panel",
    "section": "Propiedades del estimador de efectos aleatorios",
    "text": "Propiedades del estimador de efectos aleatorios\n\nEl estimador de efectos es consistente si \\(NT\\to\\infty\\), lo cual requiere que \\(N\\to\\infty\\) o \\(T\\to\\infty\\) (o ambos)\nCon paneles cortos, es natural requerir suficientes individuos en la muestra\nPor otro lado, CT proveen dos expresiones para las matrices de varianza en los casos de homocedasticidad (es decir, si \\(\\varepsilon_{it}\\) y \\(\\alpha_i\\) son iid) y para el caso general donde donde se permite heterocedasticidad y autocorrelación de \\(\\varepsilon_{it}\\)"
  },
  {
    "objectID": "diapositivas/panel.html#modelo-agrupado-o-pooled-1",
    "href": "diapositivas/panel.html#modelo-agrupado-o-pooled-1",
    "title": "Modelos y datos de panel",
    "section": "Modelo agrupado o pooled",
    "text": "Modelo agrupado o pooled\n\nSi asumimos coeficientes constantes, como en sección cruzada\n\n\\[y_{it}=\\alpha+x_{it}'\\beta+u_{it}\\] tenemos lo que se conoce como modelo pooled o de coeficientes constantes\n\nEste modelo no explota la estructura del panel\nEl modelo es correcto si los regresores no están correlacionados con el errror\nPodemos estimar consistemente \\(\\beta\\) por MCO\nLos errores estándar deben tomar en cuenta la correlación serial\nSi estamos dispuestos a asumir un modelo pooled, debemos al menos estimar un modelo de MCO con errores robustos agrupados a nivel individual"
  },
  {
    "objectID": "diapositivas/panel.html#qué-estimador-usar",
    "href": "diapositivas/panel.html#qué-estimador-usar",
    "title": "Modelos y datos de panel",
    "section": "¿Qué estimador usar?",
    "text": "¿Qué estimador usar?\n\nSi el modelo asumido es de efectos fijos\n\nSolo el estimador within o de primeras diferencias es consistente\nEstimadores de efectos aleatorios y pooled son inconsistentes\n\nSi el modelo asumido es de efectos aleatorios\n\nEl estimador de efectos aleatorios y el estimador pooled son consistentes, pero el estimador de efectos aleatorios es más eficiente"
  },
  {
    "objectID": "diapositivas/panel.html#prueba-de-hausman-1",
    "href": "diapositivas/panel.html#prueba-de-hausman-1",
    "title": "Modelos y datos de panel",
    "section": "Prueba de Hausman",
    "text": "Prueba de Hausman\n\nUna prueba de Hausman compara dos estimadores, uno consistente bajo la \\(H_0\\) y otro que no lo es\nSi los efectos individuales son efectos fijos correlaciondos con el error, el estimador within es consistente y el de efectos aleatorios no lo es\nLa \\(H_0\\) es que los efectos fijos no están correlacionados con el error\nSi se rechaza la \\(H_0\\), existe evidencia en favor de usar un estimador de efectos fijos"
  },
  {
    "objectID": "diapositivas/muestra-seleccionada.html#mecanismos-de-censura-y-truncamiento",
    "href": "diapositivas/muestra-seleccionada.html#mecanismos-de-censura-y-truncamiento",
    "title": "Modelos de muestras seleccionadas",
    "section": "Mecanismos de censura y truncamiento",
    "text": "Mecanismos de censura y truncamiento\nConsideremos una variable latente \\(y^*\\) que se observa de acuerdo a una regla de observación \\(g(\\cdot)\\)\nLo que observamos es \\(y=g(y^*)\\)"
  },
  {
    "objectID": "diapositivas/muestra-seleccionada.html#censura",
    "href": "diapositivas/muestra-seleccionada.html#censura",
    "title": "Modelos de muestras seleccionadas",
    "section": "Censura",
    "text": "Censura\nSiempre observamos \\(X\\) pero no \\(y\\):\n\nCensura por abajo: \\(y=\\begin{cases}y^* \\quad \\text{si }y^*&gt;L \\\\ L \\quad \\text{si }y^*\\leq L \\end{cases}\\)\nCensura por arriba: \\(y=\\begin{cases}y^* \\quad \\text{si }y^*&lt;U \\\\ U \\quad \\text{si }y^*\\geq U \\end{cases}\\)\n\nEl típico ejemplo de censura se encuentra en los datos top coded, como los de ingreso\nOtro ejemplo es la oferta laboral: en un problema de optimización, las horas óptimas pueden ser negativas, pero entonces observamos la variable censurada en el cero"
  },
  {
    "objectID": "diapositivas/muestra-seleccionada.html#truncamiento",
    "href": "diapositivas/muestra-seleccionada.html#truncamiento",
    "title": "Modelos de muestras seleccionadas",
    "section": "Truncamiento",
    "text": "Truncamiento\nTanto \\(X\\) como \\(y\\) son no observados para ciertos valores de \\(y\\)\n\nTruncamiento por abajo: \\(y=y^*\\) si \\(y^*&gt;L\\) y no osbervada si \\(y^*\\leq L\\)\nTruncamiento por arriba: \\(y=y^*\\) si \\(y^*&lt;U\\) y no osbervada si \\(y^*\\geq U\\)"
  },
  {
    "objectID": "diapositivas/muestra-seleccionada.html#función-de-verosimilitud-censurada",
    "href": "diapositivas/muestra-seleccionada.html#función-de-verosimilitud-censurada",
    "title": "Modelos de muestras seleccionadas",
    "section": "Función de verosimilitud censurada",
    "text": "Función de verosimilitud censurada\nLa censura y el truncamiento cambian la función de verosimilitud de los datos observados\nVerosimilitud censurada (usando censura por abajo)\n\nCuando \\(&gt;L\\), la densidad de \\(y\\) es la misma que la de \\(y^*\\), es decir, \\(f(y|x)=f^*(y|x)\\)\nCuando \\(y=L\\), la densidad es discreta con masa igual a la probabilidad de que \\(y^*\\leq L\\)\n\nEn resumen \\[\nf(y|x)=\n\\begin{cases}\nf^*(y|x) \\quad\\text{si } y&gt;L \\\\\nF^*(L|x)\\quad\\text{si }y=L \\\\\n\\end{cases}\n\\]\nLa densidad es un híbrido entre una función de masa de probabilidad (una densidad propiamente) y una función de densidad acumulada"
  },
  {
    "objectID": "diapositivas/muestra-seleccionada.html#función-de-verosimilitud-censurada-1",
    "href": "diapositivas/muestra-seleccionada.html#función-de-verosimilitud-censurada-1",
    "title": "Modelos de muestras seleccionadas",
    "section": "Función de verosimilitud censurada",
    "text": "Función de verosimilitud censurada\nDefinamos\n\\[\nd=\n\\begin{cases}\n1\\quad\\text{si }y&gt;L \\\\\n0\\quad\\text{si }y=L \\\\\n\\end{cases}\n\\]\nEntonces la densidad condicional debido a la censura es\n\\[f(y|x)=f^*(y|x)^dF^*(L|x)^{1-d}\\]\nY la función de log verosimilitud será \\[\\mathcal{L}_N(\\theta)=\\sum_i\\left(d_i\\ln f^*(y_i|x_i,\\theta) + (1-d_i)\\ln F^*(L_i|x_i,\\theta)\\right)\\]\nNoten que hemos dejado abierta la opción de que \\(L\\) difiera entre individuos, es decir, que \\(L=L_i\\)\nSi la densidad de \\(y^*\\), \\(f^*(y^*|x,\\theta)\\), está bien especificada, \\(\\theta_{MV}\\) es consitente y asintóticamente normal"
  },
  {
    "objectID": "diapositivas/muestra-seleccionada.html#función-de-verosimilitud-truncada",
    "href": "diapositivas/muestra-seleccionada.html#función-de-verosimilitud-truncada",
    "title": "Modelos de muestras seleccionadas",
    "section": "Función de verosimilitud truncada",
    "text": "Función de verosimilitud truncada\nConsideremos el caso de truncamiento por abajo\nNoten que la función de densidad de \\(y\\) es\n\\[\n\\begin{aligned}\nf(y)&=f^*(y|y&gt;L) \\\\\n&=\\frac{f^*(y)}{P(y|y&gt;L)}\\\\\n&=\\frac{f^*(y)}{1-F^*(L)}\n\\end{aligned}\n\\]\nEntonces, la log verosimilitud truncada es:\n\\[\\mathcal{L}_N(\\theta)=\\sum_i\\left(\\ln f^*(y_i|x_i,\\theta)-\\ln(1-F^*(L_i|x_i,\\theta))\\right)\\]"
  },
  {
    "objectID": "diapositivas/muestra-seleccionada.html#modelo-tobit",
    "href": "diapositivas/muestra-seleccionada.html#modelo-tobit",
    "title": "Modelos de muestras seleccionadas",
    "section": "Modelo Tobit",
    "text": "Modelo Tobit\nEs una modelo simple y con supuestos muy fuertes sobre la estructura de la censura\nTobin (1958) lo planteó originalmente como una forma de modelar la compra de bienes durables (muchos hogares gastan 0 en bienes durables)\nConsideramos un proceso con errores normales \\[\n\\begin{aligned}\n&y^*=x'\\beta+\\varepsilon \\\\\n&\\varepsilon\\sim\\mathcal{N}(0,\\sigma^2)\n\\end{aligned}\n\\]\nSupongamos que observamos \\(y\\) de acuerdo a la siguiente regla:\n\\[\ny=\n\\begin{cases}\ny^*\\quad\\text{si }y^*&gt;0 \\\\\n-\\quad\\text{si } y^*\\leq 0\\\\\n\\end{cases}\n\\]"
  },
  {
    "objectID": "diapositivas/muestra-seleccionada.html#modelo-tobit-1",
    "href": "diapositivas/muestra-seleccionada.html#modelo-tobit-1",
    "title": "Modelos de muestras seleccionadas",
    "section": "Modelo Tobit",
    "text": "Modelo Tobit\nCon los errores normales, podemos definir la cdf como \\[\n\\begin{aligned}\nF^*(0)&=P(y^*\\leq0) \\\\\n&=P(x'\\beta+\\varepsilon\\leq 0) \\\\\n&=\\Phi(-x'\\beta/\\sigma) \\\\\n&=1-\\Phi(x'\\beta/\\sigma)\n\\end{aligned}\n\\]\nEsto nos permite definir la densidad censurada como \\[\nf(y)=\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp\\left(-\\frac{1}{2\\sigma^2}(y-x'\\beta)^2\\right)\\right)^d\\left(1-\\Phi\\left(\\frac{x'\\beta}{\\sigma}\\right)\\right)^{1-d}\n\\]\nY entonces la función de log verosimilitud será \\[\n\\begin{aligned}\n\\mathcal{L}_N(\\beta,\\sigma^2)&=\\sum_i\\left( d_i\\left(-\\frac{1}{2}\\ln(\\sigma^2)-\\frac{1}{2}\\ln(2\\pi)-\\frac{1}{2\\sigma^2}(y_i-x'\\beta)^2\\right)+ \\right. \\\\\n&\\left. +(1-d_i)\\ln\\left(1-\\Phi\\left(\\frac{x_i'\\beta}{\\sigma}\\right)\\right) \\right)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "diapositivas/muestra-seleccionada.html#condiciones-de-primer-orden",
    "href": "diapositivas/muestra-seleccionada.html#condiciones-de-primer-orden",
    "title": "Modelos de muestras seleccionadas",
    "section": "Condiciones de primer orden",
    "text": "Condiciones de primer orden\n\\[\\frac{\\partial \\mathcal{L}_N}{\\partial \\beta}=\\sum_i\\frac{1}{\\sigma^2}\\left(d_i(y_i-x_i'\\beta)-(1-d_i)\\frac{\\sigma \\phi_i}{1-\\Phi_i}\\right)x_i=0\\] \\[\\frac{\\partial \\mathcal{L}_N}{\\partial \\sigma^2}=\\sum_i\\left(di\\left(-\\frac{1}{2\\sigma^2}+\\frac{(y_i-x_i'\\beta)^2}{2\\sigma^4}\\right)+(1-d_i)\\left(\\frac{\\phi_ix_i'\\beta}{(1-\\Phi_i)2\\sigma ^3}\\right)\\right)=0\\]\nLa solución se obtiene numéricamente\n\\(\\hat{\\theta}\\) es consistente si la densidad está bien especificada\nEl estimador de MV es asintóticamente normal: \\(\\theta\\stackrel{a}{\\sim}\\mathcal{N}(\\theta,V(\\hat{\\theta}))\\)\nMaddala (1983) y Amemiya (1985) proveen expresiones para la matriz de varianzas"
  },
  {
    "objectID": "diapositivas/muestra-seleccionada.html#nota-sobre-terminología",
    "href": "diapositivas/muestra-seleccionada.html#nota-sobre-terminología",
    "title": "Modelos de muestras seleccionadas",
    "section": "Nota sobre terminología",
    "text": "Nota sobre terminología\nEl modelo Tobit fue plantado inicialmente para un problema de censura en cero\nCuando nos refiramos al Tobit estaremos pensando en la estructura particular que tienen \\(y^*\\) y \\(y\\)\nSi en vez de censura, ocurriera truncamiento, la log verosimilitud sería \\[\\mathcal{L}_N(\\beta,\\sigma^2)=\\sum_i \\left(-\\frac{1}{2}\\ln(\\sigma^2)-\\frac{1}{2}\\ln(2\\pi)-\\frac{1}{2\\sigma^2}(y_i-x'\\beta)^2-\\ln\\left(\\Phi(x_i'\\beta/\\sigma)\\right)\\right)\\]"
  },
  {
    "objectID": "diapositivas/muestra-seleccionada.html#ejemplo-tobit",
    "href": "diapositivas/muestra-seleccionada.html#ejemplo-tobit",
    "title": "Modelos de muestras seleccionadas",
    "section": "Ejemplo: tobit",
    "text": "Ejemplo: tobit\nVeamos un problema típico de economía laboral, la participación de las mujeres en el mercado de trabajo\nUsemos unos datos bastante estudiados mroz.csv\n\n\ndata.part &lt;- read.dta(\"../files/mroz.dta\") \n\ndata.part %&gt;% \n  filter(hours&lt;=3000) %&gt;% \n  ggplot(aes(x=hours)) +\n  geom_histogram()"
  },
  {
    "objectID": "diapositivas/muestra-seleccionada.html#ejemplo-tobit-1",
    "href": "diapositivas/muestra-seleccionada.html#ejemplo-tobit-1",
    "title": "Modelos de muestras seleccionadas",
    "section": "Ejemplo: tobit",
    "text": "Ejemplo: tobit\nEstimemos ahora el tobit, usando tobit del paquete AER\nTambién estimamos MCO a la muestra completa y a la muestra de participantes\n\n#Hacemos MCO ignorando la solución de esquina\nmmco &lt;- lm(hours ~ nwifeinc + educ + exper +\n           expersq + age + kidslt6 + kidsge6,\n         data = data.part)\n\n#Si truncamos la muestra\nmmcot &lt;- lm(hours ~ nwifeinc + educ + exper +\n             expersq + age + kidslt6 + kidsge6,\n           data = filter(data.part,hours&gt;0))\n\n#Usando tobit\nmtobit &lt;- AER::tobit(hours ~ nwifeinc + educ + exper +\n        expersq + age + kidslt6 + kidsge6,\n        left = 0,\n        data = data.part)"
  },
  {
    "objectID": "diapositivas/muestra-seleccionada.html#ejemplo-tobit-2",
    "href": "diapositivas/muestra-seleccionada.html#ejemplo-tobit-2",
    "title": "Modelos de muestras seleccionadas",
    "section": "Ejemplo: tobit",
    "text": "Ejemplo: tobit\n\n\nmodelsummary(list(mmco, mmcot, mtobit))\n\n\n \n\n  \n    \n    \n    tinytable_y7x7q92r5fd3apjg7rdu\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                (1)\n                (2)\n                (3)\n              \n        \n        \n        \n                \n                  (Intercept)\n                  1330.482 \n                  2056.643 \n                  965.305  \n                \n                \n                             \n                  (270.785)\n                  (346.484)\n                  (446.436)\n                \n                \n                  nwifeinc   \n                  -3.447   \n                  0.444    \n                  -8.814   \n                \n                \n                             \n                  (2.544)  \n                  (3.613)  \n                  (4.459)  \n                \n                \n                  educ       \n                  28.761   \n                  -22.788  \n                  80.646   \n                \n                \n                             \n                  (12.955) \n                  (16.434) \n                  (21.583) \n                \n                \n                  exper      \n                  65.673   \n                  47.005   \n                  131.564  \n                \n                \n                             \n                  (9.963)  \n                  (14.556) \n                  (17.279) \n                \n                \n                  expersq    \n                  -0.700   \n                  -0.514   \n                  -1.864   \n                \n                \n                             \n                  (0.325)  \n                  (0.437)  \n                  (0.538)  \n                \n                \n                  age        \n                  -30.512  \n                  -19.664  \n                  -54.405  \n                \n                \n                             \n                  (4.364)  \n                  (5.894)  \n                  (7.419)  \n                \n                \n                  kidslt6    \n                  -442.090 \n                  -305.721 \n                  -894.022 \n                \n                \n                             \n                  (58.847) \n                  (96.450) \n                  (111.878)\n                \n                \n                  kidsge6    \n                  -32.779  \n                  -72.367  \n                  -16.218  \n                \n                \n                             \n                  (23.176) \n                  (30.361) \n                  (38.641) \n                \n                \n                  Num.Obs.   \n                  753      \n                  428      \n                  753      \n                \n                \n                  R2         \n                  0.266    \n                  0.140    \n                           \n                \n                \n                  R2 Adj.    \n                  0.259    \n                  0.126    \n                           \n                \n                \n                  AIC        \n                  12117.1  \n                  6863.2   \n                  7656.2   \n                \n                \n                  BIC        \n                  12158.7  \n                  6899.7   \n                  7697.8   \n                \n                \n                  Log.Lik.   \n                  -6049.534\n                  -3422.581\n                           \n                \n                \n                  F          \n                  38.495   \n                  9.792    \n                           \n                \n                \n                  RMSE       \n                  746.18   \n                  718.92   \n                  953.68"
  },
  {
    "objectID": "diapositivas/muestra-seleccionada.html#modelos-de-muestras-seleccionadas",
    "href": "diapositivas/muestra-seleccionada.html#modelos-de-muestras-seleccionadas",
    "title": "Modelos de muestras seleccionadas",
    "section": "Modelos de muestras seleccionadas",
    "text": "Modelos de muestras seleccionadas\n\n\nLas muestras pueden estar seleccionadas\n\nPor el econometrista\nPor que los agentes escogen participar\n\nEcuación de participación\n\\[\ny_1=\n\\begin{cases}\n1\\quad\\text{si } y_1^*&gt;0\\\\\n0\\quad\\text{si } y_1^*\\leq 0\n\\end{cases}\n\\] Ecuación de resultados\n\\[\ny_2=\n\\begin{cases}\ny_2^*\\quad\\text{si } y_1^*&gt;0\\\\\nNA \\quad\\text{si } y_1^*\\leq 0\n\\end{cases}\n\\]\n\nPor tanto\n\\[\n\\begin{aligned}\ny_1^*=x_1'\\beta_1+\\varepsilon_1 \\\\\ny_2^*=x_2'\\beta_2+\\varepsilon_2 \\\\\n\\end{aligned}\n\\] En el caso en que \\(y_1^*=y_2^*\\), el modelo se colapsa al Tobit"
  },
  {
    "objectID": "diapositivas/muestra-seleccionada.html#modelo-de-heckman",
    "href": "diapositivas/muestra-seleccionada.html#modelo-de-heckman",
    "title": "Modelos de muestras seleccionadas",
    "section": "Modelo de Heckman",
    "text": "Modelo de Heckman\nNo hay consenso de cómo llamarlo\nEl estimador que veremos fue desarrollado por Heckman\nAlgunos otros autores le llaman Tobit de Tipo II o modelo con ecuación de selección\nSupuesto: errores con distribución conjunta normal\n\\[\n\\begin{pmatrix} \\varepsilon_1 \\\\ \\varepsilon_2 \\end{pmatrix}\\sim \\mathcal{N}\\left(A, B \\right)\n\\] \\(A=\\begin{pmatrix} 0 \\\\ 0  \\end{pmatrix}\\)\n\\(B=\\begin{pmatrix} 1 & \\sigma_{12} \\\\ \\sigma_{21} & \\sigma_2^2 \\\\ \\end{pmatrix}\\)\n\\(\\sigma_1^2=1\\) es una normalización"
  },
  {
    "objectID": "diapositivas/muestra-seleccionada.html#resultados-de-la-distribución-normal-conjunta",
    "href": "diapositivas/muestra-seleccionada.html#resultados-de-la-distribución-normal-conjunta",
    "title": "Modelos de muestras seleccionadas",
    "section": "Resultados de la distribución normal conjunta",
    "text": "Resultados de la distribución normal conjunta\nPor nuestro supuesto de normalidad resulta que: \\(\\varepsilon_2=\\sigma_{12}\\varepsilon_1 +\\xi\\)\n\\(\\xi\\) es independiente de \\(\\varepsilon_1\\)\nMedia truncada\n\\[E(y_2|x,y_1^*&gt;0)=x_2'\\beta_2+E(\\varepsilon_2|\\varepsilon_1&gt;-x_1'\\beta_1)\\]\nUsando el resultado de la normalidad de los errores\n\\[\n\\begin{aligned}\nE(y_2|x,y_1^*&gt;0)&=x_2'\\beta_2+E(\\sigma_{12}\\varepsilon_1+\\xi|\\varepsilon_1&gt;-x_1'\\beta_1) \\\\\n&=x_2'\\beta_2+\\sigma_{12}E(\\varepsilon_1|\\varepsilon&gt;-x_1'\\beta_1) \\\\\n&=x_2'\\beta_2+\\sigma_{12}\\lambda(x_1'\\beta_1)\n\\end{aligned}\n\\]\nSimilarmente\n\\[V(y_2|x,y_1^*)=\\sigma_2^2-\\sigma_{12}^2\\lambda(x_1'\\beta_1)(x_1'\\beta_1+\\lambda(x_1'\\beta_1))\\]"
  },
  {
    "objectID": "diapositivas/muestra-seleccionada.html#resultados-de-la-distribución-normal-conjunta-1",
    "href": "diapositivas/muestra-seleccionada.html#resultados-de-la-distribución-normal-conjunta-1",
    "title": "Modelos de muestras seleccionadas",
    "section": "Resultados de la distribución normal conjunta",
    "text": "Resultados de la distribución normal conjunta\nMedia censurada\nCuando \\(y_2=0\\) si \\(y_1^*&lt;0\\)\n\\[\n\\begin{aligned}\nE(y_2|x)&=E_{y_{1}^{*}}(E(y_2|x,y_1^*)) \\\\\n&=P(y_1^*\\leq 0 | x)\\times 0+P(y_1^* &gt; 0 | x)E(y_2^*|X,y_1^*&gt;0) \\\\\n&=0+\\Phi(x_1'\\beta_1)(x_2'\\beta_2+\\sigma_{12}\\lambda(x_1'\\beta_1)) \\\\\n&=\\Phi(x_1'\\beta_1)x_2'\\beta_2 + \\sigma_{12}\\phi(x_1'\\beta_1)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "diapositivas/muestra-seleccionada.html#estimación-por-mv",
    "href": "diapositivas/muestra-seleccionada.html#estimación-por-mv",
    "title": "Modelos de muestras seleccionadas",
    "section": "Estimación por MV",
    "text": "Estimación por MV\nBajo los supuestos de normalidad podemos escribir la verosimilitud como sigue\n\\[L=\\prod_{i=1}^{N}\nP(y_{1i}^*\\leq 0)^{1-y_{1i}}\\left(f(y_{2i}|y_{1i}^*&gt;0)\\times P(y_{1i}^*&gt;0)\\right)^{y_{1i}}\\]\nLa forma final de la log verosimilitud es una fórmula larga que no tiene caso plantear aquí (ver Amemiya, 1985, p. 368)\nLa intuición es que los parámetros de interés, especialmente \\(\\beta_1\\), son estimados de la misma manera en que hemos hecho en otros problemas\nEl vector de parámetros estimados será consistente si la verosimilitud está bien planteada"
  },
  {
    "objectID": "diapositivas/muestra-seleccionada.html#estimador-de-heckman-en-dos-etapas",
    "href": "diapositivas/muestra-seleccionada.html#estimador-de-heckman-en-dos-etapas",
    "title": "Modelos de muestras seleccionadas",
    "section": "Estimador de Heckman en dos etapas",
    "text": "Estimador de Heckman en dos etapas\nAlgunos autores lo conocen como heckit\nConsiste en ver el problema como uno de variable omitida donde la variable omitida es \\(\\lambda(x_{1i}'\\beta_1)\\)\nPodemos pensar el problema en dos etapas\n\nProbit de \\(y_1\\) en \\(x_1\\) usando toda la muestra, dado que asumimos que \\(P(y_1^*&gt;0)=\\Phi(X_1'\\beta_1)\\):\nUsamos \\(\\hat{\\beta}_1\\) para calcular el estimado del inverso de la razón de Mills:\n\\[\\lambda(x_i'\\hat{\\beta}_1)=\\frac{\\phi(x_1'\\hat{\\beta}_1)}{\\Phi(x_1'\\hat{\\beta}_1)}=\\hat{\\lambda}(x_1'\\hat{\\beta}_1)\\]\nUsamos los valores positivos de \\(y_2\\) para estimar la regresión \\[y_{2i}=x_{2i}'\\beta_2+\\sigma_{12}\\lambda(x_{1i}'\\beta_1)+v_i\\]"
  },
  {
    "objectID": "diapositivas/muestra-seleccionada.html#estimador-de-heckman-en-dos-etapas-1",
    "href": "diapositivas/muestra-seleccionada.html#estimador-de-heckman-en-dos-etapas-1",
    "title": "Modelos de muestras seleccionadas",
    "section": "Estimador de Heckman en dos etapas",
    "text": "Estimador de Heckman en dos etapas\nUsando el resultado de la varianza truncada podemos estimar \\[\\sigma_2^2=\\frac{1}{N}\\sum_i(\\hat{v}_i+\\hat{\\sigma}_{12}^2\\hat{\\lambda}_i(x_1'\\beta_1+\\hat{\\lambda}_i))\\] donde \\(\\hat{v}_i\\) son los residuales estimados\nLa correlación de errores puede ser estimada como \\[\\hat{\\rho}=\\hat{\\sigma}_{12}/\\hat{\\sigma}_2\\]\nPor tanto, una prueba de que \\(\\rho=0\\) o \\(\\sigma_{12}=0\\) es una prueba de si los errores están correlacionados y si es necesaria la correción por muestra seleccionada\nPoner atención a la significancia del inverso de la razón de Mills en la segunda etapa"
  },
  {
    "objectID": "diapositivas/muestra-seleccionada.html#ejemplo-heckit",
    "href": "diapositivas/muestra-seleccionada.html#ejemplo-heckit",
    "title": "Modelos de muestras seleccionadas",
    "section": "Ejemplo: heckit",
    "text": "Ejemplo: heckit\nUsamos datos de una muestra de hogares que reportan sus gastos médicos ambulatorios limdep_ambexp.dta\nMuchos hogares tienen cero gastos"
  },
  {
    "objectID": "diapositivas/muestra-seleccionada.html#ejemplo-heckit-1",
    "href": "diapositivas/muestra-seleccionada.html#ejemplo-heckit-1",
    "title": "Modelos de muestras seleccionadas",
    "section": "Ejemplo: heckit",
    "text": "Ejemplo: heckit\nProcedimiento en dos etapas"
  },
  {
    "objectID": "diapositivas/muestra-seleccionada.html#errores-estándar",
    "href": "diapositivas/muestra-seleccionada.html#errores-estándar",
    "title": "Modelos de muestras seleccionadas",
    "section": "Errores estándar",
    "text": "Errores estándar\nPara estimar la varianza hay que considerar dos cosas:\n\nSabemos que \\(V(y_2|x,y_1^*&gt;0)\\) depende de \\(X\\), es decir, la varianza es heterocedástica\nEn la segunda etapa, \\(\\hat{\\lambda}_i\\) no es observado sino estimado\n\nHeckman (1979) provee las fórmulas de los errores correctos (R y otros paquetes ya lo implementan correctamente)"
  },
  {
    "objectID": "diapositivas/muestra-seleccionada.html#efectos-marginales",
    "href": "diapositivas/muestra-seleccionada.html#efectos-marginales",
    "title": "Modelos de muestras seleccionadas",
    "section": "Efectos marginales",
    "text": "Efectos marginales\nDefinamos en un solo vector \\(x=[x_1\\;x_2]\\)\nPodemos reescribir \\(x_1\\beta_1=x'\\gamma_1\\) y \\(x_2'\\beta_2=x'\\gamma_2\\), donde \\(\\gamma_1\\) y \\(\\gamma_2\\) tendrán algunas entradas iguales a cero si \\(x_1\\neq x_2\\)\nAsí, la media truncada es \\[E(y_2|x)=x'\\gamma+\\sigma_{12}\\lambda(x'\\gamma_1)\\]\nY los efectos marginales relevantes son:\n\nProceso sin censura: \\(\\frac{\\partial E(y_2^*|x)}{\\partial x}=\\gamma_2\\)\nTruncado en cero: \\(\\frac{\\partial E(y_2|x, y_1=1)}{\\partial x}=\\gamma_2-\\sigma_{12}\\lambda(x'\\gamma_1)(x'\\gamma_1+\\lambda(x'\\gamma_1))\\)\nCensurado en cero: \\(\\frac{\\partial E(y_2|x)}{\\partial x}=\\gamma_1\\phi(x'\\gamma_1)x'\\gamma_2+\\Phi(x'\\gamma_1)\\gamma_2-\\sigma_{12}x'\\gamma_1\\phi(x'\\gamma_1)\\gamma_1\\)"
  },
  {
    "objectID": "diapositivas/muestra-seleccionada.html#detalles-de-la-estimación",
    "href": "diapositivas/muestra-seleccionada.html#detalles-de-la-estimación",
    "title": "Modelos de muestras seleccionadas",
    "section": "Detalles de la estimación",
    "text": "Detalles de la estimación\nEn teoría, los parámetros del modelo de dos ecuaciones están identificados si los mismos regresores se incluyen en ambas ecuaciones\nPero cuando imponemos errores normales, al hacer \\(x_1=x_2\\) y, recordando que el IRM es casi lineal para un rango grande de su argumento, la segunda ecuación indica que\n\\[E(y_2|y_1^*&gt;0)\\approx x_2'\\beta_2+a+bx_2'\\beta_1\\] Es decir, el modelo está cerca de no estar identificado\nPor tanto, en la práctica, se recomienda que haya una o varias variables que estén en una ecuación y no en la otra\nAlgunos autores llaman a esto restricción de exclusión, término que no me gusta tanto porque se confunde con la misma restricción en el contexto de variables instrumentales"
  },
  {
    "objectID": "diapositivas/muestra-seleccionada.html#resumen",
    "href": "diapositivas/muestra-seleccionada.html#resumen",
    "title": "Modelos de muestras seleccionadas",
    "section": "Resumen",
    "text": "Resumen\nTanto el tobit como el modelo de muestras seleccionadas recaen en fuertes supuestos distribucionales\nEn el modelo de muestras seleccionadas relajamos el supuesto de que el mismo proceso da origen a la censura o truncamiento, y a la variable dependiente\nEl tobit requiere de una interpretación de \\(y^*\\) similar a la de horas deseadas\nEl modelo de muestra seleccionada es más intuitivo para un proceso del tipo:\n\nDecisión de participación\nMargen intensivo"
  },
  {
    "objectID": "diapositivas/index.html",
    "href": "diapositivas/index.html",
    "title": "Diapositivas",
    "section": "",
    "text": "Variables instrumentales\nMétodo generalizado de momentos\nVariables instrumentales en la práctica\nPanel\nErrores estándar\nBootstrap",
    "crumbs": [
      "Diapositivas"
    ]
  },
  {
    "objectID": "diapositivas/did.html#introducción",
    "href": "diapositivas/did.html#introducción",
    "title": "Diferencia en diferencias",
    "section": "Introducción",
    "text": "Introducción\n\n\n\nAngrist & Pischke (2014) describen lo sucedido con el sector bancario en Mississippi durante la Gran Depresión\nEn EUA, la FED tiene 12 bancos regionales y cada uno tiene autonomía para tomar ciertas decisiones de política monetaria\nEn particular, Mississippi tiene una parte del estado bajo el mando del distrito 6 (Atlanta) y la otra mitad en el distrito 8 (San Luis)"
  },
  {
    "objectID": "diapositivas/did.html#diferencia-en-diferencias-1",
    "href": "diapositivas/did.html#diferencia-en-diferencias-1",
    "title": "Diferencia en diferencias",
    "section": "Diferencia en diferencias",
    "text": "Diferencia en diferencias\n\nComo respuesta a las corridas bancarias que caracterizaron la crisis de 1929, los bancos comerciales en Mississippi se vieron expuestos a dos políticas distintas\n\n\\[\nT=\n\\begin{cases}\n1\\quad\\quad \\text{proveer liquidez adicional (distrito 6)} \\\\\n0\\quad\\quad \\text{dar igual o menos liquidez (distrito 8)} \\\\\n\\end{cases}\n\\]\n\nSi estamos interesados en la cantidad de bancos que sobrevivieron y decir algo sobre qué política es más efectiva, ¿qué podemos hacer?\nUna primera respuesta sería contar la diferencia después de la crisis:\n\n\n\n\nDistrito 8\nDistrito 6\nDiferencia\n\n\n\n\n\\(T=0\\)\n\\(T=1\\)\n\n\n\n132 bancos\n121 bancos\n11 bancos\n\n\n\n\nPareciera que la política de proveer liquidez, easy money, causó que quebraran más bancos\nSin embargo, esta comparación claramente ignora las condiciones iniciales"
  },
  {
    "objectID": "diapositivas/did.html#representación-gráfica",
    "href": "diapositivas/did.html#representación-gráfica",
    "title": "Diferencia en diferencias",
    "section": "Representación gráfica",
    "text": "Representación gráfica\n\nGráficamente observamos\n\n\n\nbanks&lt;-read_csv(\"banks_mm.csv\",\n                       locale = locale(encoding = \"latin1\"))\nbanks &lt;- banks %&gt;%\n  filter(month(date) == 7L,\n         mday(date) == 1L) %&gt;%\n  mutate(year = year(date)) %&gt;%\n  select(year,\n         matches(\"bi[ob][68]\")) %&gt;% \n  select(year,bib6,bib8) %&gt;% \n  gather(distrito,banks,bib6:bib8) %&gt;% \n  mutate(treatment=ifelse(distrito==\"bib6\",1,0)) %&gt;% \n  mutate(post=ifelse(year&gt;=1931,1,0))\n\nbanks %&gt;% \n  mutate(banks=ifelse(year==1930 | year==1931,banks,NA)) %&gt;% \n  filter(year &lt;= 1932) %&gt;% \n  ggplot(aes(x=year, y=banks, color=distrito)) +\n  geom_line(size=2) +\n  scale_y_continuous(limits=c(100,180))"
  },
  {
    "objectID": "diapositivas/did.html#representación-gráfica-1",
    "href": "diapositivas/did.html#representación-gráfica-1",
    "title": "Diferencia en diferencias",
    "section": "Representación gráfica",
    "text": "Representación gráfica\n\nDel distrito 8 (no tratado) podemos obtener la pendiente:\n\n\\[m_{NT}=\\frac{Y_{8,post}-Y_{8,pre}}{X_{8,post}-X_{8,pre}}=\\frac{132-165}{1931-1930}=-33\\]\n\nY entonces, podemos encontrar cuál hubiera sido el número de bancos en el distrio 6 (tratado) si hubiera seguido la pendiente del distrito 8:\n\n\\[m_T=\\frac{\\tilde{Y}_{6,post}-Y_{6,pre}}{X_{6,post}-X_{6,pre}}=\\frac{\\tilde{Y}_{6,post}-135}{1931-1930}=-33\\] - Por tanto, \\(\\tilde{Y}_{6,post}=102\\) es el número de bancos que el distrito 6 hubiera tenido si hubiera seguido una tendencia paralela a la del distrito 8"
  },
  {
    "objectID": "diapositivas/did.html#representación-gráfica-2",
    "href": "diapositivas/did.html#representación-gráfica-2",
    "title": "Diferencia en diferencias",
    "section": "Representación gráfica",
    "text": "Representación gráfica\n\nPodemos contruir el contrafactual para el distrito 6 observando la pendiente del distrito 8\n\n\n\nbanks_contrafactual &lt;- banks %&gt;%\n    mutate(banks=ifelse(year==1930 | year==1931,banks,NA)) %&gt;% \n  filter(year &lt;= 1932) %&gt;% \n  mutate(tipo=\"observado\")\n  \nd6_contrafactual &lt;- banks_contrafactual %&gt;% \n  filter(distrito==\"bib6\") %&gt;% \n  mutate(banks=ifelse(year==1931,102,banks),\n         tipo=\"contrafactual\")\n\nbanks_contrafactual &lt;- rbind(banks_contrafactual,d6_contrafactual)\n\ndd_grafica &lt;- banks_contrafactual %&gt;% \n  ggplot(aes(x=year, y=banks, color=distrito, linetype=tipo))+\n  geom_line(size=2) +\n  scale_linetype_manual(values=c(\"dashed\", \"solid\")) +\n  scale_y_continuous(limits=c(100,180)) \n\n\ndd_grafica"
  },
  {
    "objectID": "diapositivas/did.html#diferencia-en-diferencias-2",
    "href": "diapositivas/did.html#diferencia-en-diferencias-2",
    "title": "Diferencia en diferencias",
    "section": "Diferencia en diferencias",
    "text": "Diferencia en diferencias\n\nPodemos dar así una primera definición de lo que es la diferencia en diferencias del número de bancos que sobrevivieron a la Gran Depresión en Mississippi\n\n\\[\n\\begin{aligned}\n\\delta_{DID}&=(Y_{6,post}-Y_{6,pre})-(Y_{8,post}-Y_{8,pre}) \\\\\n&=(Y_{6,1931}-Y_{6,1930})-(Y_{8,1931}-Y_{8,1930}) \\\\\n&=(121-135)-(132-165) \\\\\n&=-14+33 = 19\n\\end{aligned}\n\\]\n\nEl estimador de DID toma en cuenta las diferencias inciales\nEn este caso, el distrito 8 ya tenía más bancos abiertos que el 6 antes de la crisis\nDID construye un contrafactual para las unidades tratadas usando la pendiente de las unidades no tratadas"
  },
  {
    "objectID": "diapositivas/did.html#diferencia-en-diferencias-3",
    "href": "diapositivas/did.html#diferencia-en-diferencias-3",
    "title": "Diferencia en diferencias",
    "section": "Diferencia en diferencias",
    "text": "Diferencia en diferencias\n\nEl supuesto fundamental es el de tendencias comunes, es decir, que en ausencia del tratamiento, el grupo de tratamiento se hubiera comportado igual al grupo de control\nSi hay varios puntos pre intervención, el supuesto de tendencias comunes puede probarse empíricamente"
  },
  {
    "objectID": "diapositivas/did.html#regresión-en-did",
    "href": "diapositivas/did.html#regresión-en-did",
    "title": "Diferencia en diferencias",
    "section": "Regresión en DID",
    "text": "Regresión en DID\n\nEl método puede generalizarse a más periodos de tiempo\nAquí, una regresión nos permite identificar el efecto del tratamiento\nTenemos datos sobre el número de bancos en cada distrito en cada año (1929-1934)\n\n\\[y_{dt}=\\alpha+\\beta T_d+\\gamma POST_t + \\delta_{r,DID}(T_d\\times POST_t)+e_{dt}\\] - \\(T_d\\) es una dummy para los distritos tratados (distrito 6 en este caso)\n\nLes llamamos efectos fijos individuales y sirven para controlar diferencias entre distritos que no cambian en el tiempo\n\\(POST_t\\) es una dummy para los periodos post tratamiento (1931 en adelante)\nAl término \\(T_d\\times POST_t\\) se le conoce como el término de interacción, que es una dummy igual a 1 para los distritos tratados en los años post intervención\n\\(\\delta_{r,DID}\\) es el estimador de DID del efecto del tratamiento"
  },
  {
    "objectID": "diapositivas/did.html#regresión-en-did-1",
    "href": "diapositivas/did.html#regresión-en-did-1",
    "title": "Diferencia en diferencias",
    "section": "Regresión en DID",
    "text": "Regresión en DID\n\nUsemos los datos del archivo banks_mm.csv que se utilizan en el libro\nEs una forma muy básica de datos en panel: cada fila representa un distrito en un periodo de tiempo\nPodemos identificar si cada fila pertenece a un distrito tratado o no o a un periodo posterior a al tratamiento o no\n\n\nbanks\n\n# A tibble: 12 × 5\n    year distrito banks treatment  post\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1  1929 bib6       141         1     0\n 2  1930 bib6       135         1     0\n 3  1931 bib6       121         1     1\n 4  1932 bib6       113         1     1\n 5  1933 bib6       102         1     1\n 6  1934 bib6       102         1     1\n 7  1929 bib8       169         0     0\n 8  1930 bib8       165         0     0\n 9  1931 bib8       132         0     1\n10  1932 bib8       120         0     1\n11  1933 bib8       111         0     1\n12  1934 bib8       109         0     1"
  },
  {
    "objectID": "diapositivas/did.html#regresión-en-did-2",
    "href": "diapositivas/did.html#regresión-en-did-2",
    "title": "Diferencia en diferencias",
    "section": "Regresión en DID",
    "text": "Regresión en DID\n\nEstimemos la regresión que acabamos de motivar usando todos los periodos en el panel\n\n\ndid_bank &lt;- lm(banks ~ treatment + post+ treatment*post,\n               data=banks)\n\nsummary(did_bank)\n\n\nCall:\nlm(formula = banks ~ treatment + post + treatment * post, data = banks)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.000 -7.125  0.000  3.125 14.000 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     167.000      6.190  26.980 3.83e-09 ***\ntreatment       -29.000      8.754  -3.313 0.010652 *  \npost            -49.000      7.581  -6.464 0.000195 ***\ntreatment:post   20.500     10.721   1.912 0.092224 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.754 on 8 degrees of freedom\nMultiple R-squared:  0.8906,    Adjusted R-squared:  0.8496 \nF-statistic: 21.71 on 3 and 8 DF,  p-value: 0.0003369"
  },
  {
    "objectID": "diapositivas/did.html#regresión-en-did-3",
    "href": "diapositivas/did.html#regresión-en-did-3",
    "title": "Diferencia en diferencias",
    "section": "Regresión en DID",
    "text": "Regresión en DID\n\nNoten que si solo usamos dos años, obtenemos exactamente lo que obtendríamos haciendo las diferencias a mano\n\n\\[\\delta_{DiD}=(Y_{6,1931}-Y_{6,1930})-(Y_{8,1931}-Y_{8,1930})=19\\]\n\ndid_bank2 &lt;- lm(banks ~ treatment + post+ treatment*post,\n               data=filter(banks,year==1930 | year==1931))\n\n\nsummary(did_bank2)\n\n\nCall:\nlm(formula = banks ~ treatment + post + treatment * post, data = filter(banks, \n    year == 1930 | year == 1931))\n\nResiduals:\nALL 4 residuals are 0: no residual degrees of freedom!\n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)         165        NaN     NaN      NaN\ntreatment           -30        NaN     NaN      NaN\npost                -33        NaN     NaN      NaN\ntreatment:post       19        NaN     NaN      NaN\n\nResidual standard error: NaN on 0 degrees of freedom\nMultiple R-squared:      1, Adjusted R-squared:    NaN \nF-statistic:   NaN on 3 and 0 DF,  p-value: NA"
  },
  {
    "objectID": "diapositivas/did.html#violación-a-los-supuestos",
    "href": "diapositivas/did.html#violación-a-los-supuestos",
    "title": "Diferencia en diferencias",
    "section": "Violación a los supuestos",
    "text": "Violación a los supuestos\n\nConsideremos cosas que podrían salir mal con respecto al supuesto de tendencias paralelas\nSupongamos que un tratamiento ocurre en el periodo marcado con la línea vertical, por ejemplo, un cambio en una legislación\nEn este ejemplo algo sucedió en Allatsea que produjo un cambio en la trayectoria de mortalidad antes del cambio en la legislación\nEl supuesto de tendencias paralelas sí se sostenía hasta antes de este cambio\nSin embargo, nuestra estrategia de DID atribuiría el efecto al cambio en la legislación cuando en realidad cuando ese cambio ocurrió ya nole pasó nada a Allatsea"
  },
  {
    "objectID": "diapositivas/did.html#violación-a-los-supuestos-1",
    "href": "diapositivas/did.html#violación-a-los-supuestos-1",
    "title": "Diferencia en diferencias",
    "section": "Violación a los supuestos",
    "text": "Violación a los supuestos\n\nEn este segundo ejemplo, el supuesto de tendencias paralelas pre intervención se viola\nAunque en el momento del cambio de la política, la línea de Allatsea es más inclinada, estimar esta relación por DID de nuevo atribuiría a la política diferencias que ya existían antes de la intervención"
  },
  {
    "objectID": "diapositivas/did.html#violación-a-los-supuestos-2",
    "href": "diapositivas/did.html#violación-a-los-supuestos-2",
    "title": "Diferencia en diferencias",
    "section": "Violación a los supuestos",
    "text": "Violación a los supuestos\n\nEn este tercer ejemplo hay tendencias que no son paralelas pre intervención\nSin embargo, después de la intervención, la trayectoria de Allatsea tiene una pendiente claramente más inclinada que antes de la intervención"
  },
  {
    "objectID": "diapositivas/did.html#notación",
    "href": "diapositivas/did.html#notación",
    "title": "Diferencia en diferencias",
    "section": "Notación",
    "text": "Notación\n\nNotación basada en Roth et al.\nEstado de tratamiennto: \\[\nD_i=\n\\begin{cases}\n1\\quad\\quad \\text{si intervenido} \\\\\n0\\quad\\quad \\text{si no} \\\\\n\\end{cases}\n\\]\n\\(N\\) unidades\n\\(y_{it}\\) es la variable de impacto\nNos interesa\n\n\\[\\tau=E(y_{i1}(1)-y_{i1}(0)| D_i=1)\\]\n\nEs el la diferencia en valor esperado de la variable de impacto en las unidades tratadas en los dos regímenes de tratamiento\n\\(y_{i1}(0)\\) es el contrafactual no observado"
  },
  {
    "objectID": "diapositivas/did.html#supuestos",
    "href": "diapositivas/did.html#supuestos",
    "title": "Diferencia en diferencias",
    "section": "Supuestos",
    "text": "Supuestos\nS1. Tendencias paralelas\n\\[E(y_{i1}(0)-y_{i0}(0)|D_i=1)=E(y_{i1}(0)-y_{i0}(0)|D_i=0)\\]\n\nEn estado contrafactual de no tratamiento, los dos grupos tendrían la misma trayectoria\n\nS2. No hay efectos de anticipación\n\\[y_{i0}(0)=y_{i0}(1)\\quad \\forall i \\quad \\text{con}\\quad D_i=1\\]\n\nEn el periodo inicial, el estado de tratamiento no afecta la variable de impacto"
  },
  {
    "objectID": "diapositivas/did.html#estimación-del-contrafactual",
    "href": "diapositivas/did.html#estimación-del-contrafactual",
    "title": "Diferencia en diferencias",
    "section": "Estimación del contrafactual",
    "text": "Estimación del contrafactual\n\nPartiendo del supuesto de tendencias paralelas, podemos despejar el término contrafactual\n\n\\[E(y_{i1}(0)|D_i=1)=E(y_{i0}(0)|D_i=1)+E(y_{i1}(0)-y_{i0}(0)|D_i=0)\\] - Usando el supuesto de no anticipación\n\\[E(y_{i1}(0)|D_i=1)=E(y_{i0}(1)|D_i=1)+E(y_{i1}(0)-y_{i0}(0)|D_i=0)\\]\n\nEs decir:\n\n\\[E(y_{i1}(0)|D_i=1)=E(y_{i0}|D_i=1)+E(y_{i1}-y_{i0}|D_i=0)\\]\n\nPodemos recuperar el contrafactual no observado \\(E(y_{i0}(0)|D_i=1)\\) usando el valor esperado de \\(y\\) en el periodo \\(t=0\\) para el grupo tratado y sumándole el cambio promedio experimentado por el grupo no tratado"
  },
  {
    "objectID": "diapositivas/did.html#efecto-de-tratamiento",
    "href": "diapositivas/did.html#efecto-de-tratamiento",
    "title": "Diferencia en diferencias",
    "section": "Efecto de tratamiento",
    "text": "Efecto de tratamiento\n\nSustituyendo la expresión de \\(E(y_{i0}(0)|D_i=1)\\) en la definición del efecto de tratamiento\n\n\\[\n\\begin{aligned}\n\\tau&=E(y_{i1}(1)-y_{i1}(0)| D_i=1) \\\\\n&=E(y_{i1}| D_i=1)-(E(y_{i0}|D_i=1)+E(y_{i1}-y_{i0}|D_i=0))\n\\end{aligned}\n\\]\n\nReordenando:\n\n\\[\n\\begin{aligned}\n\\tau&=E(y_{i1}-y_{i0}| D_i=1)-E(y_{i1}-y_{i0}|D_i=0)\n\\end{aligned}\n\\]\n\nEs decir, la diferencia de las diferencias"
  },
  {
    "objectID": "diapositivas/did.html#estimación",
    "href": "diapositivas/did.html#estimación",
    "title": "Diferencia en diferencias",
    "section": "Estimación",
    "text": "Estimación\n\nPodemos implementar el estimador con los análogos muestrales\n\n\\[\\hat{\\tau}=(\\bar{y}_{t=1,D=1}-\\bar{y}_{t=0,D=1})-(\\bar{y}_{t=1,D=0}-\\bar{y}_{t=0,D=0})\\]"
  },
  {
    "objectID": "diapositivas/did.html#estimación-1",
    "href": "diapositivas/did.html#estimación-1",
    "title": "Diferencia en diferencias",
    "section": "Estimación",
    "text": "Estimación\n\nSin embargo, como vimos antes, una regresión permite recuperar el efecto de tratamiento\n\n\\[y_{it}=\\alpha+\\gamma D_{i}+\\lambda POST_{t}+\\tau(D_i\\times POST_{t}) + \\varepsilon_{it}\\]\n\\(E(y_{it}|D_i=0, POST_t=0)=\\alpha\\)\n\\(E(y_{it}|D_i=1, POST_t=0)=\\alpha+\\gamma\\)\n\\(E(y_{it}|D_i=0, POST_t=1)=\\alpha+\\lambda\\)\n\\(E(y_{it}|D_i=1, POST_t=1)=\\alpha+\\gamma+\\lambda+\\tau\\)\n\nY por tanto\n\n\\[\n\\begin{aligned}\n(E(y_{it}|D_i=1&, POST_t=1)-E(y_{it}|D_i=1, POST_t=0))-(E(y_{it}|D_i=0, POST_t=1)-E(y_{it}|D_i=0, POST_t=0))=\\\\\n&=((\\alpha+\\gamma+\\lambda+\\tau)-(\\alpha-\\gamma))-((\\alpha+\\lambda)-\\alpha)  \\\\\n&=(\\lambda+\\tau)-(\\lambda)\\\\\n&=\\tau\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "diapositivas/did.html#estimación-2",
    "href": "diapositivas/did.html#estimación-2",
    "title": "Diferencia en diferencias",
    "section": "Estimación",
    "text": "Estimación\n\nLa practica más común es usar efectos fijos\n\n\\[y_{it}=\\alpha_i+\\phi_t + \\mathbf{1}(t=1)D_i\\tau+\\varepsilon_{it}\\] - Donde \\(\\mathbf{1}(t=1)\\) es la función indicadora que toma el valor de 1 cuando la condición \\((t=1)\\) es verdadera\n\nEsta especificación se conoce como efectos fijos en dos vías (!) o two-way fixed effects\n\\(\\alpha_i\\) es una variable dummy que identifica a las observaciones de la unidad \\(i\\)\n\\(\\phi_t\\) es una variable dummy que identifica a cada periodo \\(t\\)\nEn R esto equivale a:\n\n\nlm(y ~ factor(id) + factor(periodo) + D*POST,\n   data = db)"
  },
  {
    "objectID": "diapositivas/did.html#extensión-a-varios-periodos",
    "href": "diapositivas/did.html#extensión-a-varios-periodos",
    "title": "Diferencia en diferencias",
    "section": "Extensión a varios periodos",
    "text": "Extensión a varios periodos\n\nLa especificación de efectos fijos permite extender el caso en el que hay varios periodos pre y post intervención\nSupongamos que el tratamiento ocurre en \\(t_0\\)\n\n\\[y_{it}=\\alpha_i+\\phi_t + \\mathbf{1}(t&gt;t_0)D_i\\tau+\\varepsilon_{it}\\] - También podríamos reducir el problema a uno con solo dos periodos al promediar los valores de la variable dependiente en los periodos pre y post intervención"
  },
  {
    "objectID": "diapositivas/did.html#ejemplo-1",
    "href": "diapositivas/did.html#ejemplo-1",
    "title": "Diferencia en diferencias",
    "section": "Ejemplo",
    "text": "Ejemplo\n\nAccedemos a los datos del paquete causaldata, que incluye los datos empleados en (The Effect)[https://theeffectbook.net/ch-DifferenceinDifference.html]\nImplementamos la estimación por MCO\n\n\n\nod &lt;- causaldata::organ_donations\n\nod &lt;- od %&gt;%\n     mutate(tratado = case_when(State == 'California' ~ 1,\n                                .default = 0),\n            post = case_when(Quarter %in% c('Q32011','Q42011','Q12012') ~ 1,\n                             .default = 0))\n\nm.mco &lt;- lm(Rate ~ factor(tratado) + factor(post) + factor(tratado)*factor(post),\n          data= od)\n\nmodelsummary(models = list(\"MCO\"=m.mco),\n          output = 'gt',\n          stars = c('***' = .01, '**' = .05, '*' = 0.1),\n          gof_map = \"nobs\")\n\n\n\n\n\n\n\n\n\nMCO\n\n\n\n\n(Intercept)\n0.445***\n\n\n\n(0.017)\n\n\nfactor(tratado)1\n-0.174*\n\n\n\n(0.089)\n\n\nfactor(post)1\n0.014\n\n\n\n(0.024)\n\n\nfactor(tratado)1 × factor(post)1\n-0.022\n\n\n\n(0.125)\n\n\nNum.Obs.\n162\n\n\n\n* p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01"
  },
  {
    "objectID": "diapositivas/did.html#ejemplo-2",
    "href": "diapositivas/did.html#ejemplo-2",
    "title": "Diferencia en diferencias",
    "section": "Ejemplo",
    "text": "Ejemplo\n\nLa forma en que se implementa típicamente es por TWFE\nPodemos hacer esto con lm o con un paquete para datos en panel, como plm\n\n\n\nm.twfe &lt;- lm(Rate ~ factor(State) + factor(Quarter) + tratado*post,\n          data= od)\n\n\nm.plm &lt;- plm( Rate ~ factor(State) + factor(Quarter) + tratado*post,\n                 data=od,\n                 model=\"within\",\n                 index = c(\"State\", \"Quarter\"))\n \n\nmodelsummary(models = list(\"MCO\"=m.mco,\n                           \"TWFE\"=m.twfe,\n                           \"TWFE, plm\"=m.plm),\n             coef_map = c('tratado:post',\n                          'factor(tratado)1:factor(post)1'),\n          output = 'gt',\n          stars = c('***' = .01, '**' = .05, '*' = 0.1),\n          gof_map = \"nobs\")\n\n\n\n\n\n\n\n\n\nMCO\nTWFE\nTWFE, plm\n\n\n\n\ntratado:post\n\n-0.022\n-0.022\n\n\n\n\n(0.020)\n(0.020)\n\n\nfactor(tratado)1:factor(post)1\n-0.022\n\n\n\n\n\n(0.125)\n\n\n\n\nNum.Obs.\n162\n162\n162\n\n\n\n* p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n\n\n\n\n\n\n\n\n\nSolo con dos periodos los errores estándar son idénticos"
  },
  {
    "objectID": "diapositivas/did.html#ejemplo-3",
    "href": "diapositivas/did.html#ejemplo-3",
    "title": "Diferencia en diferencias",
    "section": "Ejemplo",
    "text": "Ejemplo\n\nSabemos que cuando usamos datos en panel debemos ajustar la matriz de varianzas por correlación serial\nAfortunadamente tenemos paquetes que nos permite hacer eso de forma automática, como por ejemplo lfe\n\n\n\nm.lfe &lt;- felm(Rate ~ tratado*post | State + Quarter | 0 | State,\n              data = od)\n\nmodelsummary(models = list(\"MCO\"=m.mco,\n                           \"TWFE\"=m.twfe,\n                           \"TWFE, plm\"=m.plm,\n                           \"TWFE, lfe\"=m.lfe),\n             coef_map = c('tratado:post',\n                          'factor(tratado)1:factor(post)1'),\n          output = 'gt',\n          stars = c('***' = .01, '**' = .05, '*' = 0.1),\n          gof_map = \"nobs\")\n\n\n\n\n\n\n\n\n\nMCO\nTWFE\nTWFE, plm\nTWFE, lfe\n\n\n\n\ntratado:post\n\n-0.022\n-0.022\n-0.022***\n\n\n\n\n(0.020)\n(0.020)\n(0.006)\n\n\nfactor(tratado)1:factor(post)1\n-0.022\n\n\n\n\n\n\n(0.125)\n\n\n\n\n\nNum.Obs.\n162\n162\n162\n162\n\n\n\n* p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01"
  },
  {
    "objectID": "diapositivas/did.html#adopción-escalonada-en-la-antiguedad",
    "href": "diapositivas/did.html#adopción-escalonada-en-la-antiguedad",
    "title": "Diferencia en diferencias",
    "section": "Adopción escalonada en la antiguedad",
    "text": "Adopción escalonada en la antiguedad\n\nDID es uno de esos campos tan activos en la investigación que cosas que pensábamos superadas o completamente estudiadas se han modificado en los últimos años\nEn esta sección mostramos el tratamiento que MHE y MM dan a los problemas de adopción escalonada\nLa adopción escalonada ocurre cuando existen varios periodos de tratamiento y algunas unidades van siendo tratadas de forma desfasada\nPor ejemplo, en Méxio las leyes de despenalización del aborto o las que reconocen las uniones entre personas del mismo sexo son buenos ejemplos de tratamientos con adopción escalonada\nHasta los trabajos de Goodman-Bacon (2021) y Chaisemartin & D’Haultfœuille (2020), los problemas con adopción escalonada se abordaban econométricamente de la misma forma que hemos descrito hasta ahora, conocida como efectos fijos en bidireccionales (sí, suena horrible dicha traducción) o two-way fixed effects\nEsta sección presenta el tipo de tratamiento que hoy ya consideramos inapropiado\nMás adelante se presentan alternativas para los problemas de adopción escalonada"
  },
  {
    "objectID": "diapositivas/did.html#edad-legal-para-beber-en-eua",
    "href": "diapositivas/did.html#edad-legal-para-beber-en-eua",
    "title": "Diferencia en diferencias",
    "section": "Edad legal para beber en EUA",
    "text": "Edad legal para beber en EUA\n\nDiferencias en la edad legal para beber en Estados Unidos\n¿Las restricciones a la edad mínima para comprar alcohol tienen un impacto en la mortalidad?\nAlabama redujo la edad legal a 19 en 1975, mientras que, por ejemplo, Arkansas mantuvo la edad en 21\nTenemos datos de mortalidad de 1970 a 1983 para personas de entre 18 y 20 años\nLo que hemos aprendido hasta ahora nos sugiere estimar el impacto por DID comos sigue\n\n\\[y_{st}=\\alpha+\\beta T_s+\\gamma POST_t+\\delta_{DID}(T_s\\times POST_t)+e_{st}\\] - \\(T_s\\) es una dummy igual a 0 para Arkansas en todos los periodos e igual a 1 para Alabama en todos los periodos\n\n\\(POST_t\\) es igual a cero para el periodo 1970-1975 e igual a 1 para el periodo 1976-1983\n\\(T_s\\times POST_t\\) es igual a 1 para las observaciones de Alabama en los años en los que la nueva política ya está en vigor"
  },
  {
    "objectID": "diapositivas/did.html#más-de-un-estado",
    "href": "diapositivas/did.html#más-de-un-estado",
    "title": "Diferencia en diferencias",
    "section": "Más de un estado",
    "text": "Más de un estado\n\n¿Por qué quedarnos solo con la comparación con Arkansas?\nPodemos incluir más unidades que implementen cambios en la política en distintos momentos\nEn vez de \\(POST_t\\) usamos efectos fijos por año\nY en vez de una dummy de tratamiento, incluimos efectos fijos por unidad\nAdemás, en el cambio en la ley podría no ser el mismo\n\nAlgunos estados mueven la edad a 18, otros a 19 y otros a 20\nPodemos definir \\(LEGAL_{st}\\) como la proporción de individuos de entre 18 y 20 años autorizados para beber en el estado \\(s\\) y en el año \\(t\\)\n\n\n\\[y_{st}=\\alpha+\\delta_{DID}LEGAL_{st}+\\sum_k\\beta_k STATE_{ks}+\\sum_j \\gamma_j YEAR_{jt}+e_{st}\\]"
  },
  {
    "objectID": "diapositivas/did.html#estructura-de-datos-en-panel",
    "href": "diapositivas/did.html#estructura-de-datos-en-panel",
    "title": "Diferencia en diferencias",
    "section": "Estructura de datos en panel",
    "text": "Estructura de datos en panel\n\nLos datos que acabamos de describir tienen una estructura de panel\nLa variable de panel es el estado \\(s\\) y la variable de tiempo es el año \\(t\\)\nEn nuestros datos, cada estado se encuentra presente en varios años, y para cada estado y año sabemos la mortalidad y la fracción de personas de 18 a 20 años que pueden beber\nEfectos fijos \\(STATE_{ks}\\): diferencias entre estados que no cambian con el tiempo\nEfectos año \\(YEAR_{jt}\\): factores que afectan a todas las unidades por igual en un momento del tiempo"
  },
  {
    "objectID": "diapositivas/did.html#interpretación",
    "href": "diapositivas/did.html#interpretación",
    "title": "Diferencia en diferencias",
    "section": "Interpretación",
    "text": "Interpretación\n\n\n\nMuertes por 100,000 habs.:\n\\(\\hat{\\delta}_{DID}\\)\n\n\n\n\nTodas\n10.80\n\n\n\n(4.59)\n\n\nAccidentes en vehículos\n7.59\n\n\n\n(2.50)\n\n\nSuicidio\n0.59\n\n\n\n(0.59)\n\n\nCausas internas\n1.33\n\n\n\n(1.59)\n\n\n\n\nLa interpretación de los resultados es directa\nEl acceso a alcohol causa un incremento de casi 11 muertes adicionales por cada 100,000 habitantes y este efecto es estadísticamente significativo\nNo hay efectos donde no esperaríamos tenerlos"
  },
  {
    "objectID": "diapositivas/did.html#qué-hace-el-estimador-de-twfe",
    "href": "diapositivas/did.html#qué-hace-el-estimador-de-twfe",
    "title": "Diferencia en diferencias",
    "section": "¿Qué hace el estimador de TWFE?",
    "text": "¿Qué hace el estimador de TWFE?\nConsideremos el siguiente problema de adopción escalonada\n\nObservamos \\(T\\) periodos\n\\(k\\) denota la unidad tratada temprano\n\\(l\\) denota la unidad tratada tarde\nDenotamos \\(\\bar{D}_k\\) al tiempo que \\(k\\) pasa tratado\nSimilarmente, \\(\\bar{D}_l\\)\n\\(n_k\\) es el número de unidades tratadas temprano\n\\(n_l\\) es el número de unidades tratadas tarde\n\\(n_U\\) es el número de unidades nunca tratadas\nPara cualesquiera dos grupos \\(a\\) y \\(b\\), \\(a_{ab}=\\frac{n_a}{n_a+n_b}\\)"
  },
  {
    "objectID": "diapositivas/did.html#qué-hace-el-estimador-de-twfe-1",
    "href": "diapositivas/did.html#qué-hace-el-estimador-de-twfe-1",
    "title": "Diferencia en diferencias",
    "section": "¿Qué hace el estimador de TWFE?",
    "text": "¿Qué hace el estimador de TWFE?"
  },
  {
    "objectID": "diapositivas/did.html#qué-hace-el-estimador-de-twfe-2",
    "href": "diapositivas/did.html#qué-hace-el-estimador-de-twfe-2",
    "title": "Diferencia en diferencias",
    "section": "¿Qué hace el estimador de TWFE?",
    "text": "¿Qué hace el estimador de TWFE?\nTenemos cuatro posibles comparaciones \\(2x2\\)\n\nLas comparaciones con las unidades nunca tratadas:\n\n\\[\\hat{\\beta}_{jU}=\\left(\\bar{y}_j^{POST(j)}-\\bar{y}_j^{PRE(j)}\\right)-\\left(\\bar{y}_U^{POST(j)}-\\bar{y}_U^{PRE(j)}\\right),\\;j=\\{k,l\\}\\]\n\nLa comparación entre \\(k\\) y \\(l\\) cuando \\(k\\) es tratada, usando los periodos antes de que \\(l\\) sea tratada\n\n\\[\\hat{\\beta}^{k}_{kl}=\\left(\\bar{y}_k^{MID(k,l)}-\\bar{y}_k^{PRE(k)}\\right)-\\left(\\bar{y}_l^{MID(k,l)}-\\bar{y}_l^{PRE(k)}\\right)\\]\n\nLa comparación entre \\(l\\) y \\(k\\) cuando \\(l\\) es tratada, usando los periodos después de que \\(k\\) fue tratada\n\n\\[\\hat{\\beta}^{l}_{kl}=\\left(\\bar{y}_l^{POST(l)}-\\bar{y}_l^{MID(k,l)}\\right)-\\left(\\bar{y}_k^{POST(l)}-\\bar{y}_k^{MID(k,l)}\\right)\\]"
  },
  {
    "objectID": "diapositivas/did.html#qué-hace-el-estimador-de-twfe-3",
    "href": "diapositivas/did.html#qué-hace-el-estimador-de-twfe-3",
    "title": "Diferencia en diferencias",
    "section": "¿Qué hace el estimador de TWFE?",
    "text": "¿Qué hace el estimador de TWFE?"
  },
  {
    "objectID": "diapositivas/did.html#qué-hace-el-estimador-de-twfe-4",
    "href": "diapositivas/did.html#qué-hace-el-estimador-de-twfe-4",
    "title": "Diferencia en diferencias",
    "section": "¿Qué hace el estimador de TWFE?",
    "text": "¿Qué hace el estimador de TWFE?\nDenotamos \\(D_{it}=1\\) cuando la unidad \\(i\\) es tratada en el periodo \\(0\\)\nPor el teorema de Frisch-Waugh-Lovell sabemos que podemos purgar de efectos fijos a la variable \\(D_{it}\\)\nEs decir, podemos estimar por MCO:\n\\[D_{it}=\\sum_t \\gamma_t P_t + \\sum_i \\pi S_i + \\varepsilon_{it}\\]\ndonde \\(P_t\\) son dummies temporales y \\(S_i\\) son dummies individuales\nLuego obtenemos\n\\[\\tilde{D}_{it}=D_{it}-\\hat{D}_{it}\\] Y entonces el estimador de TWFE puede escribirse como el coeficiente de una regresión bivariada:\n\\[\\hat{\\beta}_{DD}=\\frac{\\hat{C}(y_{it}, \\tilde{D}_{it})}{\\hat{V}^D}\\] \\(\\hat{V}^D\\) es la varianza de la variable de tratamiento purgada de efectos fijos"
  },
  {
    "objectID": "diapositivas/did.html#qué-hace-el-estimador-de-twfe-5",
    "href": "diapositivas/did.html#qué-hace-el-estimador-de-twfe-5",
    "title": "Diferencia en diferencias",
    "section": "¿Qué hace el estimador de TWFE?",
    "text": "¿Qué hace el estimador de TWFE?\nGoodman-Bacon (2021) muestra que el estimador de TWFE puede descomponerse como:\n\\[\\hat{\\beta}_{DD}=s_{kU}\\hat{\\beta}_{kU}+s_{lU}\\hat{\\beta}_{lU}+s_{kl}^k\\hat{\\beta}_{kl}^k+s_{kl}^l\\hat{\\beta}_{kl}^l\\]"
  },
  {
    "objectID": "diapositivas/did.html#qué-son-los-pesos",
    "href": "diapositivas/did.html#qué-son-los-pesos",
    "title": "Diferencia en diferencias",
    "section": "¿Qué son los pesos?",
    "text": "¿Qué son los pesos?\nGoodman-Bacon (2021) muestra que los pesos están dados por:\n\n\\(s_{kU}=\\frac{(n_k+n_U)^2\\hat{V}_{kU}^D}{\\hat{V}^D}\\)\n\\(s_{lU}=\\frac{(n_l+n_U)^2\\hat{V}_{lU}^D}{\\hat{V}^D}\\)\n\\(s_{kl}^k=\\frac{[(n_k+n_l)(1-\\bar{D}_l)]^2\\hat{V}_{kl}^{D,k}}{\\hat{V}^D}\\)\n\\(s_{kl}^l=\\frac{[(n_k+n_l)\\bar{D}_k]^2\\hat{V}_{kl}^{D,l}}{\\hat{V}^D}\\)"
  },
  {
    "objectID": "diapositivas/did.html#qué-son-los-pesos-1",
    "href": "diapositivas/did.html#qué-son-los-pesos-1",
    "title": "Diferencia en diferencias",
    "section": "¿Qué son los pesos?",
    "text": "¿Qué son los pesos?\nLas varianzas del tratamiento en la submuestra tienen las siguientes formas:\n\n\\(s_{kU}=\\frac{(n_k+n_U)^2\\hat{V}_{kU}^D}{\\hat{V}^D}=\\frac{(n_k+n_U)^2n_{kU}(1-n_{kU})\\bar{D}_k(1-\\bar{D}_k)}{\\hat{V}^D}\\)\n\\(s_{lU}=\\frac{(n_l+n_U)^2\\hat{V}_{lU}^D}{\\hat{V}^D}=\\frac{(n_l+n_U)^2n_{lU}(1-n_{lU})\\bar{D}_l(1-\\bar{D}_l)}{\\hat{V}^D}\\)\n\\(s_{kl}^k=\\frac{[(n_k+n_l)(1-\\bar{D}_l)]^2\\hat{V}_{kl}^{D,k}}{\\hat{V}^D}=\\frac{[(n_k+n_l)(1-\\bar{D}_l)]^2n_{kl}(1-n_{kl}) \\frac{\\bar{D}_k-\\bar{D}_l}{1-\\bar{D}_l}\\frac{1-\\bar{D}_k}{1-\\bar{D}_l}}{\\hat{V}^D}\\)\n\\(s_{kl}^l=\\frac{[(n_k+n_l)\\bar{D}_k]^2\\hat{V}_{kl}^{D,l}}{\\hat{V}^D}=\\frac{[(n_k+n_l)\\bar{D}_k]^2n_{kl}(1-n_{kl})\\frac{\\bar{D}_l}{\\bar{D}_k}\\frac{\\bar{D}_k-\\bar{D}_l}{\\bar{D}_k}}{\\hat{V}^D}\\)"
  },
  {
    "objectID": "diapositivas/did.html#qué-son-los-pesos-2",
    "href": "diapositivas/did.html#qué-son-los-pesos-2",
    "title": "Diferencia en diferencias",
    "section": "¿Qué son los pesos?",
    "text": "¿Qué son los pesos?\nPongamos atención a uno de los pesos, \\(s_{kU}\\) para ver la intuición\nVimos que:\n\\[s_{kU}=\\frac{(n_k+n_U)^2\\hat{V}_{kU}^D}{\\hat{V}^D}=\\frac{(n_k+n_U)^2n_{kU}(1-n_{kU})\\bar{D}_k(1-\\bar{D}_k)}{\\hat{V}^D}\\]\nPrimero, noten que el peso depende de \\((n_k+n_u)^2\\), es decir, la parte de la submuestra que se usa para estimar cada comparación \\(2x2\\)"
  },
  {
    "objectID": "diapositivas/did.html#qué-son-los-pesos-3",
    "href": "diapositivas/did.html#qué-son-los-pesos-3",
    "title": "Diferencia en diferencias",
    "section": "¿Qué son los pesos?",
    "text": "¿Qué son los pesos?\nAhora veamos la varianza que entra en este peso:\n\\[\\hat{V}_{kU}^D=n_{kU}(1-n_{kU})\\bar{D}_k(1-\\bar{D}_k)\\] Una primer cosa que entra en juego es \\(n_{kU}=\\frac{n_k}{n_k+n_U}\\), el peso relativo del grupo tratado temprano en la submuestra\nRecordemos que \\(\\bar{D}_k\\) indica la fracción del tiempo del panel que \\(k\\) permanece tratado\nDado que \\(D_{it}\\) es binaria, \\(\\bar{D}_k(1-\\bar{D}_k)\\) es la varianza de \\(D_{it}\\)\n¿Qué es lo más grande que puede ser \\(\\bar{D}_k(1-\\bar{D}_k)\\)?\nClaramente esto es a lo más 0.5, es decir, la contribución se maximiza cuando el tratamiento ocurre lo más cercano a la mitad del panel"
  },
  {
    "objectID": "diapositivas/did.html#qué-son-los-pesos-4",
    "href": "diapositivas/did.html#qué-son-los-pesos-4",
    "title": "Diferencia en diferencias",
    "section": "¿Qué son los pesos?",
    "text": "¿Qué son los pesos?\nEn resumen, los pesos están dados por\n\\[pesos = \\frac{(\\text{participación de la submuestra})^2(\\text{varianza del tratamiento purgada de efectos fijos en la submuestra})}{\\text{varianza del tratamiento purgada de efectos fijos en la muestra completa}}\\]"
  },
  {
    "objectID": "diapositivas/did.html#qué-nos-dice-la-descomposición",
    "href": "diapositivas/did.html#qué-nos-dice-la-descomposición",
    "title": "Diferencia en diferencias",
    "section": "¿Qué nos dice la descomposición?",
    "text": "¿Qué nos dice la descomposición?\nTodos los grupos funcionan como controles (algunas veces)\nLos pesos en cada comparación \\(2x2\\) dependen del tamaño de la muestra usada para estimarlos y del tiempo que las unidades permanecen tratadas\nEl efecto estimado se modificaría solo por agregar más años al final del panel\nLos términos que dominen pueden incluso cambiar el signo del efecto estimado\nGoodman-Bacon (2021) muestra que, en general usar TWFE solo recupera un efecto de tratamiento bajo supuestos muy fuertes, el más crucial, bajo efectos de tratamiento que no varían en el tiempo (no hay efectos dinámicos)"
  },
  {
    "objectID": "diapositivas/did.html#estimador-de-callaway-santanna-1",
    "href": "diapositivas/did.html#estimador-de-callaway-santanna-1",
    "title": "Diferencia en diferencias",
    "section": "Estimador de Callaway & Sant’Anna",
    "text": "Estimador de Callaway & Sant’Anna\nEl estimador de Callaway & Sant’Anna (2021) es una forma muy sencilla de explotar la información del desfase del tratamiento\nAdemás permite una presentación muy intuitiva de los efectos de tratamiento\nEsencialmente consiste en definir una serie de cohortes de unidades y estimar los impactos para cada cohorte\nLuego podemos agregar los impactos por cohorte o presentar la dinámica de los impactos de forma gráfica"
  },
  {
    "objectID": "diapositivas/did.html#definiciones",
    "href": "diapositivas/did.html#definiciones",
    "title": "Diferencia en diferencias",
    "section": "Definiciones",
    "text": "Definiciones\n\n\\(G_i=\\min\\{t:D_{it}=1\\}\\) es el primer periodo en que \\(i\\) es tratado\n\\(G_i=\\infty\\) indoca que \\(i\\) nunca es tratado\n\\(t=1,\\ldots,T\\) son los periodos\n\\(D_{it}=1\\;\\forall\\;t\\geq G_i\\) indica tratamiento absorbente\n\nDefinamos el efecto del tratamiento al periodo \\(t\\) para el cohorte que empezó a ser tratado en \\(g\\):\n\\[ATT(g,t)=E(y_{it}(g)-y_{it}(\\infty)|G_i=g)\\] Por ejemplo, \\(ATT(2014,2016)\\) es el efecto del tratamiento en 2016 para aquellos tratados empezando en 2014"
  },
  {
    "objectID": "diapositivas/did.html#supuestos-1",
    "href": "diapositivas/did.html#supuestos-1",
    "title": "Diferencia en diferencias",
    "section": "Supuestos",
    "text": "Supuestos\nTendencias paralelas en el contexto escalonado\n\\[E(y_{it}(\\infty)-y_{it'}(\\infty)|G_i=g)=E(y_{it}(\\infty)-y_{it'}(\\infty)|G_i=g')\\quad\\forall\\;t\\neq t',\\; g\\neq q'\\]\nEste supuesto indica que en el estado contrafactual de nunca haber sido tratados, dos grupos tratados en dos momentos diferentes \\(g\\) y \\(g'\\) habrían tenido una trayectoria igual entre los periodos \\(t\\) y \\(t'\\)\nTendencias paralelas en el contexto escalonado (a)\nModifica el supuesto anterior pero \\(\\forall \\;t,t'\\geq g_{min}-1\\) donde \\(g_{min}=\\min G\\) es el primer periodo en donde alguna unidad es tratada (Callaway y Sant’Anna solo usan este supuesto más débil)"
  },
  {
    "objectID": "diapositivas/did.html#supuestos-2",
    "href": "diapositivas/did.html#supuestos-2",
    "title": "Diferencia en diferencias",
    "section": "Supuestos",
    "text": "Supuestos\nNo anticipación en el contexto escalonado\n\\[y_{it}(g)=y_{it}(\\infty)\\quad \\forall\\;i \\;\\text{y}\\;\\forall\\;t&lt;g\\]"
  },
  {
    "objectID": "diapositivas/did.html#estimador-de-callaway-santanna-2",
    "href": "diapositivas/did.html#estimador-de-callaway-santanna-2",
    "title": "Diferencia en diferencias",
    "section": "Estimador de Callaway & Sant’Anna",
    "text": "Estimador de Callaway & Sant’Anna\nEl estimador de Callaway & Sant’Anna (2021) se define como:\n\\[ATT(g,t)=E(y_{it}-y_{i,g-1}|G_i=g)-E(y_{it}-y_{i,g-1}|G_i\\in\\mathcal{G}_{comp})\\] Queremos comparar el valor de la variable de impacto entre el periodo \\(g-1\\) y el \\(t\\) para el grupo que fue tratado en \\(g\\) con algún grupo de comparación denominado \\(\\mathcal{G}_{comp}\\)\nSustituyendo por el análogo muestral:\n\\[\\hat{ATT}(g,t)=\\frac{1}{N_g}\\sum_{i:\\;G_i=g}(y_{it}-y_{i,g-1})-\\frac{1}{N_{\\mathcal{G}_{comp}}}\\sum_{i\\;G_i\\in \\mathcal{G}_{comp}}(y_{it}-y_{i,g-1})\\]\n¿Qué es \\(\\mathcal{G}_{comp}\\)?\n\n\\(\\mathcal{G}_{comp}=\\{\\infty \\}\\): aquellas unidades nunca tratadas\n\\(\\mathcal{G}_{comp}=\\{g':g'&gt;t \\}\\): aquellas unidades aún no tratadas al periodo \\(t\\)"
  },
  {
    "objectID": "diapositivas/did.html#reportar-resultados",
    "href": "diapositivas/did.html#reportar-resultados",
    "title": "Diferencia en diferencias",
    "section": "Reportar resultados",
    "text": "Reportar resultados\nSi tenemos pocos periodos y cohortes puede ser práctico reportar todos los \\(\\hat{ATT}(g,t)\\)\nTambién podemos construir medidas agregadas:\n\\[\\theta = \\sum_{g\\in\\mathcal{G}}\\sum_{t=2}^{\\mathcal{T}}w(g,t)ATT(g,t)\\]"
  },
  {
    "objectID": "diapositivas/did.html#formas-de-agregación",
    "href": "diapositivas/did.html#formas-de-agregación",
    "title": "Diferencia en diferencias",
    "section": "Formas de agregación",
    "text": "Formas de agregación\nLa notación usada en el artículo es densa pero en resumen, algunas agregaciones posibles son:\nEfecto de tratamiento por duración de exposición\n\nPodemos calcular \\(\\theta(e)\\), el efecto promedio \\(e\\) periodos después del tratamiento, donde \\(e=t-g\\) es el tiempo que ocurre desde que se realizó el tratamiento\nPodemos hacer un gráfico de \\(\\theta(e)\\)\n\nEfecto de tratamiento por grupo o cohorte\n\nLo llamamos \\(\\theta(\\tilde{g})\\) y nos permite conocer si el efecto del tratamiento es distinto para cohortes que fueron tratados temprano o tarde\n\nEfecto de tratamiento acumulado\n\nLo llamamos \\(\\theta(\\tilde{t})\\) y lo definimos como el efecto promedio por participar en el tratamiento hasta una fecha dada \\(\\tilde{t}\\)"
  },
  {
    "objectID": "diapositivas/did.html#ventajas-del-estimador",
    "href": "diapositivas/did.html#ventajas-del-estimador",
    "title": "Diferencia en diferencias",
    "section": "Ventajas del estimador",
    "text": "Ventajas del estimador\nToma en cuenta la heterogeneidad de los efectos de tratamiento\nLos pesos son especificados por el investigador y no mecánicamente con en TWFE\nHace explícito el grupo de comparación (nunca tratados o aún no tratados)"
  },
  {
    "objectID": "diapositivas/did.html#referencias-recomendadas-1",
    "href": "diapositivas/did.html#referencias-recomendadas-1",
    "title": "Diferencia en diferencias",
    "section": "Referencias recomendadas",
    "text": "Referencias recomendadas\n\nRoth, J., Sant’Anna, P. H., Bilinski, A., & Poe, J. (2023). What’s trending in difference-in-differences? A synthesis of the recent econometrics literature. Journal of Econometrics, 235(2), 2218-2244.\nCallaway, B., & Sant’Anna, P. H. (2021). Difference-in-differences with multiple time periods. Journal of Econometrics, 225(2), 200-230."
  },
  {
    "objectID": "diapositivas/bootstrap.html#introducción-a-bootstrap",
    "href": "diapositivas/bootstrap.html#introducción-a-bootstrap",
    "title": "Bootstrap",
    "section": "Introducción a bootstrap",
    "text": "Introducción a bootstrap\n\nA veces es difícil encontrar una expresión analítica de los errores estándar\nLa idea de las técnicas bootstrap es consutrir una distribución empírica del estimador de interés\nUna muestra bootstrap es una muestra tomada de los mismos datos\nEn las rutinas para errores bootstrap, pensamos en \\(\\{(y_1,x_1),\\ldots,(y_N,X_n)\\}\\) como la población\nUna muestra bootstrap es una muestra de tamaño \\(N\\) tomada de la muestra original\nEl procedimiento bootstrap más usado es el bootstrap no paramétrico o boostrap en parejas (nos enfocaremos en este tipo de bootstrap en el curso)\nLa idea es remuestrear la pareja completa \\((y_i,x_i)\\)"
  },
  {
    "objectID": "diapositivas/bootstrap.html#algoritmo-para-errores-estándar-bootstrap",
    "href": "diapositivas/bootstrap.html#algoritmo-para-errores-estándar-bootstrap",
    "title": "Bootstrap",
    "section": "Algoritmo para errores estándar bootstrap",
    "text": "Algoritmo para errores estándar bootstrap\n\nDada una muestra \\(W_1,\\ldots,W_N\\), obtener una muestra de tamaño \\(N\\), remuestreando de la muestra original con reemplazo\nCalcular el estadístico \\(\\hat{\\theta}_b\\) usado con la muestra bootstrap (coeficiente de regresión, diferencia de medias, función de coeficientes)\nRepetir los pasos 1 y 2 \\(B\\) veces, donde \\(B\\) es lo suficientemente grande (usualmente 1000 es suficiente)\nUsar las \\(B\\) repeticiones para obtener el error estándar del estadístico como la raíz cuadrada de \\(s^2_{\\hat{\\theta},B}\\):\n\n\\[s^2_{\\hat{\\theta},B}=\\frac{1}{B-1}\\sum_{b=1}^B(\\hat{\\theta}_{b}-\\bar{\\hat{\\theta}})^2\\] donde \\(\\bar{\\hat{\\theta}}=\\frac{1}{B}\\sum_{b=1}^B\\hat{\\theta}_b\\)"
  },
  {
    "objectID": "diapositivas/bootstrap.html#cómo-hacer-remuestreo-en-r",
    "href": "diapositivas/bootstrap.html#cómo-hacer-remuestreo-en-r",
    "title": "Bootstrap",
    "section": "¿Cómo hacer remuestreo en R?",
    "text": "¿Cómo hacer remuestreo en R?\n\nset.seed(927)\n\ndata.ingresos &lt;- read.csv(\"./ingresos_iv.csv\")\n\nobs &lt;- nrow(data.ingresos)\nobs\n\n[1] 3010\n\n#En la muestra original\nmean(data.ingresos$lwage)\n\n[1] 6.261832"
  },
  {
    "objectID": "diapositivas/bootstrap.html#cómo-hacer-remuestreo-en-r-1",
    "href": "diapositivas/bootstrap.html#cómo-hacer-remuestreo-en-r-1",
    "title": "Bootstrap",
    "section": "¿Cómo hacer remuestreo en R?",
    "text": "¿Cómo hacer remuestreo en R?\n\n#Una muestra bootstrap\ndata.b &lt;-data.ingresos[sample(nrow(data.ingresos),obs, replace = TRUE),]\n\nmean(data.b$lwage)\n\n[1] 6.261918\n\n#Otra muestra bootstrap\ndata.b &lt;-data.ingresos[sample(nrow(data.ingresos),obs, replace = TRUE),]\n\nmean(data.b$lwage)\n\n[1] 6.262326"
  },
  {
    "objectID": "diapositivas/bootstrap.html#aplicaciones-comunes-de-bootstrap",
    "href": "diapositivas/bootstrap.html#aplicaciones-comunes-de-bootstrap",
    "title": "Bootstrap",
    "section": "Aplicaciones comunes de bootstrap",
    "text": "Aplicaciones comunes de bootstrap\n\nMétodos de varias etapas (por ejemplo, el estimador de dos etapas de Heckman)\nFunciones de estimadores (aunque aquí el método Delta también podría ser usado)\nDatos agrupados con pocos grupos (remuestrear grupos en vez de individuos)\nEl consejo práctico es usar resultados teóricos cuando se puede (por ejemplo, las matrices robustas descritas antes)\nPensemos siempre en la estructura de los datos antes de hacer boostrap\nUsar una semilla siempre para poder reproducir sus resultados"
  },
  {
    "objectID": "diapositivas/bootstrap.html#bootstrap-salvaje",
    "href": "diapositivas/bootstrap.html#bootstrap-salvaje",
    "title": "Bootstrap",
    "section": "Bootstrap salvaje",
    "text": "Bootstrap salvaje\n\nEn presencia de heterocedasticidad se prefiere usar bootstrap salvaje (wild bootstrap) (MacKinnon, 2012)\nPropuesto originalmente por Liu (1988), cada muestra bootstrap tiene la siguiente forma:\n\n\\[y_i^*=X_i\\hat{\\beta}+f(\\hat{u}_i)v_i^*\\] - Noten que mantiene fijos los \\(X_i\\) en cada muestra bootstrap\n\nUna especificación comúnmente usada es hacer es \\(f(\\hat{u}_i)=\\hat{u}_i\\) y \\[v_i^*=\\begin{cases} 1 \\quad\\text{con probabilidad 0.5} \\\\ -1 \\quad\\text{con probabilidad 0.5} \\end{cases}\\]\n\\(\\hat{\\beta}\\) y \\(\\hat{u}_i\\) son estimados con la muestra original"
  },
  {
    "objectID": "diapositivas/bootstrap.html#bootstrap-salvaje-1",
    "href": "diapositivas/bootstrap.html#bootstrap-salvaje-1",
    "title": "Bootstrap",
    "section": "Bootstrap salvaje",
    "text": "Bootstrap salvaje\n\nEn cada una de las \\(B\\) muestras bootstrap, mantenemos a los mismos individuos (no hay remuestreo)\nTendremos \\(B\\) muestras bootstrap, pero ahora la aleatoriedad viene por \\(f(\\hat{u}_i)v_i^*\\)\nPueden usarse otras funciones más complicadas para \\(f(\\hat{u}_i)\\)\nLa ventaja de este método es que conserva la relación entre las varianzas residuales y las \\(X_i\\) observadas en los datos originales\nDavidson & Flachaire (2008) utilizan simulaciones para mostrar que con esta forma para \\(f(\\hat{u}_i)v_i^*\\) la inferencia es más confiable que con otras especificaciones"
  },
  {
    "objectID": "diapositivas/bootstrap.html#refinamiento-asintótico",
    "href": "diapositivas/bootstrap.html#refinamiento-asintótico",
    "title": "Bootstrap",
    "section": "Refinamiento asintótico",
    "text": "Refinamiento asintótico\n\nUna aplicación de las técnicas bootstrap es el refinamiento asintótico de la prueba \\(t\\) de coeficientes de regresión\nSupongamos que \\(H_0:\\quad \\beta=0\\) y trabajamos con un nivel \\(\\alpha\\)\nEn cada repetición bootstrap el estadístico calculado es \\(t_b\\)\nOrdenamos los \\(B\\) estadísticos obtenidos\nRechazamos \\(H_0\\) si \\(|t|\\) está por encima del \\((1-\\alpha)\\)ésimo percentil de los \\(|t_b|\\) en la distribución bootstrap\nA pesar de sus propiedades teóricas, el refinamiento asintótico es poco usado"
  },
  {
    "objectID": "diapositivas/bootstrap.html#jacknife",
    "href": "diapositivas/bootstrap.html#jacknife",
    "title": "Bootstrap",
    "section": "Jacknife",
    "text": "Jacknife\n\nFormalmente no es un método bootstrap\nUna muestra jacknife es una muestra de tamaño \\(N-1\\) construida a partir de la muestra original donde una observación es eliminada a la vez\nEn cada muestra jacknife estimamos el estadístico de interés \\(\\hat{\\theta}_{(j)}\\) (tendremos \\(N\\) estadísticos)\nEl error estándar jacknife será\n\n\\[\\hat{se}(\\hat{\\theta})=\\left(\\frac{N-1}{N}\\sum_{j=1}^N\\left(\\hat{\\theta}_{(j)}-\\hat{\\theta}\\right)^2\\right)^{1/2}\\]\n\nFunciona bien para estadísticas suaves y funciones lineales\nSe puede hacer jacknife por bloques (Cameron y Miller, 2015)"
  },
  {
    "objectID": "cronograma.html",
    "href": "cronograma.html",
    "title": "Cronograma",
    "section": "",
    "text": "El siguiente cronograma es informativo sobre la organización del curso:",
    "crumbs": [
      "Cronograma"
    ]
  },
  {
    "objectID": "diapositivas/cuantil.html#motivación",
    "href": "diapositivas/cuantil.html#motivación",
    "title": "Regresión cuantil",
    "section": "Motivación",
    "text": "Motivación\n\nCuando vemos estadística descriptiva, muchas veces necesitamos más que la media para tener una idea de cómo se ven los datos\nSimilarmente, la regresión por MCO nos sirve para descubrir relaciones promedio basadas en \\(E(y|x)\\)\nEn muchos problemas nos interesan aspectos distribucionales\n¿Cómo es la distribución del ingreso para las personas con grado universitario comparada con la distribución del ingreso de las personas sin primaria completada?"
  },
  {
    "objectID": "diapositivas/cuantil.html#cuantiles",
    "href": "diapositivas/cuantil.html#cuantiles",
    "title": "Regresión cuantil",
    "section": "Cuantiles",
    "text": "Cuantiles\n\nEl cuantil poblacional \\(q\\) de la variable aleatoria \\(y\\), con \\(q\\in(0,1)\\), es el valor \\(\\mu_q\\) tal que \\(y\\leq \\mu_q\\) con probabilidad \\(q\\)\n\n\\[q=P(y\\leq \\mu_q)=F_y(\\mu_q)\\]\ndonde \\(F_y\\) es la función de distribución acumulada de \\(y\\)\n\nPor tanto, \\(\\mu_q=F_y^{-1}(q)\\)\nAlgunos cuantiles comúnmente usados son\n\n\\(q=0.5\\) es la mediana\n\\(q=0.25\\) es el cuartil inferior\n\\(q=0.75\\) es el cuartil superior\n\nPor ejemplo, si \\(y\\) es el salario mensual y \\(\\mu_{0.75}=6,000\\), entonces la probabilidad de que \\(y\\leq 6,000\\) es 0.75 (75% de los individuos tiene un salario menor o igual que 6,000)"
  },
  {
    "objectID": "diapositivas/cuantil.html#cuantiles-muestrales",
    "href": "diapositivas/cuantil.html#cuantiles-muestrales",
    "title": "Regresión cuantil",
    "section": "Cuantiles muestrales",
    "text": "Cuantiles muestrales\n\nCon una variable aleatoria \\(y\\), podemos estimar el cuantil \\(\\hat{\\mu}_q\\) como sigue\n\nOrdenamos la muestra en orden ascendente\n\\(\\hat{\\mu}_q\\) será la \\([Nq]\\)ésima observación más pequeña, donde \\([X]\\) significa redondear al siguiente entero\n\nPor ejemplo, si \\(N=97\\) y buscamos el cuartil inferior, \\(q=0.25\\)\n\n\\(\\hat{\\mu}_{0.25}\\) es el valor de \\(y\\) de la observación 25 porque \\([97*0.25]=[24.25]=25\\)\n\nKroenker & Bassett (1978) mostraron que \\(\\hat{\\mu}_q\\) puede ser estimado como la solución al siguiente problema de minimización\n\n\\[\\hat{\\mu}_q=\\arg\\min_{\\beta} \\sum_{i|y_i\\geq \\beta}^N q|y_i-\\beta|+\\sum_{i|y_i &lt;\\beta}^N(1-q)|y_i-\\beta|\\]"
  },
  {
    "objectID": "diapositivas/cuantil.html#regresión-cuantil-1",
    "href": "diapositivas/cuantil.html#regresión-cuantil-1",
    "title": "Regresión cuantil",
    "section": "Regresión cuantil",
    "text": "Regresión cuantil\n\nConsideremos ahora la regresión de \\(y\\) dado \\(x\\)\nEl cuantil poblacional \\(q\\) de \\(y\\) dado \\(x\\) es la función \\(\\mu_q(x)\\) tal que \\(y\\) condicional en \\(x\\) es menor o igual que \\(\\mu_q(x)\\) con probabilidad \\(q\\)\nAnálogamente\n\n\\[q=P(y\\leq\\mu_q(x)|X=x)=F_{y|x}(\\mu_q(x))\\]\n\nPor tanto\n\n\\[\\mu_q(x)=F^{-1}_{y|x}(q)\\]\ndonde \\(F_{y|x}\\) es la cdf de \\(y\\) dado \\(x\\)"
  },
  {
    "objectID": "diapositivas/cuantil.html#funciones-de-regresión-cuantiles",
    "href": "diapositivas/cuantil.html#funciones-de-regresión-cuantiles",
    "title": "Regresión cuantil",
    "section": "Funciones de regresión cuantiles",
    "text": "Funciones de regresión cuantiles\n\n\n\nConsideremos las relación entre educación e ingreso por hora\nGraficamos los cuantiles \\(q=\\{0.1, 0.3, 0.5, 0.7, 0.9\\}\\)\nCada línea punteada es una función de regresión cuantil\nPara cada nivel de educación, los cuantiles condicionales \\(\\mu_q(x)\\) están ordenados en \\(q\\)\nPara niveles bajos de educación, los cuantiles condicionales están más juntos, en comparación con los niveles altos\n\n\n\n\n\n\n\nFuente: Hansen (2022)"
  },
  {
    "objectID": "diapositivas/cuantil.html#estimador-de-regresión-cuantil",
    "href": "diapositivas/cuantil.html#estimador-de-regresión-cuantil",
    "title": "Regresión cuantil",
    "section": "Estimador de regresión cuantil",
    "text": "Estimador de regresión cuantil\n\nEl estimador de regresión cuantil es la solución al problema análogo de regresión lineal\n\n\\[\\hat{\\beta}_q=\\arg\\min_{\\beta_q} \\sum_{i|y_i\\geq x_i'\\beta}^N q|y_i-x_i'\\beta_q|+\\sum_{i|y_i &lt;x_i'\\beta}^N(1-q)|y_i-x_i'\\beta_q|\\]\n\nSe hace explícito que \\(\\beta_q\\) depende de \\(q\\), es decir, el cuantil elegido\nEste es un problema de minimización de una función de pérdida\nEn el caso de la regresión cuantil tiene la característica de pesar asimétricamente los errores\n\n\\((1-q)\\) es la penalización para las sobrepredicciones\n\\(q\\) es la penalización para las subpredicciones\n\nEl caso especial de \\(q=0.5\\) se conoce como estimador de regresión en la mediana o estimador de mínimas desviaciones absolutas"
  },
  {
    "objectID": "diapositivas/cuantil.html#regresión-en-la-mediana",
    "href": "diapositivas/cuantil.html#regresión-en-la-mediana",
    "title": "Regresión cuantil",
    "section": "Regresión en la mediana",
    "text": "Regresión en la mediana\n\nMCO minimiza \\(\\sum_i e_i^2\\)\nRegresión en la mediana minimiza \\(\\sum_i |e_i|\\)\nEn el caso de regresión en la mediana, el problema es encontrar \\(\\hat{\\beta}_{0.5}\\) que minimiza\n\n\\[\\sum_i |y_i-x_i'\\beta| \\]\n\nUna ventaja del estimador de regresión en la mediana es que es más robusto a la presencia de observaciones atípicas (outliers) que MCO"
  },
  {
    "objectID": "diapositivas/cuantil.html#funciones-de-pérdida",
    "href": "diapositivas/cuantil.html#funciones-de-pérdida",
    "title": "Regresión cuantil",
    "section": "Funciones de pérdida",
    "text": "Funciones de pérdida\n\n\n\nLa diferencia entre regresión cuantil y MCO radica en cómo se penalizan los errores de la aproximación \\(x'\\beta\\) para \\(y\\)\nEn MCO, la penalización ocurre con una función de pérdida cuadrática\nCon regresión en la mediana, los errores grandes son penalizados menos severamente que en MCO\nLas penalizaciones son no simétricas en el caso de regresión cuantil en general\n\n\n\n\n\n\n\nFuente: Hansen (2022)"
  },
  {
    "objectID": "diapositivas/cuantil.html#funciones-de-pérdida-1",
    "href": "diapositivas/cuantil.html#funciones-de-pérdida-1",
    "title": "Regresión cuantil",
    "section": "Funciones de pérdida",
    "text": "Funciones de pérdida\n\n\n\nLa diferencia entre regresión cuantil y MCO radica en cómo se penalizan los errores de la aproximación \\(x'\\beta\\) para \\(y\\)\nEn MCO, la penalización ocurre con una función de pérdida cuadrática\nCon regresión en la mediana, los errores grandes son penalizados menos severamente que en MCO\nLas penalizaciones son no simétricas en el caso de regresión cuantil en general\n\n\n\n\n\n\n\nFuente: Hansen (2022)"
  },
  {
    "objectID": "diapositivas/cuantil.html#estimación",
    "href": "diapositivas/cuantil.html#estimación",
    "title": "Regresión cuantil",
    "section": "Estimación",
    "text": "Estimación\n\nClaramente la función objetivo no es diferenciable\nEl problema puede formularse como uno de programación matemática\nKoenker describe algunos de los primeros algoritmos\nBuchinsky (1998) mostró además que\n\n\\[\\sqrt{N}(\\hat{\\beta}_q-\\beta_q)\\stackrel{d}{\\sim}\\mathcal{N}\\left(0,A^{-1}BA^{-1}\\right)\\] con \\[A=p\\lim\\frac{1}{N}\\sum_i f_{u_q}(0|x_i)x_ix_i' \\\\ B=p\\lim\\frac{1}{N}\\sum_i q(1-q)x_ix_i'\\]\n\n\\(f_{u_q}(0|x)\\) es la densidad condicional del término de error, \\(u_q=y-x'\\beta_q\\)"
  },
  {
    "objectID": "diapositivas/cuantil.html#ejemplo",
    "href": "diapositivas/cuantil.html#ejemplo",
    "title": "Regresión cuantil",
    "section": "Ejemplo",
    "text": "Ejemplo\n\nUsamos los datos en vietnam_hogares.csv\nDatos de una encuesta levantada en Vietnam en 1997 (tipo ENIGH)\nDatos de 5,006 hogares con gastos médicos\nConsideremos solo la relación entre el gasto médico y el gasto total (como proxy del ingreso total)\nUna elasticidad menor que uno indica que el bien es una necesidad\nUsaremos la paquetería quantreg"
  },
  {
    "objectID": "diapositivas/cuantil.html#ejemplo-1",
    "href": "diapositivas/cuantil.html#ejemplo-1",
    "title": "Regresión cuantil",
    "section": "Ejemplo",
    "text": "Ejemplo\n\n\n\nVemos que pasa con MCO\nMCO indica que los gastos médicos son una necesidad\nLa demanda es inelástica\n\n\ndata.wb&lt;-read_csv(\"vietnam_hogares.csv\",\n                        locale = locale(encoding = \"latin1\")) %&gt;% \n  filter(!is.na(lhhex12m)) %&gt;% \n  rename(lnmed=lhhex12m, lntotal=lhhexp1)\n\n\n\nsummary(r.mco &lt;- lm(lnmed  ~ lntotal, data=data.wb))\n\n\nCall:\nlm(formula = lnmed ~ lntotal, data = data.wb)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.5305 -0.9480  0.1062  1.0636  5.4642 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.93521    0.30515   3.065  0.00219 ** \nlntotal      0.57365    0.03248  17.661  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.546 on 5004 degrees of freedom\nMultiple R-squared:  0.05867,   Adjusted R-squared:  0.05849 \nF-statistic: 311.9 on 1 and 5004 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "diapositivas/cuantil.html#ejemplo-2",
    "href": "diapositivas/cuantil.html#ejemplo-2",
    "title": "Regresión cuantil",
    "section": "Ejemplo",
    "text": "Ejemplo\n\nEstimemos la regresión cuantil en el cuantil 90, es decir, estimemos \\(\\hat{\\beta}_{90}\\)\n\n\nsummary(r.q90 &lt;- rq(lnmed  ~ lntotal, data=data.wb, tau=0.9))\n\n\nCall: rq(formula = lnmed ~ lntotal, tau = 0.9, data = data.wb)\n\ntau: [1] 0.9\n\nCoefficients:\n            Value    Std. Error t value  Pr(&gt;|t|)\n(Intercept)  0.67510  0.43925    1.53692  0.12438\nlntotal      0.80036  0.04688   17.07153  0.00000\n\n\n\nEl cuantil 90 de la distribución del gasto en medicamentos se incrementa en 0.8% cuando el ingreso cambia en 1%"
  },
  {
    "objectID": "diapositivas/cuantil.html#ejemplo-3",
    "href": "diapositivas/cuantil.html#ejemplo-3",
    "title": "Regresión cuantil",
    "section": "Ejemplo",
    "text": "Ejemplo\n\nGráficamente\n\n\n\ndata.wb %&gt;% \n  ggplot(aes(x=lntotal,y=lnmed)) + \n  geom_point(alpha=0.3, size=.1) + \n  geom_abline(intercept=coef(r.q90)[1], slope=coef(r.q90)[2], color=\"black\", linetype=\"dashed\")+\n  geom_abline(intercept=coef(r.mco)[1], slope=coef(r.mco)[2], color=\"red\")"
  },
  {
    "objectID": "diapositivas/cuantil.html#ejemplo-4",
    "href": "diapositivas/cuantil.html#ejemplo-4",
    "title": "Regresión cuantil",
    "section": "Ejemplo",
    "text": "Ejemplo\n\nCuando se emplea regresión cuantil, comúnmente se presenta el efecto estimado en múltiples cuantiles\n\n\nr.q1_9 &lt;- rq(lnmed  ~ lntotal, data=data.wb, tau= 1:9/10)\n\n\nPodemos ver los coeficientes estimados\n\n\ncoef(r.q1_9)\n\n            tau= 0.1  tau= 0.2  tau= 0.3  tau= 0.4  tau= 0.5  tau= 0.6\n(Intercept) 2.825071 1.7402326 1.0971052 0.6808687 0.5921625 0.6631984\nlntotal     0.151201 0.3552251 0.4797725 0.5691746 0.6210917 0.6531126\n             tau= 0.7  tau= 0.8  tau= 0.9\n(Intercept) 0.4732287 0.3966890 0.6750989\nlntotal     0.7147830 0.7675658 0.8003567"
  },
  {
    "objectID": "diapositivas/cuantil.html#ejemplo-5",
    "href": "diapositivas/cuantil.html#ejemplo-5",
    "title": "Regresión cuantil",
    "section": "Ejemplo",
    "text": "Ejemplo\n\n\n\nLo que es más ilustrativo es un gráfico con los coeficientes de regresión cuantil y su intervalo de confianza\nSobreponemos el coeficiente de MCO y su intervalo de confianza\n\n\n\nplot(summary(r.q1_9),\n     mar = c(5.1, 4.1, 2.1, 2.1), \n     parm=\"lntotal\", \n     main = \"Coeficientes de regresión cuantil\",\n     ylab=\"Coeficiente de lntotal\",\n     xlab=\"Cuantil\")"
  },
  {
    "objectID": "diapositivas/cuantil.html#interpretación",
    "href": "diapositivas/cuantil.html#interpretación",
    "title": "Regresión cuantil",
    "section": "Interpretación",
    "text": "Interpretación\n\nEn nuestro ejemplo, cuando estimamos el coeficiente de regresión cuantil con \\(\\hat{\\beta}_{90}\\), interpretamos que cuando el ingreso cambia en 1% en hogares en el percentil 90 de la distriubción condicional del (log) gasto en medicamentos, entonces el gasto en medicamentos se incrementa 0.8%\nUn incremento de 1% del ingreso tiene un efecto más grande en el gasto en medicamentos en hogares más ricos que en hogares más pobres (porque \\(\\hat{\\beta}_q\\) va creciendo conforme \\(q\\) crece)"
  },
  {
    "objectID": "diapositivas/cuantil.html#interpretación-incorrecta",
    "href": "diapositivas/cuantil.html#interpretación-incorrecta",
    "title": "Regresión cuantil",
    "section": "Interpretación incorrecta",
    "text": "Interpretación incorrecta\n\nEl coeficiente de regresión cuantil con \\(\\hat{\\beta}_{90}\\) no dice que un incremento en el salario incrementa el gasto en medicamentos en el 10% de hogares más ricos en 0.8%\nTampoco dice que un incremento en el salario incrementa el gasto en medicamenos en el 10% de hogares con más gasto en medicamentos en 0.8%\nLo anterior equivaldría a quedarnos con un subconjunto de la muestra de hogares (el 10% más rico o el 10% con más gasto en medicamentos)\nCuando usamos regresión cuantil usamos toda la muestra para estimar los coeficientes para el \\(q\\) elegido"
  },
  {
    "objectID": "diapositivas/cuantil.html#interpretación-1",
    "href": "diapositivas/cuantil.html#interpretación-1",
    "title": "Regresión cuantil",
    "section": "Interpretación",
    "text": "Interpretación\n\n\n\nRegresión cuantil le pasa un línea al gráfico de puntos de tal manera que los valores del gasto en medicamentos tengan una probabilidad \\(q\\) de estar arriba de dicha línea para cada valor del ingreso\nEl 90% de los puntos están debajo de la recta con pendiente \\(\\hat{\\beta}_{90}\\)\n\\(\\hat{\\beta}_{90}\\) indica cómo afecta un incremento en el ingreso al gasto en medicamentos de hogares tienen un alto gasto en medicamentos relativo a otros hogares, para un nivel dado de ingreso\n\n\n\ndata.wb %&gt;% \n  ggplot(aes(x=lntotal,y=lnmed)) + \n  geom_point(alpha=0.3, size=.1) + \n  geom_abline(intercept=coef(r.q90)[1], slope=coef(r.q90)[2], color=\"black\", linetype=\"dashed\")"
  },
  {
    "objectID": "diapositivas/cuantil.html#cuantiles-vs-percentiles",
    "href": "diapositivas/cuantil.html#cuantiles-vs-percentiles",
    "title": "Regresión cuantil",
    "section": "Cuantiles vs percentiles",
    "text": "Cuantiles vs percentiles\n\n\nLeeds (2014) hace la distinción entre cuantiles y percentiles\n\nSe enfoca en la relación entre salarios y goles en jugadores de hockey\nLa línea que marca el cuantil con \\(q=0.9\\) pasa por la cola superior de las dos distribuciones\nEl cuantil 90 puede corresponder a jugadores de salario bajo\nLa línea del percentil 90 muestra el salario por arriba del cual se encuentra el 10% de los jugadores mejor pagados"
  },
  {
    "objectID": "diapositivas/cuantil.html#recomendaciones",
    "href": "diapositivas/cuantil.html#recomendaciones",
    "title": "Regresión cuantil",
    "section": "Recomendaciones",
    "text": "Recomendaciones\n\nRegresión cuantil nos ayuda a presentar un panorama más completo del problema (así como los cuantiles nos ayudan a describir los datos más allá de la media)\nAlgunos autores emplean regresión cuantil como evidencia de errores heterocedásticos cuando el modelo es lineal\nNo confundir regresión cuantil con lo siguiente:\n\nPartimos la muestra en \\(Q\\) segmentos y hacemos MCO en cada uno de ellos\nYa vimos que truncar la muestra en general es mala idea\n\nLos efectos cuantil nos dicen efectos sobre distribuciones, no sobre individuos"
  },
  {
    "objectID": "diapositivas/errores-estandar.html#errores-robustos-a-la-heterocedasticidad",
    "href": "diapositivas/errores-estandar.html#errores-robustos-a-la-heterocedasticidad",
    "title": "Errores estándar",
    "section": "Errores robustos a la heterocedasticidad",
    "text": "Errores robustos a la heterocedasticidad\nUn estimador de la varianza del estimador de MCO que no asume homocedasticidad es el estimador propuesto por White (1980)\n\\[\\hat{V}(\\beta_{MCO}^R)=(X'X)^{-1}\\left(\\sum_i\\hat{u}_i^2x_ix_i'\\right)(X'X)^{-1}\\]\nAquí un recordatorio de por qué podemos escribir \\(X'uu'X\\) como una sumatoria\nConsideremos la carnita del sándiwch\n\\[\\sum_i\\hat{u}_i^2x_ix_i \\equiv \\sum_i \\hat{\\psi}_i x_ix_i'\\]"
  },
  {
    "objectID": "diapositivas/errores-estandar.html#errores-estándar-robustos-1",
    "href": "diapositivas/errores-estandar.html#errores-estándar-robustos-1",
    "title": "Errores estándar",
    "section": "Errores estándar robustos",
    "text": "Errores estándar robustos\nDependiendo de cómo se especifique \\(\\hat{\\psi}_i\\), obtenemos distintas versiones del estimador de varianzas robusto\nLa propuesta de White original es:\n\\[HC0:\\quad\\hat{\\psi}_i=\\hat{u}_i^2\\]\nEste estimador asintóticamente consistente\nEn muestras pequeñas, muchas veces se emplea la siguiente corrección:\n\\[HC1:\\quad\\hat{\\psi}_i=\\frac{N}{N-k}\\hat{u}_i^2\\]"
  },
  {
    "objectID": "diapositivas/errores-estandar.html#desviación-a-la-influencia",
    "href": "diapositivas/errores-estandar.html#desviación-a-la-influencia",
    "title": "Errores estándar",
    "section": "Desviación a la influencia",
    "text": "Desviación a la influencia\nUn par de resultados nos ayudarán a entender qué hacen las otras correcciones a la matriz robusta en el software\nDefinimos la influencia de la observación \\(i\\) como:\n\\[h_{ii}=X_i'(X'X)^{-1}X_i\\]\n\\(h_{ii}\\) nos dice qué tanto jala la observación \\(i\\) a la línea de regresión\nEn una regresión con un solo regresor \\(x\\), se puede mostrar que la influencia de la observación \\(i\\) es:\n\\[h_{ii}=\\frac{1}{N}+\\frac{(x_i-\\bar{x})^2}{\\sum(x_j-\\bar{x})^2}\\] es decir, que la influencia se incrementa cuando \\(x_i\\) se aleja de la media\nLa influencia es un número entre 0 y 1 y además \\(\\sum_i h_{ii}=k\\), siendo \\(k\\) el número de regresores"
  },
  {
    "objectID": "diapositivas/errores-estandar.html#errores-estándar-robustos-2",
    "href": "diapositivas/errores-estandar.html#errores-estándar-robustos-2",
    "title": "Errores estándar",
    "section": "Errores estándar robustos",
    "text": "Errores estándar robustos\nAlgunos autores sugieren usar la influencia en la matriz de varianzas robusta\nSe proponen algunas alternativas:\n\\[HC2:\\quad\\hat{\\psi}_i=\\frac{1}{1-h_{ii}}\\hat{u}_i^2\\]\n\\[HC3:\\quad\\hat{\\psi}_i=\\frac{1}{(1-h_{ii})^2}\\hat{u}_i^2\\]\nLong & Ervin (2000) realizaron un experimento de simulación y recomendaron usar \\(HC3\\) en muestras pequeñas, por lo que el paquete sandwich en R usa \\(HC3\\) por default\nEs importante tener en cuenta qué tipo de errores estándar piden que el software calcule"
  },
  {
    "objectID": "diapositivas/errores-estandar.html#errores-agrupados-2",
    "href": "diapositivas/errores-estandar.html#errores-agrupados-2",
    "title": "Errores estándar",
    "section": "Errores agrupados",
    "text": "Errores agrupados\nPodemos mostrar que la correlación de errores entre dos observaciones \\(i\\) y \\(j\\) que pertenecen a \\(g\\) es \\[E(e_{ig}e_{jg})=\\overbrace{\\rho_e}^{\\substack{\\text{coeficiente de correlación} \\\\ \\text{intraclase residual}}} \\underbrace{\\sigma_e^2}_{\\text{varianza residual}}\\]\nLe damos una estructura aditiva a los errores:\n\\[e_{ig}=\\nu_g+\\eta_{ig}\\] donde \\(\\nu_g\\) captura toda la correlación dentro del grupo\n\\(\\eta_{ig}\\) es un error idiosincrático con media cero e independiente de cualquier otro \\(\\eta_{jg}\\)\nComo queremos analizar el problema del agrupamiento, asumimos que tanto \\(v_g\\) y \\(\\eta_{ig}\\) son homocedásticos"
  },
  {
    "objectID": "diapositivas/errores-estandar.html#errores-agrupados-3",
    "href": "diapositivas/errores-estandar.html#errores-agrupados-3",
    "title": "Errores estándar",
    "section": "Errores agrupados",
    "text": "Errores agrupados\nCon esta estructura de errores, el coeficiente de correlación intraclase es:\n\\[\\rho_e=\\frac{\\sigma_{\\nu}^2}{\\sigma_{\\nu}^2+\\sigma_{\\eta}^2}\\]\nDeberíamos calcular la matriz de varianzas \\(V_C(\\hat{\\beta})\\) tomando en cuenta esta estructura\n¿Qué pasa si hacemos MCO en el contexto de este problema?\nMoulton (1984) muestra que:\n\\[\\frac{V_C(\\hat{\\beta})}{V_{MCO}(\\hat{\\beta})}=1+(n-1)\\rho_e\\] - A \\(\\sqrt{\\frac{V_C(\\hat{\\beta})}{V_{MCO}(\\hat{\\beta})}}\\) se le conoce como el factor de Moulton"
  },
  {
    "objectID": "diapositivas/errores-estandar.html#factor-de-moulton",
    "href": "diapositivas/errores-estandar.html#factor-de-moulton",
    "title": "Errores estándar",
    "section": "Factor de Moulton",
    "text": "Factor de Moulton\nEl factor de Moulton nos dice qué tanto sobreestimamos la precisión al ignorar la correlación intra clase\nVisto de otro modo:\n\\[V_C(\\hat{\\beta})=\\left(1+(n-1)\\rho_e\\right)V_{MCO}(\\hat{\\beta})\\]\nEs decir entre más grande sea la correlación dentro de los grupos, más deberíamos inflar los errores de MCO\nConsideremos el caso extremo de que \\(\\rho_e=1\\), es decir, que todas las \\(y_{ig}\\) dentro del mismo \\(g\\) son iguales\nEntonces el factor de Moulton es simplemente \\(\\sqrt{n}\\)\nVisto de otro modo, la matriz de varianzas correcta se obtendría multiplicando por \\(n\\) la matriz \\(V_{MCO}(\\hat{\\beta})\\)\n\\[V_C(\\hat{\\beta})=n V_{MCO}(\\hat{\\beta})\\]"
  },
  {
    "objectID": "diapositivas/errores-estandar.html#errores-agrupados-en-general",
    "href": "diapositivas/errores-estandar.html#errores-agrupados-en-general",
    "title": "Errores estándar",
    "section": "Errores agrupados en general",
    "text": "Errores agrupados en general\nEn general, \\(x_{ig}\\) varía a nivel individual y tenemos grupos de tamaño \\(n_g\\)\nEn este caso, el factor de Moulton es la raíz cuadrada de:\n\\[\\frac{V_C(\\hat{\\beta})}{V_{MCO}(\\hat{\\beta})}=1+\\left(\\frac{V(n_g)}{\\bar{n}}+\\bar{n}-1\\right)\\rho_x\\rho_e\\] donde \\(\\bar{n}\\) es el tamaño promedio del grupo y \\(\\rho_x\\) es la correlación intraclase de \\(x_{ig}\\)\nNo es necesario asumir una forma para \\(\\rho_x\\) (se puede calcular)\nNoten que el error que cometemos es más grande entre más heterogéneo es el tamaño de grupos y entre más grande es \\(\\rho_x\\)\nPor tanto, cuando el tratamiento no varía entre grupos, este error es grande"
  },
  {
    "objectID": "diapositivas/errores-estandar.html#soluciones-para-errores-agrupados",
    "href": "diapositivas/errores-estandar.html#soluciones-para-errores-agrupados",
    "title": "Errores estándar",
    "section": "Soluciones para errores agrupados",
    "text": "Soluciones para errores agrupados\nSolución paramétrica: calcular directamente el factor de Moulton e inflar los errores de MCO\nBootstrap por bloques: ver más adelante el concept de bootstrap\nEstimar los errores agrupados (clustered standard errors)"
  },
  {
    "objectID": "diapositivas/errores-estandar.html#errores-estándar-agrupados",
    "href": "diapositivas/errores-estandar.html#errores-estándar-agrupados",
    "title": "Errores estándar",
    "section": "Errores estándar agrupados",
    "text": "Errores estándar agrupados\nCon errores agrupados podemos escribir el estimador de MCO como\n\\[\n\\begin{aligned}\n\\hat{\\beta}&=\\beta+(X'X)^{-1}X'u \\\\\n&=(X'X)^{-1}\\left(\\sum_{g=1}^G X_gu_g\\right)\n\\end{aligned}\n\\]\nSuponiendo independencia entre \\(g\\) y correlación dentro de cada grupo:\n\\[E(u_{ig}u_{jg'}|x_{ig}x_{jg'})=0\\]\nexcepto cuando \\(g=g'\\)\nEn este caso, el estimador de MCO tiene una varianza asintótica dada por\n\\[V({\\hat{\\beta}}_{MCO})=(X'X)^{-1}\\left(\\sum_{g=1}^G X_g'u_gu_g'X\\right)(X'X)^{-1}\\]"
  },
  {
    "objectID": "diapositivas/errores-estandar.html#errores-estándar-agrupados-1",
    "href": "diapositivas/errores-estandar.html#errores-estándar-agrupados-1",
    "title": "Errores estándar",
    "section": "Errores estándar agrupados",
    "text": "Errores estándar agrupados\nCon errores heterocedásticos, pero sin agrupamiento, la matriz de varianzas de White (1980) tiene una estructura como sigue:\n\\[\\hat{V}(\\hat{\\beta}_{R})=(X'X)^{-1}X'\\hat{\\Sigma} X (X'X)^{-1}\\]\nDonde\n\\[\\hat{\\Sigma}=\\left(\\begin{matrix} \\hat{u}_{1}^2 & 0  & 0  & \\ldots & 0 \\\\ 0 & \\hat{u}_{2}^2 & 0 & \\ldots & 0 \\\\ \\vdots & & & & \\\\ 0 & & &  \\ldots & \\hat{u}_{n}^2\\end{matrix}\\right)\\]"
  },
  {
    "objectID": "diapositivas/errores-estandar.html#errores-estándar-agrupados-2",
    "href": "diapositivas/errores-estandar.html#errores-estándar-agrupados-2",
    "title": "Errores estándar",
    "section": "Errores estándar agrupados",
    "text": "Errores estándar agrupados\nPara estimar la varianza con errores agrupados empleamos una generalización de la propuesta de White para errores robustos\nSi \\(G\\to\\infty\\), el estimador de la matriz de errores agrupados robusta (CRVE) es consistente para estimar \\(V(\\hat{\\beta})\\):\n\\[\\hat{V}_{CR}(\\hat{\\beta})=(X'X)^{-1}\\left(\\sum_{g=1}^G X_g'\\hat{u}_g\\hat{u}_g'X_g\\right)(X'X)^{-1}\\] donde \\(\\hat{u}_g\\hat{u}_g'\\) es la matriz de varianzas para los individuos del grupo \\(g\\)\nDe manera compacta\n\\[\\hat{V}_{CR}(\\hat{\\beta})=(X'X)^{-1}X'\\hat{\\Sigma} X(X'X)^{-1}\\]"
  },
  {
    "objectID": "diapositivas/errores-estandar.html#errores-estándar-agrupados-3",
    "href": "diapositivas/errores-estandar.html#errores-estándar-agrupados-3",
    "title": "Errores estándar",
    "section": "Errores estándar agrupados",
    "text": "Errores estándar agrupados\nY en este caso la matriz \\(\\hat{\\Sigma}\\) tiene una estructura agrupada\n\\[\\small \\hat{\\Sigma}=\\left(\\begin{matrix} \\hat{u}_{1,1}^2 & \\hat{u}_{1,1}\\hat{u}_{2,1} & \\ldots & \\hat{u}_{1,1} \\hat{u}_{n,1}& 0 & 0 & \\ldots &  0 & \\ldots & 0 & 0 & \\ldots &  0 \\\\ \\hat{u}_{2,1}\\hat{u}_{1,1} & \\hat{u}_{2,1}^2 & \\ldots & \\hat{u}_{2,1}\\hat{u}_{n,1} & 0 & 0 & \\ldots & 0 & \\ldots  & 0 & 0 & \\ldots &  0\\\\\n\\vdots & \\vdots  & & \\vdots & \\vdots & \\vdots  & &  \\vdots& & \\vdots & \\vdots &  &  \\vdots \\\\ \\hat{u}_{n,1}\\hat{u}_{1,1} & \\hat{u}_{n,1}\\hat{u}_{2,1}& \\ldots & \\hat{u}_{n,1}^2& 0 & 0 &\\ldots & 0 & \\ldots & 0 & 0 & \\ldots &  0 \\\\  0 & 0 & \\ldots &  0 & \\hat{u}_{1,2}^2 & \\hat{u}_{1,2}\\hat{u}_{2,2} & \\ldots & \\hat{u}_{1,2}\\hat{u}_{n,2} &\\ldots & 0 & 0 & \\ldots &  0  \\\\ 0 & 0 & \\ldots &  0 & \\hat{u}_{2,2}\\hat{u}_{1,2} & \\hat{u}_{2,2}^2 & \\ldots & \\hat{u}_{2,2}\\hat{u}_{n,2} &\\ldots & 0 & 0 & \\ldots &  0 \\\\ \\vdots & \\vdots  & & \\vdots & \\vdots & \\vdots  & &  \\vdots& & \\vdots & \\vdots &  &  \\vdots  \\\\ 0 & 0 & \\ldots &  0 & \\hat{u}_{n,2}\\hat{u}_{1,2} & \\hat{u}_{n,2}\\hat{u}_{2,2} & \\ldots & \\hat{u}_{n,2}^2 &\\ldots & 0 & 0 & \\ldots &  0 \\\\ \\vdots & \\vdots  & & \\vdots & \\vdots & \\vdots  & &  \\vdots& & \\vdots & \\vdots &  &  \\vdots \\\\ 0 & 0 & \\ldots &  0 & 0 &  0 & \\ldots & 0 &\\ldots & \\hat{u}_{1,G}^2 & \\hat{u}_{12,G}\\hat{u}_{2,G} & \\ldots &  \\hat{u}_{1,G}\\hat{u}_{n,G} \\\\  0 & 0 & \\ldots &  0 & 0 &  0 & \\ldots & 0 &\\ldots & \\hat{u}_{2,G}\\hat{u}_{1,G} & \\hat{u}_{2,G}^2 & \\ldots &  \\hat{u}_{2,G}\\hat{u}_{n,G} \\\\ \\vdots & \\vdots  & & \\vdots & \\vdots & \\vdots  & &  \\vdots& & \\vdots & \\vdots &  &  \\vdots \\\\  0 & 0 & \\ldots &  0 & 0 &  0 & \\ldots & 0 &\\ldots & \\hat{u}_{n,G}\\hat{u}_{1,G} & \\hat{u}_{n,G}\\hat{u}_{2,G} & \\ldots &  \\hat{u}_{n,G}^2 \\end{matrix}\\right)\\]"
  },
  {
    "objectID": "diapositivas/errores-estandar.html#errores-estándar-agrupados-4",
    "href": "diapositivas/errores-estandar.html#errores-estándar-agrupados-4",
    "title": "Errores estándar",
    "section": "Errores estándar agrupados",
    "text": "Errores estándar agrupados\nEl resultado asintótico de consistencia depende de que \\(G\\to\\infty\\)\nSi \\(G\\) está fijo, no importa qué tan grande sea \\(N\\), \\(\\hat{V}_{CRVE}(\\hat{\\beta})\\) no será consistente\nAlgunos paquetes ajustan esta matriz de varianzas haciendo una corrección parecida a \\(HC1\\), pero ahora tomando en cuanta también \\(G\\) y no solo \\(N\\) (ver por ejemplo, vcovCR en R)\nCon pocos grupos, subestimamos los errores estándar y rechazamos la \\(H_0\\) más veces de lo que deberíamos (over-rejection)\nSi tenemos pocos grupos, recurrimos a otras soluciones (ver Cameron y Miller, 2015) - Inflar los errores con un corrector de sesgo - Bootstrap agrupado con refinamiento asintótico\nLa recomendación práctica es que se tomen en serio el problema de los pocos clusters\n¿Cuánto es poco? Cameron y Miller (2015) citan 50. (¡Qué raro, el número de estados en EUA!)"
  },
  {
    "objectID": "diapositivas/mgm.html#introducción",
    "href": "diapositivas/mgm.html#introducción",
    "title": "Método generalizado de momentos",
    "section": "Introducción",
    "text": "Introducción\nEl GMM generaliza una serie de estimadores comúnmente usados en econometría (incluyendo MCO, MV, VI, etc)\nAsumimos que existen \\(r\\) condiciones de momentos independientes para \\(q\\) parámetros \\[E(h(w_i,\\theta_0))=0\\]\ndonde \\(\\theta\\) es un vector de \\(q\\times 1\\), \\(h(\\cdot)\\) es una función vector de \\(r \\times 1\\) con \\(r\\geq q\\)\n\\(w_i\\) son los datos observables, incluyendo las variables dependientes, los regresores exógenos, potenciales regresores endógenos, así como instrumentos"
  },
  {
    "objectID": "diapositivas/mgm.html#método-generalizado-de-momentos-1",
    "href": "diapositivas/mgm.html#método-generalizado-de-momentos-1",
    "title": "Método generalizado de momentos",
    "section": "Método generalizado de momentos",
    "text": "Método generalizado de momentos\nLa forma de \\(h(\\cdot)\\) es equivalente a escoger el modelo\nPor ejemplo:\n\n\n\n\\(h(\\cdot)\\)\nMétodo de estimación\n\n\n\n\n\\(x(y-x'\\beta)\\)\nMCO\n\n\n\\(\\partial\\mathcal{L}/\\partial\\theta\\)\nMV\n\n\n\\(z(y-x'\\beta)\\)\nVI"
  },
  {
    "objectID": "diapositivas/mgm.html#método-de-momentos",
    "href": "diapositivas/mgm.html#método-de-momentos",
    "title": "Método generalizado de momentos",
    "section": "Método de momentos",
    "text": "Método de momentos\nCuando \\(r=q\\), tenemos un modelo exactamente identificado, es decir, tenemos tantos momentos como parámetros a estimar\nPodemos obtener el estimador de método de momentos \\(\\hat{\\theta}_{MM}\\) como la solución a\n\\[\\frac{1}{N}h(w_i,\\hat{\\theta})=0\\]"
  },
  {
    "objectID": "diapositivas/mgm.html#método-de-momentos-1",
    "href": "diapositivas/mgm.html#método-de-momentos-1",
    "title": "Método generalizado de momentos",
    "section": "Método de momentos",
    "text": "Método de momentos\nLos momentos poblacionales dan lugar a momentos muestrales que pueden ser usados para estimar los parámetros\nEjemplo: estimación de la media poblacional con observaciones iid y media \\(\\mu\\)\nEl momento poblacional es:\n\\[E(y-\\mu)=0\\]\nSi sustituimos con el momento poblacional, es decir, sustituyendo la esperanza con el operador del promedio obtenemos el momento muestral correspondiente:\n\\[\\frac{1}{N}\\sum_{i=1}^N (y_i-\\mu)=0\\]\nAl resolver para \\(\\mu\\) obtenemos \\(\\mu_{MM}=\\frac{1}{N}\\sum_i y_i =\\bar{y}\\)\nEl estimador de MM de a media poblacional es la media muestral"
  },
  {
    "objectID": "diapositivas/mgm.html#método-generalizado-de-momentos-2",
    "href": "diapositivas/mgm.html#método-generalizado-de-momentos-2",
    "title": "Método generalizado de momentos",
    "section": "Método generalizado de momentos",
    "text": "Método generalizado de momentos\nEl caso que nos ocupa más en el contexto de MC2E es cuando \\(r&gt;q\\), es decir, un modelo sobreidentificado\nEn este caso, tenemos más ecuaciones que incógnitas en la condición de momentos\nEl estimador de método generalizado de momentos \\(\\hat{\\theta}_{GMM}\\) se define como el vector de parámetros que minimiza la forma cuadrática\n\\[Q_N(\\theta)=\\left(\\frac{1}{N}\\sum_ih(w_i,\\theta)\\right)'W_N\\left(\\frac{1}{N}\\sum_ih(w_i,\\theta)\\right)\\]\ndonde \\(W_N\\) es una matriz simétrica y positiva definida que no depende de \\(\\theta\\)\nDiferentes matrices \\(W_N\\) dan origen a distintos estimadores"
  },
  {
    "objectID": "diapositivas/mgm.html#método-generalizado-de-momentos-3",
    "href": "diapositivas/mgm.html#método-generalizado-de-momentos-3",
    "title": "Método generalizado de momentos",
    "section": "Método generalizado de momentos",
    "text": "Método generalizado de momentos\nLas COP son:\n\\[\\left(\\frac{1}{N}\\sum_{i=1}^N\\frac{\\partial h_i(\\hat{\\theta})'}{\\partial\\theta}\\right)W_N\\left(\\frac{1}{N}\\sum_{i=1}^Nh_i(\\hat{\\theta})\\right)=\\mathbf{0}\\] donde para acotar la notación \\(h_i(\\theta)=h_i(w_i,\\theta)\\)\nEsto es un sistema de ecuaciones en general no lineal y complicado de resolver\nRecurrimos a métodos numéricos para encontrar \\(\\hat{\\theta}_{MGM}\\)\nPodemos establecer teoría asintótica para mostrar las propiedades asintóticas del estimador de MGM"
  },
  {
    "objectID": "diapositivas/mgm.html#estimador-de-mgm",
    "href": "diapositivas/mgm.html#estimador-de-mgm",
    "title": "Método generalizado de momentos",
    "section": "Estimador de MGM",
    "text": "Estimador de MGM\nProposición 6.1 en CT: bajo una serie de supuestos para poder establecer LGN y TLC, \\(\\hat{\\theta}_{GMM}\\), definido como una raíz de las condiciones de primer orden \\(\\partial Q_N(\\theta) / \\partial \\theta=0\\), es tal que:\n\\[\\sqrt{N}\\left(\\hat{\\theta}_{GMM}-\\theta_0\\right)\\stackrel{a}{\\sim}\\mathcal{N}\\left(0,(G_0'W_0G_0)^{-1}(G_0'W_0S_0W_0G_0)(G_0'W_0G_0)^{-1}\\right)\\]\ndonde \\(W_0\\) es una matriz finita, simétrica y positiva definida, y\n\\[\n\\begin{aligned}\nG_0&=p\\lim\\frac{1}{N}\\sum_{i=1}^N \\left(\\frac{\\partial h_i}{\\partial\\theta'}\\Bigg|_{\\theta_0}\\right) \\\\\nS_0&=p\\lim \\frac{1}{N} \\sum_{i=1}^N \\sum_{j=1}^N \\left(h_i h_j \\Bigg|_{\\theta_0} \\right)\n\\end{aligned}\n\\]\nLa derivación es un poco más complicada que la usada para los estimadores extremos, pero pueden ver una propuesta en la sección 6.3.9 de CT"
  },
  {
    "objectID": "diapositivas/mgm.html#matriz-de-varianzas-óptima",
    "href": "diapositivas/mgm.html#matriz-de-varianzas-óptima",
    "title": "Método generalizado de momentos",
    "section": "Matriz de varianzas óptima",
    "text": "Matriz de varianzas óptima\nPara implementar MGM debemos especificar las condiciones de momentos y la matriz \\(W_N\\)\nEn el caso de modelos sobreidentificados y con \\(S_0\\) conocida, el estimador de MGM más eficiente se obtiene al especificar \\(W_N=S_0^{-1}\\)\nCon esta elección, la expresión para la varianza de \\(\\hat{\\beta}_{MGM}\\) se simplifica a\n\\[\\sqrt{N}\\left(\\hat{\\theta}_{GMM}-\\theta_0\\right)\\stackrel{a}{\\sim}\\mathcal{N}\\left(0,(G_0'S_0^{-1}G_0)^{-1}\\right)\\]\nEn la práctica, \\(S_0\\) es desconocida, así que la sustituimos por un estimador consistente \\(\\hat{S}\\)"
  },
  {
    "objectID": "diapositivas/mgm.html#matriz-de-varianzas-óptima-1",
    "href": "diapositivas/mgm.html#matriz-de-varianzas-óptima-1",
    "title": "Método generalizado de momentos",
    "section": "Matriz de varianzas óptima",
    "text": "Matriz de varianzas óptima\nImportante: el que la elección de la matriz de varianzas óptima sea \\(W_N=S_0^{-1}\\) no ocurre por que \\((G_0'S_0^{-1}G_0)\\) se escribe mucho más corto que \\((G_0'W_0G_0)^{-1}(G_0'W_0S_0W_0G_0)(G_0'W_0G_0)\\)\nMás bien, es óptima porque se puede mostrar que\n\\[(G_0'W_0G_0)^{-1}(G_0'W_0S_0W_0G_0)(G_0'W_0G_0)^{-1}\\geq (G_0'S_0^{-1}G_0)^{-1}\\]\npara cualquier \\(W_0\\) positiva definida\nPueden ver una procedimiento para mostrar esto en Hayashi (2000) p. 245"
  },
  {
    "objectID": "diapositivas/mgm.html#mgm-óptimo",
    "href": "diapositivas/mgm.html#mgm-óptimo",
    "title": "Método generalizado de momentos",
    "section": "MGM óptimo",
    "text": "MGM óptimo\nEn la práctica, no se conoce \\(S_0\\) sino que se sustituye por un estimador consistente \\(\\hat{S}\\)\nLa matriz de varianzas se estima siguiendo un procedimiento de dos etapas\n\nObtener el estimador de MGM usando una matriz subóptima, generalmente \\(W_N=I_r\\) y con estos coeficientes obtener un estimador para \\(S_0\\): \\[\\hat{S}=\\frac{1}{N}\\sum_i h_i(\\hat{\\theta})h_j(\\hat{\\theta})'\\]\nObtener un estimador de MGM óptimo o estimador de MGM de dos etapas óptimo \\(\\hat{\\theta}_{MGM,O}\\) minimizando\n\n\\[Q_N(\\theta)=\\left(\\frac{1}{N}\\sum_ih(\\theta)\\right)'\\hat{S}^{-1}\\left(\\frac{1}{N}\\sum_ih(\\theta)\\right)\\]"
  },
  {
    "objectID": "diapositivas/mgm.html#mgm-óptimo-1",
    "href": "diapositivas/mgm.html#mgm-óptimo-1",
    "title": "Método generalizado de momentos",
    "section": "MGM óptimo",
    "text": "MGM óptimo\nLa distribución límite de \\(\\hat{\\theta}_{MGM,O}\\) es normal, centrada en \\(\\theta_0\\)\nPara estimar la varianza de \\(\\hat{\\theta}_{MGM,O}\\) usamos\n\\[\\hat{V}(\\hat{\\theta}_{MGM,O})=N^{-1}(\\hat{G}\\tilde{S}^{-1}\\hat{G})^{-1}\\]\ndonde \\(\\hat{G}\\) y \\(\\tilde{S}\\) se evalúan en \\(\\hat{\\theta}_{MGM,O}\\)"
  },
  {
    "objectID": "diapositivas/mgm.html#estimador-lineal-de-mgm",
    "href": "diapositivas/mgm.html#estimador-lineal-de-mgm",
    "title": "Método generalizado de momentos",
    "section": "Estimador lineal de MGM",
    "text": "Estimador lineal de MGM\nConsideremos el modelo lineal\n\\[y_i=x_i'\\beta + u_i\\]\nSupongamos que algún componente de \\(x\\) es endógeno, por lo que recurrimos a un instrumento que cumple con:\n\\[E(u_i|z_i)=0\\]\nLa restricción de exclusión nos especifica una condición de momentos\n\\[E(z_i(y_i-x_i'\\beta))=0\\]"
  },
  {
    "objectID": "diapositivas/mgm.html#estimador-lineal-de-mgm-1",
    "href": "diapositivas/mgm.html#estimador-lineal-de-mgm-1",
    "title": "Método generalizado de momentos",
    "section": "Estimador lineal de MGM",
    "text": "Estimador lineal de MGM\nEl estimador MGM minimiza la forma cuadrática siguiente\n\\[\n\\begin{aligned}\nQ(\\beta)&=\\left(\\frac{1}{N}\\sum_i (y_i-x_i'\\beta)z_i\\right)'W_N\\left(\\frac{1}{N}\\sum_i (y_i-x_i'\\beta)z_i\\right) \\\\\n&=\\left(\\frac{1}{N}(y-X\\beta)'Z\\right)W_N\\left(\\frac{1}{N}Z'(y-X\\beta)\\right)\n\\end{aligned}\n\\]\nLas condiciones de primer orden son\n\\[\\frac{\\partial Q_N(\\beta)}{\\partial\\beta}=-2\\left(\\frac{1}{N}X'Z\\right)W_N\\Big(\\frac{1}{N}Z'(y-X\\beta)\\Big)=0\\]\nResolviendo para \\(\\beta\\) obtenemos el estimador lineal de VI de GMM:\n\\[\\hat{\\beta}_{GMM}=(X'ZW_NZ'X)^{-1}X'ZW_NZ'y\\]\nLas propiedades asintóticas de este estimador se pueden obtener de manera similar a como se obtuvieron las del estimador de MCO o usando las propiedades más generales para problemas de MGM"
  },
  {
    "objectID": "diapositivas/mgm.html#estimador-de-la-varianza-de-hatbeta_mgm",
    "href": "diapositivas/mgm.html#estimador-de-la-varianza-de-hatbeta_mgm",
    "title": "Método generalizado de momentos",
    "section": "Estimador de la varianza de \\(\\hat{\\beta}_{MGM}\\)",
    "text": "Estimador de la varianza de \\(\\hat{\\beta}_{MGM}\\)\nPodemos hacer un procedimiento similar al que hacíamos con MCO\n\\[\n\\begin{aligned}\n\\hat{\\beta}_{GMM}&=(X'ZW_NZ'X)^{-1}X'ZW_NZ'(X\\beta+u) \\\\\n&=\\beta + (X'ZW_NZ'X)^{-1}X'ZW_NZ'u\n\\end{aligned}\n\\] Por lo tanto, reescalando:\n\\[\n\\begin{aligned}\n\\sqrt{N}(\\hat{\\beta}_{GMM}-\\beta)&=\\sqrt{N}(X'ZW_NZ'X)^{-1}X'ZW_NZ'u \\\\\n\\end{aligned}\n\\] Si podemos aplicar un TLC a \\(\\frac{1}{\\sqrt{N}}Z'u\\) entonces\n\\[\\frac{1}{\\sqrt{N}}Z'u\\xrightarrow{d}\\mathcal{N}(0,S)\\]\ncon \\(S=\\lim\\frac{1}{N}\\sum_iE(u_i^2z_iz_i')\\)"
  },
  {
    "objectID": "diapositivas/mgm.html#estimador-de-la-varianza-de-hatbeta_mgm-1",
    "href": "diapositivas/mgm.html#estimador-de-la-varianza-de-hatbeta_mgm-1",
    "title": "Método generalizado de momentos",
    "section": "Estimador de la varianza de \\(\\hat{\\beta}_{MGM}\\)",
    "text": "Estimador de la varianza de \\(\\hat{\\beta}_{MGM}\\)\nEl estimador \\(\\hat{\\beta}_{MGM}\\) es asintóticamente normal, centrado en \\(\\beta\\) y con una varianza asintótica estimada dada por\n\\[\\hat{V}(\\hat{\\beta}_{GMM})=N(X'ZW_NZ'X)^{-1}(X'ZW_N\\hat{S}W_NZ'X)(X'ZW_NZ'X)^{-1}\\]\ndonde \\(\\hat{S}\\) es un estimador consistente de \\(S\\)\nDependiendo de si estamos en un modelo exactamente identificado o sobreidentificado y de cómo especificamos la matriz \\(W_N\\), los resultados anteriores sobre \\(\\hat{\\beta}_{GMM}\\) y \\(\\hat{V}(\\hat{\\beta}_{GMM})\\) se especializan"
  },
  {
    "objectID": "diapositivas/mgm.html#estimador-de-la-varianza-de-hatbeta_mgm-2",
    "href": "diapositivas/mgm.html#estimador-de-la-varianza-de-hatbeta_mgm-2",
    "title": "Método generalizado de momentos",
    "section": "Estimador de la varianza de \\(\\hat{\\beta}_{MGM}\\)",
    "text": "Estimador de la varianza de \\(\\hat{\\beta}_{MGM}\\)\n¿Cómo estimamos \\(\\hat{S}\\)\nEn el caso general, con posible heterocedasticidad:\n\\[\\hat{S}=\\frac{1}{N}\\sum_i\\hat{u}_i^2z_iz_i = \\frac{1}{N}Z'DZ\\] con\n\\[\nD=\n\\begin{pmatrix}\n\\hat{u}_1^2 & 0 & 0 &\\ldots 0 \\\\\n0 & \\hat{u}_2^2 & 0 &\\ldots 0 \\\\\n\\vdots \\\\\n0 & 0 & 0 &\\ldots \\hat{u}_n^2 \\\\\n\\end{pmatrix}\n\\] Con homocedasticidad, se simplifica a:\n\\[\\hat{S}=\\frac{1}{N}s^2Z'Z\\]"
  },
  {
    "objectID": "diapositivas/mgm.html#estimador-óptimo-de-mgm",
    "href": "diapositivas/mgm.html#estimador-óptimo-de-mgm",
    "title": "Método generalizado de momentos",
    "section": "Estimador óptimo de MGM",
    "text": "Estimador óptimo de MGM\nPara obtener el estimador óptimo escogemos una forma particular para la matriz de pesos\n\\[W=\\hat{S}^{-1}\\] Y entonces el estimador de MGM se vuelve\n\\[\\hat{\\beta}_{GMM,O}=(X'Z\\hat{S}^{-1}Z'X)^{-1}X'Z\\hat{S}^{-1}Z'y\\] Si permitimos heterocedasticidad arbitraria, obtenemos \\(\\hat{\\beta}\\) en una primera etapa con una matriz subóptima para calcular \\(\\hat{S}\\)\nLuego obtenemos \\(\\hat{\\beta}_{GMM,O}\\) usando \\(\\hat{S}^{-1}\\) como matriz de pesos"
  },
  {
    "objectID": "diapositivas/mgm.html#matriz-de-varianzas",
    "href": "diapositivas/mgm.html#matriz-de-varianzas",
    "title": "Método generalizado de momentos",
    "section": "Matriz de varianzas",
    "text": "Matriz de varianzas\nY el estimador de varianza se simplifica a\n\\[\n\\begin{aligned}\n\\hat{V}(\\hat{\\beta}_{GMM,O})&=N(X'Z\\hat{S}^{-1}Z'X)^{-1}X'Z\\hat{S}^{-1}\\hat{S}\\hat{S}^{-1}Z'X(X'Z\\hat{S}^{-1}Z'X)^{-1} \\\\\n&=N(X'Z\\hat{S}^{-1}Z'X)^{-1}\n\\end{aligned}\n\\]\nPara estimar \\(\\hat{S}\\) usamos los residuales dados por \\(\\hat{u}_i=y_i-X\\hat{\\beta}_{GMM,O}\\)"
  },
  {
    "objectID": "diapositivas/mgm.html#mínimos-cuadrados-en-dos-etapas",
    "href": "diapositivas/mgm.html#mínimos-cuadrados-en-dos-etapas",
    "title": "Método generalizado de momentos",
    "section": "Mínimos cuadrados en dos etapas",
    "text": "Mínimos cuadrados en dos etapas\nSi estamos dispuestos a asumir errores homocedásticos\n\\[\\hat{S}^{-1}=\\left(\\frac{1}{N}s^2Z'Z\\right)^{-1}\\]\nY entonces hacemos\n\\[W=\\left(\\frac{1}{N}Z'Z\\right)^{-1}\\]\nCon esta simplificación, el estimador de MGM es\n\\[\n\\begin{aligned}\n\\hat{\\beta}_{MC2E}&=(X'Z(Z'Z)^{-1}Z'X)^{-1}X'ZZ(Z'Z)^{-1}Z'y \\\\\n&=(X'P_ZX)^{-1}X'P_Zy\n\\end{aligned}\n\\]\nEste es el estimador de MC2E, también llamado estimador de variables instrumentales generalizado"
  },
  {
    "objectID": "diapositivas/mgm.html#mínimos-cuadrados-en-dos-etapas-1",
    "href": "diapositivas/mgm.html#mínimos-cuadrados-en-dos-etapas-1",
    "title": "Método generalizado de momentos",
    "section": "Mínimos cuadrados en dos etapas",
    "text": "Mínimos cuadrados en dos etapas\nA \\(P_Z=Z(Z'Z)^{-1}Z'\\) se le conoce como matriz de proyección\nY la matriz de varianzas se simplifica a\n\\[\\hat{V}(\\hat{\\beta}_{MC2E})=s^2\\left(X'P_z X\\right)^{-1}\\]\nEste estimador también se conoce como estimador de variables instrumentales generalizado porque generaliza el estimador de VI al caso sobreidentificado\nTambien se conoce como estimador de MGM de una etapa por obvias razones"
  },
  {
    "objectID": "diapositivas/mgm.html#estimador-de-variables-instrumentales",
    "href": "diapositivas/mgm.html#estimador-de-variables-instrumentales",
    "title": "Método generalizado de momentos",
    "section": "Estimador de variables instrumentales",
    "text": "Estimador de variables instrumentales\nEn el caso cuando \\(r=q\\), es decir, tantos instrumentos como variables endógenas, \\(X'Z\\) es una matriz cuadrada que puede ser invertida, resultando que\n\\[(X'ZW_NZ'X)^{-1}=(Z'X)^{-1}W_N^{-1}(X'Z)^{-1}\\]\nSustituyendo esto en la forma general del estimador de MGM obtenemos:\n\\[\n\\begin{aligned}\n\\hat{\\beta}_{VI}&=(X'ZW_NZ'X)^{-1}X'ZW_NZ'y \\\\\n&=(Z'X)^{-1}W_N^{-1}(X'Z)^{-1} X'ZW_NZ'y\\\\\n&=(Z'X)^{-1}Z'y \\\\\n\\end{aligned}\n\\]\nEste es el estimador de variables instrumentales\nEn otras palabras, el estimador de MGM es igual al de VI para cualquier matriz \\(W_N\\)"
  },
  {
    "objectID": "diapositivas/mgm.html#modelo-exactamente-identificado",
    "href": "diapositivas/mgm.html#modelo-exactamente-identificado",
    "title": "Método generalizado de momentos",
    "section": "Modelo exactamente identificado",
    "text": "Modelo exactamente identificado\nCon posible heterocedasticidad, tenemos una matriz de varianzas de la forma\n\\[\\begin{align}\\hat{V}(\\hat{\\beta}_{VI})&=N(Z'X)^{-1}\\hat{S}(X'Z)^{-1}\\end{align}\\]\ncon \\(\\hat{S}=Z'DZ/N\\), y donde \\(D=diag[\\hat{u}_i^2]\\)\nY con homocedasticidad, la matriz de varianzas de nuevo es:\n\\[\\hat{V}(\\hat{\\beta}_{VI})=s ^2\\left(X'P_z X\\right)^{-1}\\]"
  },
  {
    "objectID": "diapositivas/mgm.html#recapitulando",
    "href": "diapositivas/mgm.html#recapitulando",
    "title": "Método generalizado de momentos",
    "section": "Recapitulando",
    "text": "Recapitulando\nSiguiendo las convenciones de Cameron y Trivedi (2005)\nEl estimador de MGM es el estimador para el caso general de método de momentos, cuales quiera que sean las formas de los momentos especificados\nEl estimador óptimo de MGM ocurre cuando asumimos una forma particular para la matriz de pesos, \\(W=\\hat{S}^{-1}\\)\nEl estimador óptimo de MGM se emplea en el caso más general de modelos de variables instrumentales sobreidentificados con heterocedasticidad\nEl estimador de variables instrumentales generalizado se obtiene cuando asumimos homocedasticidad en el modelo sobreidentificado y lleva el apellido generalizado porque es la generalización del estimador IV para el caso sobreidentificado\nEl estimador de variables instrumentales surge en el modelo exactamente identificado"
  },
  {
    "objectID": "diapositivas/no-parametricos.html#motivación",
    "href": "diapositivas/no-parametricos.html#motivación",
    "title": "Métodos no paramétricos y semiparamétricos",
    "section": "Motivación",
    "text": "Motivación\n\nHemos trabajado hasta ahora modelando las distribuciones de variables aleatorias o la media condicional\nMuchos problemas nos dan cierta estructura (restricciones de homogeneidad, por ejemplo) que nos hacen asumir ciertos modelos\nEn otras ocasiones, podemos analizar los datos sin imponer supuestos distribucionales\nLos métodos no paramétricos son buenos aliados para la representación de relaciones\nFrecuentemente complementamos análisis paramétricos con sus contrapartes no paramétricas"
  },
  {
    "objectID": "diapositivas/no-parametricos.html#histograma",
    "href": "diapositivas/no-parametricos.html#histograma",
    "title": "Métodos no paramétricos y semiparamétricos",
    "section": "Histograma",
    "text": "Histograma\n\nConsideremos una variable aleatoria continua \\(x\\)\nUn histograma es un estimador de la densidad formado al partir el rango de \\(x\\) en intervalos iguales y calculando la fracción de dados en cada intervalo\nQueremos estimar la densidad \\(f(x_0)\\), es decir, la densidad de \\(x\\) en \\(x_0\\)\nUn histograma está definido por\n\n\\[\\hat{f}_{hist}(x_0)=\\frac{1}{N}\\sum_i\\frac{\\mathbf{1}(x_0-h&lt;x_i&lt;x_0+h)}{2h}\\]\n\n\\(h\\) es el ancho de banda\nA \\(2h\\) se le llama el ancho de ventana\nEl histograma pesa igual a las observaciones que están dentro el rango dado por \\((-h,h)\\)"
  },
  {
    "objectID": "diapositivas/no-parametricos.html#histograma-1",
    "href": "diapositivas/no-parametricos.html#histograma-1",
    "title": "Métodos no paramétricos y semiparamétricos",
    "section": "Histograma",
    "text": "Histograma\n\nEste peso igual queda claro al reescribir el histograma como\n\n\\[\\hat{f}_{hist}(x_0)=\\frac{1}{Nh}\\sum_i\\frac{1}{2}\\mathbf{1}\\left(\\Bigg|\\frac{x_i-x_0}{h}\\Bigg|&lt;1\\right)\\]\n\nUsemos datos sobre salarios de 175 mujeres usados en CT: salarios.csv\nUsamos 20 ventanas"
  },
  {
    "objectID": "diapositivas/no-parametricos.html#ejemplo-histograma",
    "href": "diapositivas/no-parametricos.html#ejemplo-histograma",
    "title": "Métodos no paramétricos y semiparamétricos",
    "section": "Ejemplo: histograma",
    "text": "Ejemplo: histograma\n\n\ndata.salarios&lt;-read_csv(\"salarios.csv\",\n                        locale = locale(encoding = \"latin1\"))\n\ndata.salarios %&gt;% \n  ggplot(aes(x=lnhwage)) +\n  geom_histogram(aes(y=..density..),\n                 bins=20,\n                 fill=\"#69b3a2\",\n                 color=\"#e9ecef\",\n                 alpha=0.9) +\n  ggtitle(\"Histograma del log del salario por horar\") +\n  theme(plot.title = element_text(size=15))"
  },
  {
    "objectID": "diapositivas/no-parametricos.html#densidad-kernel",
    "href": "diapositivas/no-parametricos.html#densidad-kernel",
    "title": "Métodos no paramétricos y semiparamétricos",
    "section": "Densidad Kernel",
    "text": "Densidad Kernel\n\nEl estimador de densidad Kernel (Rosenblatt, 1956) es la generalización del histograma\nDefinimos la densidad estimada Kernel como\n\n\\[\\hat{k}(x_0)=\\frac{1}{Nh}\\sum_i \\mathit{K}\\left(\\frac{x_i-x_0}{h}\\right)\\] - \\(\\mathit{K}\\) es una función de pesos Kernel\n\n\\(h\\) es el ancho de banda o parámetro de suavizaición\nLa función Kernel evalúa más datos alrededor de \\(x_0\\) que el histograma (posiblemente todos los datos)"
  },
  {
    "objectID": "diapositivas/no-parametricos.html#función-kernel",
    "href": "diapositivas/no-parametricos.html#función-kernel",
    "title": "Métodos no paramétricos y semiparamétricos",
    "section": "Función Kernel",
    "text": "Función Kernel\n\nLa función \\(\\mathit{K}\\) cumple con\n\n\\(\\mathit{K}(z)\\) es simétrica alrededor de 0 y es continua\n\\(\\int\\mathit{K}(z)dz=1\\), \\(\\int z\\mathit{K}(z)dz=0\\) y \\(\\int|\\mathit{K}(z)|dz&lt;\\infty\\)\nYa sea\n\n\\(K(z)=0\\) si \\(|z|\\geq z_0\\) para algún \\(z_0\\), o\n\\(|z|K(z)\\to 0\\) si \\(|z|\\to\\infty\\)\n\n\\(\\int z^2 \\mathit{K}(z)dz=\\kappa\\), donde \\(\\kappa\\) es una constante\n\nUna función que satisface estas condiciones puede ser una función Kernel para pesar las observaciones y estimar la densidad en \\(x_0\\)\nAl proveer \\(\\mathit{K}\\) y \\(h\\), es relativamente fácil obtener el estimador de la densidad"
  },
  {
    "objectID": "diapositivas/no-parametricos.html#funciones-kernel-comúnmente-usadas",
    "href": "diapositivas/no-parametricos.html#funciones-kernel-comúnmente-usadas",
    "title": "Métodos no paramétricos y semiparamétricos",
    "section": "Funciones Kernel comúnmente usadas",
    "text": "Funciones Kernel comúnmente usadas\n\n\n\nKernel\n\\(\\mathit{K}\\)\n\n\n\n\nUniforme, box o rectangular\n\\(\\frac{1}{2} \\mathbf{1}(abs(z)&lt;1)\\)\n\n\nTriangular\n\\((1-abs(z))\\mathbf{1}(abs(z)&lt;1)\\)\n\n\nEpanechnikov o cuadrático\n\\(\\frac{3}{4}(1-z^2)\\mathbf{1}(abs(z)&lt;1)\\)\n\n\nNormal o gausiano\n\\(\\frac{1}{\\sqrt{2\\pi}}exp(-z^2/2)\\)"
  },
  {
    "objectID": "diapositivas/no-parametricos.html#elección-del-ancho-de-banda",
    "href": "diapositivas/no-parametricos.html#elección-del-ancho-de-banda",
    "title": "Métodos no paramétricos y semiparamétricos",
    "section": "Elección del ancho de banda",
    "text": "Elección del ancho de banda\n\nUn ancho de banda pequeño reduce el sesgo (usamos observaciones muy parecidas, aunque pocas)\nUn ancho de banda grande mejora la suavización\nEl ancho de banda es una decisión más importante que la elección del tipo de Kernel\nConsideremos el error cuadrático medio como una medida de desempeño\n\n\\[MSE(\\hat{f}(x_0))=E\\left(\\hat{f}\n(x_0)-f(x_0)\\right) ^2\\]\n\nUna medida global de qué tan buena es la aproximación es el error cuadrado promedio integrado\n\n\\[MISE(h)=\\int MSE(\\hat{f}(x_0))dx_0\\]"
  },
  {
    "objectID": "diapositivas/no-parametricos.html#elección-del-ancho-de-banda-1",
    "href": "diapositivas/no-parametricos.html#elección-del-ancho-de-banda-1",
    "title": "Métodos no paramétricos y semiparamétricos",
    "section": "Elección del ancho de banda",
    "text": "Elección del ancho de banda\n\nSilverman (1986) muestra que\n\n\\[h^*=\\delta \\left(\\int f'' (x_0)^2dx_0\\right)^{-0.2}N^{-0.2}\\] donde \\(\\delta=\\left(\\frac{\\int \\mathit{K}(z)^2dz}{(\\int z^2\\mathit{K}(z)dz})^2\\right)\\)\n\nNoten que \\(h^*\\) depende de la curvatura de la densidad\nSi \\(f(x)\\) es muy variable, \\(h\\) será pequeña"
  },
  {
    "objectID": "diapositivas/no-parametricos.html#estimador-de-silverman",
    "href": "diapositivas/no-parametricos.html#estimador-de-silverman",
    "title": "Métodos no paramétricos y semiparamétricos",
    "section": "Estimador de Silverman",
    "text": "Estimador de Silverman\n\nUsamos \\(\\delta\\) estimador por Silverman, dependiendo del Kernel elegido\n\n\n\n\n\n\n\n\n\nKernel\n\\(\\mathit{K}\\)\n\\(\\delta\\)\n\n\n\n\nUniforme, box o rectangular\n\\(\\frac{1}{2}\\mathbf{1}(abs(z)&lt;1)\\)\n1.3510\n\n\nTriangular\n\\((1-abs(z))\\mathbf{1}(abs(z)&lt;1)\\)\n-\n\n\nEpanechnikov o cuadrático\n\\(\\frac{3}{4}(1-z^2)\\mathbf{1}(abs(z)&lt;1)\\)\n1.7188\n\n\nNormal o gausiano\n\\(\\frac{1}{\\sqrt{2\\pi}}exp(-z^2/2)\\)\n0.7764\n\n\n\n\nEl estimador plug-in de Silverman funciona casi siempre\n\n\\[h^*_{Silverman}=1.3643\\delta\\left(\\min \\left\\{s,\\frac{iqr}{1.349}\\right\\}\\right)N^{-0.2}\\] donde \\(s\\) es la desviación estándar de los datos, \\(iqr\\) es el rango intercuartil y \\(\\frac{iqr}{1.349}\\) protege en presencia de observaciones atípicas"
  },
  {
    "objectID": "diapositivas/no-parametricos.html#ejemplo-densidad-kernel-estimada",
    "href": "diapositivas/no-parametricos.html#ejemplo-densidad-kernel-estimada",
    "title": "Métodos no paramétricos y semiparamétricos",
    "section": "Ejemplo: densidad Kernel estimada",
    "text": "Ejemplo: densidad Kernel estimada\n\nEstimamos un Kernel epanechnikov con el ancho de banda óptimo\nPrimero calculemos el ancho de banda óptimo\nUsamos \\(\\delta\\) calculado por Silverman\n\n\ndelta &lt;- 1.7188 # Ver CT\nw.sd &lt;- sd(data.salarios$lnhwage)\nw.iqr.adj &lt;- IQR(data.salarios$lnhwage)/1.349\nw.N &lt;- nrow(data.salarios)\nconstante &lt;- 1.3643\najuste &lt;- min(w.sd,w.iqr.adj)\nh &lt;- constante*delta*ajuste*w.N^(-0.2) # ancho de banda\nh\n\n[1] 0.5453854"
  },
  {
    "objectID": "diapositivas/no-parametricos.html#ejemplo-densidad-kernel-estimada-1",
    "href": "diapositivas/no-parametricos.html#ejemplo-densidad-kernel-estimada-1",
    "title": "Métodos no paramétricos y semiparamétricos",
    "section": "Ejemplo: densidad Kernel estimada",
    "text": "Ejemplo: densidad Kernel estimada\n\nEl parámetro bw en geom_density especifica la mitad del ancho de ventana\nDebemos de especificar la mitad de \\(h\\) que acabamos de calcular\n\n\n\ndata.salarios %&gt;% \n  ggplot(aes(x=lnhwage)) +\n  geom_histogram(aes(y=..density..),\n                 bins=20,\n                 fill=\"#69b3a2\",\n                 color=\"#e9ecef\",\n                 alpha=0.9) +\n  geom_density(kernel=\"epanechnikov\",\n               bw=h/2,\n               linetype=\"solid\")+\n  theme(plot.title = element_text(size=15))"
  },
  {
    "objectID": "diapositivas/no-parametricos.html#ejemplo-densidad-kernel-estimada-2",
    "href": "diapositivas/no-parametricos.html#ejemplo-densidad-kernel-estimada-2",
    "title": "Métodos no paramétricos y semiparamétricos",
    "section": "Ejemplo: densidad Kernel estimada",
    "text": "Ejemplo: densidad Kernel estimada\n\nVemos cómo el ancho de banda seleccionado modifica el Kernel estimado\n\n\n\ndata.salarios %&gt;% \n  ggplot(aes(x=lnhwage)) +\n  geom_histogram(aes(y=..density..),\n                 bins=20, fill=\"#69b3a2\", color=\"#e9ecef\", alpha=0.9)+\n  geom_density(aes(x=lnhwage, color='Óptimo'), kernel=\"epanechnikov\", bw=h/2, adjust=1)+\n  geom_density(aes(x=lnhwage, color='1/2 óptimo'), kernel=\"epanechnikov\", bw=h/2, adjust=0.5)+\n  geom_density(aes(x=lnhwage, color='2 óptimo'), kernel=\"epanechnikov\", bw=h/2, adjust=2)+\n  theme(legend.position = 'right')+\n  scale_color_manual(\"h\",values = c('Óptimo' = 'black', '1/2 óptimo' = 'red', '2 óptimo'='blue'))"
  },
  {
    "objectID": "diapositivas/no-parametricos.html#ejemplo-densidad-kernel-estimada-3",
    "href": "diapositivas/no-parametricos.html#ejemplo-densidad-kernel-estimada-3",
    "title": "Métodos no paramétricos y semiparamétricos",
    "section": "Ejemplo: densidad Kernel estimada",
    "text": "Ejemplo: densidad Kernel estimada\n\nNotamos que la elección del tipo de Kernel es menos relevante\n\n\n\ndelta.gausiano &lt;- 0.7764\ndelta.uniforme &lt;- 1.3510\ndelta.cuartico &lt;- 2.0362\n\nh.gausiano &lt;- constante*delta.gausiano*ajuste*w.N^(-0.2)\nh.uniforme &lt;- constante*delta.uniforme*ajuste*w.N^(-0.2)\nh.cuartico &lt;- constante*delta.cuartico*ajuste*w.N^(-0.2)\n\ndata.salarios %&gt;%\n  ggplot(aes(x=lnhwage)) +\n  geom_histogram(aes(y=..density..), bins=20, fill=\"#69b3a2\", color=\"#e9ecef\", alpha=0.9)+\n  geom_density(aes(x=lnhwage, color='epanechnikov'), kernel=\"epanechnikov\", bw=h/2, adjust=1)+\n  geom_density(aes(x=lnhwage, color='gausiano'), kernel=\"gaussian\", bw=h.gausiano/2, adjust=1)+\n  geom_density(aes(x=lnhwage, color='uniforme'), kernel=\"rectangular\", bw=h.uniforme/2, adjust=1)+\n  geom_density(aes(x=lnhwage, color='cuártico'), kernel=\"biweight\", bw=h.cuartico/2, adjust=1)+\n  theme(legend.position = 'right')+\n  scale_color_manual(\"Kernel\", values = c('epanechnikov' = 'black', 'gausiano' = 'red', 'uniforme'='blue', 'cuártico'='green'))"
  },
  {
    "objectID": "diapositivas/no-parametricos.html#regresión-local-no-paramétrica-1",
    "href": "diapositivas/no-parametricos.html#regresión-local-no-paramétrica-1",
    "title": "Métodos no paramétricos y semiparamétricos",
    "section": "Regresión local no paramétrica",
    "text": "Regresión local no paramétrica\n\nConsideremos el siguiente modelo con un regresor escalar\n\n\\[y_i=m(x_i)+\\varepsilon_i\\]\n\nTenemos \\(N\\) observaciones y \\(\\varepsilon_i\\sim iid(0,\\sigma^2_{\\varepsilon})\\)\nQueremos usar las \\(x\\) para decir algo sobre \\(y\\) pero no queremos darle un modelo paramétrico"
  },
  {
    "objectID": "diapositivas/no-parametricos.html#promedios-locales-ponderados",
    "href": "diapositivas/no-parametricos.html#promedios-locales-ponderados",
    "title": "Métodos no paramétricos y semiparamétricos",
    "section": "Promedios locales ponderados",
    "text": "Promedios locales ponderados\n\nSupongamos que para un valor de \\(x\\), digamos \\(x_0\\), observamos \\(N_0\\) valores de \\(y\\)\nUna forma de estimar \\(m(x_0)\\) es con la media muestral \\(\\tilde{m}(x_0)\\)\nEl problema es que este estimador puede ser muy ruidoso con regresores discretos y muestras pequeñas\nUna alternativa es mirar en una vecindad de \\(x_0\\)\nDefinimos un estimador de medias ponderadas local\n\n\\[\\hat{m}(x_0)=\\sum_{i=1}^Nw_{i0,h}y_i\\]\n\nLos pesos \\(w_{i0,h}\\), con \\(\\sum_i w_{i0,h}=1\\) indican cuánto pesan las observaciones alrededor de \\(x_0\\), dándole más peso a las más cercanas\n\\(h\\) es el ancho de ventana"
  },
  {
    "objectID": "diapositivas/no-parametricos.html#promedios-locales-ponderados-1",
    "href": "diapositivas/no-parametricos.html#promedios-locales-ponderados-1",
    "title": "Métodos no paramétricos y semiparamétricos",
    "section": "Promedios locales ponderados",
    "text": "Promedios locales ponderados\n\nSi decidimos que cada observación en la vecindad dada por \\(h\\) pese lo mismo, el estimador local es\n\n\\[\\hat{m}(x_0)=\\frac{\\sum_i\\mathbf{1}\\left(\\Bigg|\\frac{x_i-x_0}{h}\\Bigg|&lt;1\\right)y_i}{\\sum_i\\mathbf{1}\\left(\\Bigg|\\frac{x_i-x_0}{h}\\Bigg|&lt;1\\right)}\\] - El numerador es la suma de \\(y_i\\) para las observaciones dentro del ancho de banda\n\nEl denominador es el número de unidades sobre las que se está sumando"
  },
  {
    "objectID": "diapositivas/no-parametricos.html#promedios-locales-ponderados-2",
    "href": "diapositivas/no-parametricos.html#promedios-locales-ponderados-2",
    "title": "Métodos no paramétricos y semiparamétricos",
    "section": "Promedios locales ponderados",
    "text": "Promedios locales ponderados\n\nDe hecho, las predicciones de MCO también pueden expresarse como un estimador de medias ponderadas\nAlgo de álgebra resulta en\n\n\\[\\hat{m}_{MCO}(x_0)=\\sum_i\\left\\{\\frac{1}{N}+\\frac{(x_0-\\bar{x})(x_i-\\bar{x})}{\\sum_j (x_j-\\bar{x})^2}\\right\\}y_i\\]"
  },
  {
    "objectID": "diapositivas/no-parametricos.html#estimador-de-vecinos-más-cercanos",
    "href": "diapositivas/no-parametricos.html#estimador-de-vecinos-más-cercanos",
    "title": "Métodos no paramétricos y semiparamétricos",
    "section": "Estimador de vecinos más cercanos",
    "text": "Estimador de vecinos más cercanos\n\nOtra forma de estimación local no paramétrica es la de vecinos más cercanos\nConsideremos una vecindad de \\(k\\) vecinos alrededor de \\(X_0\\), donde tenemos \\((k-1)/2\\) observaciones con \\(x&lt;x_0\\) y otras tantas para \\(x&gt;x_0\\)\nEntonces, el estimador de k vecinos más cercanos, de media móvil o de media móvil local\n\n\\[\\hat{m}_{k-NN}(x_0)=\\frac{1}{k}\\sum_i \\mathbf{1}(x_i\\in N_k(x_0))y_i\\]\n\nAquí \\(k\\) juega el papel de \\(h\\)\n\\(h\\) es variable: \\(h_0\\) es igual a la distancia entre \\(x_0\\) y el más lejano de los \\(k\\) vecinos más cercanos\nA \\(k/N\\) se le conoce como lapso o span"
  },
  {
    "objectID": "diapositivas/no-parametricos.html#regresión-kernel",
    "href": "diapositivas/no-parametricos.html#regresión-kernel",
    "title": "Métodos no paramétricos y semiparamétricos",
    "section": "Regresión kernel",
    "text": "Regresión kernel\n\nUna extensión natural el estimador de promedios locales ponderados es usar una función Kernel para pesar más las observaciones más cercanas a \\(x_0\\)\nEsto resulta en el estimador de regresión Kernel\n\n\\[\\hat{m}(x_0)=\\frac{\\frac{1}{Nh}\\sum_i\\mathit{K}\\left(\\frac{x_i-x_0}{h}\\right)y_i}{\\frac{1}{Nh}\\sum_i\\mathit{K}\\left(\\frac{x_i-x_0}{h}\\right)}\\]"
  },
  {
    "objectID": "diapositivas/no-parametricos.html#regresión-local-lineal",
    "href": "diapositivas/no-parametricos.html#regresión-local-lineal",
    "title": "Métodos no paramétricos y semiparamétricos",
    "section": "Regresión local lineal",
    "text": "Regresión local lineal\n\nPodemos asumir que \\(m(x)\\) es lineal en la vecindad de \\(x_0\\)\n\n\\[m(x)=a_0+b_o(x-x_0)\\]\n\nEl estimador de regresión local lineal se obtiene al esocoger \\(a_0\\) y \\(b_0\\) que minimizan\n\n\\[\\sum_i \\mathit{K}\\left(\\frac{x_i-x_0}{h}\\right)(y_i-a_0-b_0(x_i-x_0))^2\\]\n\nObtenemos \\(\\hat{m}(x)=\\hat{a}_0+\\hat{b}_0(x-x_0)\\)\nEn general, podemos asumir un polinomio de orden \\(p\\) para \\(m(x)\\) y resolver\n\n\\[\\sum_i \\mathit{K}\\left(\\frac{x_i-x_0}{h}\\right)\\left(y_i-a_{0,0}-a_{0,1}(x_i-x_0)-\\ldots-a_{0,p}\\frac{(x_i-x_0)^p}{p!}\\right)^2\\]"
  },
  {
    "objectID": "diapositivas/no-parametricos.html#estimador-lowess",
    "href": "diapositivas/no-parametricos.html#estimador-lowess",
    "title": "Métodos no paramétricos y semiparamétricos",
    "section": "Estimador Lowess",
    "text": "Estimador Lowess\n\nUn estimador comúnmente usado es el estimador suavizador de puntos con pesos locales (locally weighted scatterplot smoothing estimator) o Lowess\nCaso especial del estimador polinomial\n\nUsa un ancho de banda variable dado por la distancia entre \\(x_0\\) y el \\(k\\)-ésimo vecino más cercano\nKernel tricúbico, \\(\\mathit{K}=(70/81)(1-abs(z)^3)^3\\mathbf{1}(abs(z)&lt;1)\\)\nOtorga menor peso a observaciones con residuales grandes\n\nResulta robusto a la presencia de observaciones atípicas\nSe desempeña bien por su ventana variable y no es sensible a observaciones atípicas\nEs intensivo en cómputo"
  },
  {
    "objectID": "diapositivas/no-parametricos.html#ejemplo-regresión-no-paramétrica",
    "href": "diapositivas/no-parametricos.html#ejemplo-regresión-no-paramétrica",
    "title": "Métodos no paramétricos y semiparamétricos",
    "section": "Ejemplo: regresión no paramétrica",
    "text": "Ejemplo: regresión no paramétrica\n\nSimulamos un proceso\n\n\nset.seed(911)\nN &lt;- 100\nu &lt;- rnorm(n=N, mean=0, sd=25)\nx &lt;- seq(1:N)\ny &lt;- 150 + 6.5*x -0.15*x^2+0.001*x^3+u\n\ndata.sim &lt;- as.data.frame(cbind(x,y))\n\n\nUsamos la función knn.reg de la paquetería FNN\n\n\nknn5 &lt;- knn.reg(data.sim$x, y=data.sim$y, k=5)\nknn25 &lt;- knn.reg(data.sim$x, y=data.sim$y, k=25)\n\ndata.sim &lt;- data.sim %&gt;% \n  mutate(y5=knn5$pred) %&gt;% \n  mutate(y25=knn25$pred)"
  },
  {
    "objectID": "diapositivas/no-parametricos.html#ejemplo-regresión-no-paramétrica-1",
    "href": "diapositivas/no-parametricos.html#ejemplo-regresión-no-paramétrica-1",
    "title": "Métodos no paramétricos y semiparamétricos",
    "section": "Ejemplo: regresión no paramétrica",
    "text": "Ejemplo: regresión no paramétrica\n\nNotamos que \\(k\\) más grande genera una función más suave\n\n\n\ndata.sim %&gt;% \n  ggplot(aes(x=x,y=y))+\n  geom_point()+\n  geom_smooth(aes(x=x, y=y, color='MCO'),\n              method=lm,\n              se=FALSE)+\n  geom_line(aes(x=x, y=y5, color='kNN, k=5'))+\n  geom_line(aes(x=x, y=y25, color='kNN, k=25'))+\n  theme(legend.position = 'right')+\n  scale_color_manual(\"Método\",\n                     values = c('kNN, k=5' = 'blue', 'kNN, k=25'='green', 'MCO'='black'))"
  },
  {
    "objectID": "diapositivas/no-parametricos.html#ejemplo-regresión-no-paramétrica-2",
    "href": "diapositivas/no-parametricos.html#ejemplo-regresión-no-paramétrica-2",
    "title": "Métodos no paramétricos y semiparamétricos",
    "section": "Ejemplo: regresión no paramétrica",
    "text": "Ejemplo: regresión no paramétrica\n\nY ahora incluimos la función generada por Lowess\n\n\n\nfit.lowess &lt;- lowess(data.sim$x, y=data.sim$y, f=25/100)\n\ndata.sim &lt;- data.sim %&gt;% \n  mutate(y.lowess=fit.lowess$y)\n  \ndata.sim %&gt;% \n  ggplot(aes(x=x, y=y))+\n  geom_point()+\n  geom_smooth(aes(x=x, y=y, color='MCO cúbico'),\n              method=lm,\n              formula= y ~ x+I(x^2)+I(x^3),\n              se=FALSE)+\n  geom_line(aes(x=x, y=y.lowess, color='Lowess k=25'))+\n  theme(legend.position = 'right')+\n  scale_color_manual(\"Método\",\n                     values = c('MCO cúbico' = 'blue', 'Lowess k=25' = 'red'))"
  },
  {
    "objectID": "diapositivas/no-parametricos.html#regresión-semiparamétrica-1",
    "href": "diapositivas/no-parametricos.html#regresión-semiparamétrica-1",
    "title": "Métodos no paramétricos y semiparamétricos",
    "section": "Regresión semiparamétrica",
    "text": "Regresión semiparamétrica\n\nLa teoría económica puede informar sobre la estructura de problemas y queremos introducir esta estructura en los modelos\nCon muchos regresores, la estimación no paramétrica se vuelve impráctica\nPodemos usar modelos híbridos que tengan una parte paramétrica y otra no paramétrica\nAlgunos de estos modelos son más usados que otros en microeconometría"
  },
  {
    "objectID": "diapositivas/no-parametricos.html#modelos-lineal-parcial",
    "href": "diapositivas/no-parametricos.html#modelos-lineal-parcial",
    "title": "Métodos no paramétricos y semiparamétricos",
    "section": "Modelos lineal parcial",
    "text": "Modelos lineal parcial\n\nEspecificamos\n\n\\[E(y|x,z)=x'\\beta+\\lambda(z)\\]\ndonde \\(\\lambda(\\cdot)\\) es una función escalar no especificada\n\nPor ejemplo, la demanda de cierto bien puede ser lineal en \\(x\\) y no lineal en \\(z\\)\nEl modelo de regresión es\n\n\\[\n\\begin{aligned}\n&y_i=x_i'\\beta+\\lambda(z_i)+\\varepsilon_i \\\\\n&E(\\varepsilon_i | x_i,z_i)=0\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "diapositivas/no-parametricos.html#transformación-de-robinson",
    "href": "diapositivas/no-parametricos.html#transformación-de-robinson",
    "title": "Métodos no paramétricos y semiparamétricos",
    "section": "Transformación de Robinson",
    "text": "Transformación de Robinson\n\nRobinson (1988) propone lo siguiente\n\nAplicando el operador de esperanza y por la ley de expectativas iteradas\n\n\\[E(y_i|z_i)=E(x_i'\\beta|z_i)+E(\\lambda(z_i)|z_i)+E(\\varepsilon_i|z_i) = E(x_i|z_i)'\\beta+\\lambda(z_i)\\]\n\nRestando a la ecuación de \\(y_i\\) original\n\n\\[y_i-E(y_i|z_i)=(x_i-E(x_i|z_i))'\\beta+\\varepsilon_i\\]"
  },
  {
    "objectID": "diapositivas/no-parametricos.html#estimador-de-robinson",
    "href": "diapositivas/no-parametricos.html#estimador-de-robinson",
    "title": "Métodos no paramétricos y semiparamétricos",
    "section": "Estimador de Robinson",
    "text": "Estimador de Robinson\n\nEl estimador de Robinson consiste en hacer MCO a la ecuación transformada\n\n\\[y_i-\\hat{m}_{yi}=(x-\\hat{m}_{xi})'\\beta_{LP}+\\nu\\]\n\n\\(\\hat{m}_{yi}\\) y \\(\\hat{m}_{xi}\\) son predicciones obtenidas por regresiones no paramétricas de \\(y\\) y \\(x\\) sobre \\(z\\)\nRobinson (1988) muestra que el estimador LP es consistente y asintóticamente normal\nVer CT, Sección 9.7.3, con las expresiones de la varianza del estimador de Robinson"
  },
  {
    "objectID": "diapositivas/no-parametricos.html#ejemplo-estimador-de-robinson",
    "href": "diapositivas/no-parametricos.html#ejemplo-estimador-de-robinson",
    "title": "Métodos no paramétricos y semiparamétricos",
    "section": "Ejemplo: estimador de Robinson",
    "text": "Ejemplo: estimador de Robinson\n\nUsamos la función npplreg de la paquetería np\nUsaremos una base de datos que viene con np con 524 observaciones\nTenemos datos de edad, raza, salarios, educación, experiencia\n\n\ndata(wage1)\ncolnames(wage1)\n\n [1] \"wage\"     \"educ\"     \"exper\"    \"tenure\"   \"nonwhite\" \"female\"  \n [7] \"married\"  \"numdep\"   \"smsa\"     \"northcen\" \"south\"    \"west\"    \n[13] \"construc\" \"ndurman\"  \"trcommpu\" \"trade\"    \"services\" \"profserv\"\n[19] \"profocc\"  \"clerocc\"  \"servocc\"  \"lwage\"    \"expersq\"  \"tenursq\""
  },
  {
    "objectID": "diapositivas/no-parametricos.html#ejemplo-estimador-de-robinson-1",
    "href": "diapositivas/no-parametricos.html#ejemplo-estimador-de-robinson-1",
    "title": "Métodos no paramétricos y semiparamétricos",
    "section": "Ejemplo: estimador de Robinson",
    "text": "Ejemplo: estimador de Robinson\n\nPropongamos un modelo donde el salario por hora depende no paramétricamente de la experiencia\n\n\\[\\ln w_i= \\beta_0+\\beta_1 educ_i + \\beta_2 tenure_i + \\lambda(exper_i) \\]\n\nLa función npplregbw nos permite obtener los anchos de banda apropiados para cada variable\n\n\nbw &lt;- npplregbw(formula=lwage~educ+ tenure |\n                  exper,\n                data=wage1,\n                regtype=\"ll\")\n\n\nPasamos el objeto bw a la función npplreg\n\n\nmodel.pl &lt;- npplreg(bw)"
  },
  {
    "objectID": "diapositivas/no-parametricos.html#ejemplo-estimador-de-robinson-2",
    "href": "diapositivas/no-parametricos.html#ejemplo-estimador-de-robinson-2",
    "title": "Métodos no paramétricos y semiparamétricos",
    "section": "Ejemplo: estimador de Robinson",
    "text": "Ejemplo: estimador de Robinson\n\nVemos el resumen\n\n\n\nsummary(model.pl)\n\n\n\nPartially Linear Model\nRegression data: 526 training points, in 3 variable(s)\nWith 2 linear parametric regressor(s), 1 nonparametric regressor(s)\n\n                  y(z)\nBandwidth(s): 3.176922\n\n                  x(z)\nBandwidth(s): 3.945161\n              1.044574\n\n                      educ     tenure\nCoefficient(s): 0.08373704 0.02174876\n\nKernel Regression Estimator: Local-Linear\nBandwidth Type: Fixed\n\nResidual standard error: 0.4203625\nR-squared: 0.3734032\n\nContinuous Kernel Type: Second-Order Gaussian\nNo. Continuous Explanatory Vars.: 1"
  },
  {
    "objectID": "diapositivas/no-parametricos.html#ejemplo-estimador-de-robinson-3",
    "href": "diapositivas/no-parametricos.html#ejemplo-estimador-de-robinson-3",
    "title": "Métodos no paramétricos y semiparamétricos",
    "section": "Ejemplo: estimador de Robinson",
    "text": "Ejemplo: estimador de Robinson\n\nGraficamos resultados\n\n\n\npar(mar = c(1, 1, 1, 1))\n\nnpplot(bw,\n       plot.errors.method = \"asymptotic\",\n       plot.behavior=\"plot-data\")\n\n\n\n\n\n\n\n\n\n$plr1\n\nPartially Linear Model\nRegression data: 526 training points, and 50 evaluation points, in 3 variable(s)\nWith 2 linear parametric regressor(s), 1 nonparametric regressor(s)\n\n                  y(z)\nBandwidth(s): 3.176922\n\n                  x(z)\nBandwidth(s): 3.945161\n              1.044574\n                        V1         V2\nCoefficient(s): 0.08373704 0.02174876\n\nKernel Regression Estimator: Local-Linear\nBandwidth Type: Fixed\n\nContinuous Kernel Type: Second-Order Gaussian\nNo. Continuous Explanatory Vars.: 1\n\n\n$plr2\n\nPartially Linear Model\nRegression data: 526 training points, and 50 evaluation points, in 3 variable(s)\nWith 2 linear parametric regressor(s), 1 nonparametric regressor(s)\n\n                  y(z)\nBandwidth(s): 3.176922\n\n                  x(z)\nBandwidth(s): 3.945161\n              1.044574\n                        V1         V2\nCoefficient(s): 0.08373704 0.02174876\n\nKernel Regression Estimator: Local-Linear\nBandwidth Type: Fixed\n\nContinuous Kernel Type: Second-Order Gaussian\nNo. Continuous Explanatory Vars.: 1\n\n\n$plr3\n\nPartially Linear Model\nRegression data: 526 training points, and 50 evaluation points, in 3 variable(s)\nWith 2 linear parametric regressor(s), 1 nonparametric regressor(s)\n\n                  y(z)\nBandwidth(s): 3.176922\n\n                  x(z)\nBandwidth(s): 3.945161\n              1.044574\n                        V1         V2\nCoefficient(s): 0.08373704 0.02174876\n\nKernel Regression Estimator: Local-Linear\nBandwidth Type: Fixed\n\nContinuous Kernel Type: Second-Order Gaussian\nNo. Continuous Explanatory Vars.: 1"
  },
  {
    "objectID": "diapositivas/no-parametricos.html#ejemplo-estimador-de-robinson-4",
    "href": "diapositivas/no-parametricos.html#ejemplo-estimador-de-robinson-4",
    "title": "Métodos no paramétricos y semiparamétricos",
    "section": "Ejemplo: estimador de Robinson",
    "text": "Ejemplo: estimador de Robinson\n\nPodemos especificar otras formas de estimar los intervalos de confianza\nRecuperamos los objetos estimados guardados en g.robinson para hacer un gráfico con mejor edición\n\n\n\npar(mar = c(1, 1, 1, 1))\n\ng.robinson &lt;- npplot(bw,\n       plot.errors.method = \"bootstrap\",\n       plot.behavior=\"plot-data\")"
  },
  {
    "objectID": "diapositivas/no-parametricos.html#ejemplo-estimador-de-robinson-5",
    "href": "diapositivas/no-parametricos.html#ejemplo-estimador-de-robinson-5",
    "title": "Métodos no paramétricos y semiparamétricos",
    "section": "Ejemplo: estimador de Robinson",
    "text": "Ejemplo: estimador de Robinson\n\nConstruimos el gráfico solo para la experiencia\n\n\n\ng &lt;- fitted(g.robinson$plr3)\nse &lt;- g.robinson[[\"plr3\"]][[\"merr\"]]\nlci &lt;- g - se[,1]\nuci &lt;- g + se[,2]\n\n#Este objeto nos dicen dónde fueron evaluados\nexp.eval &lt;- g.robinson[[\"plr3\"]][[\"evalz\"]][[\"V1\"]]\n\nfitted &lt;- data.frame(exp.eval, g,lci,uci)\n\nggplot() + \n  geom_point(data=wage1, aes(exper,lwage), color='black', alpha=0.5) + \n  geom_line(data=fitted, aes(exp.eval, g), linetype='solid')+\n  geom_line(data=fitted, aes(exp.eval, uci), linetype='dashed')+\n  geom_line(data=fitted, aes(exp.eval, lci), linetype='dashed')"
  },
  {
    "objectID": "diapositivas/variables-instrumentales-practica.html#cuestiones-prácticas-de-vi-1",
    "href": "diapositivas/variables-instrumentales-practica.html#cuestiones-prácticas-de-vi-1",
    "title": "Variables instrumentales en R",
    "section": "Cuestiones prácticas de VI",
    "text": "Cuestiones prácticas de VI\n\nHemos aprendido que la forma general del estimador de MGMes:\n\n\\[\\hat{\\beta}_{GMM}=(X'ZW_NZ'X)^{-1}X'ZW_NZ'y\\]\n\nY vimos también la forma general del estimador de la varianza:\n\n\\[\\hat{V}(\\hat{\\beta}_{GMM})=N(X'ZW_NZ'X)^{-1}(X'ZW_N\\hat{S}W_NZ'X)(X'ZW_NZ'X)^{-1}\\]"
  },
  {
    "objectID": "diapositivas/variables-instrumentales-practica.html#estimador-óptimo-de-mgm",
    "href": "diapositivas/variables-instrumentales-practica.html#estimador-óptimo-de-mgm",
    "title": "Variables instrumentales en R",
    "section": "Estimador óptimo de MGM",
    "text": "Estimador óptimo de MGM\nPara obtener el estimador óptimo escogemos una forma particular para la matriz de pesos:\n\\[W=\\hat{S}^{-1}\\]\nY entonces el estimador de MGM se vuelve:\n\\[\\hat{\\beta}_{GMM,O}=(X'Z\\hat{S}^{-1}Z'X)^{-1}X'Z\\hat{S}^{-1}Z'y\\]\nY el estimador de varianza se simplifica a:\n\\[\\hat{V}(\\hat{\\beta}_{GMM,O})=N(X'Z\\hat{S}^{-1}Z'X)^{-1}\\]\nHasta aquí no asumimos nada sobre la forma de los errores\nLo único que nos permitió pasar de la forma general al estimador óptimo es la elección de \\(W\\)\nCon esto obtenemos el estimador más eficiente"
  },
  {
    "objectID": "diapositivas/variables-instrumentales-practica.html#álgebra-de-matrices",
    "href": "diapositivas/variables-instrumentales-practica.html#álgebra-de-matrices",
    "title": "Variables instrumentales en R",
    "section": "Álgebra de matrices",
    "text": "Álgebra de matrices\nUsaremos los datos del estudio de Card (1995) sobre rendimientos a la educación para mostrar cómo funcionan las expresiones para estimar el vector de coeficientes y los errores estándar de los distintos estimadores de VI.\nCard usa la proximidad a una institución de educación superior como instrumento de los años de educación acumulados.\n\ndata.ingresos &lt;- read_csv(\"ingresos_iv.csv\",\n                          locale = locale(encoding = \"latin1\"))"
  },
  {
    "objectID": "diapositivas/variables-instrumentales-practica.html#modelo-exactamente-identificado",
    "href": "diapositivas/variables-instrumentales-practica.html#modelo-exactamente-identificado",
    "title": "Variables instrumentales en R",
    "section": "Modelo exactamente identificado",
    "text": "Modelo exactamente identificado\nPara tener una referencia, veamos lo que obtenemos con ivreg del paquete AER. Nuestro modelo tiene cinco regresores más una constante:\n\n\niv_ei &lt;- ivreg(lwage ~ educ + exper + expersq + black + south |\n                 . - educ + nearc4, data = data.ingresos)\n\nmodelsummary(list(iv_ei),\n          output=\"gt\",\n          coef_map = c(\"educ\", \"exper\"),\n          fmt = 4)\n\n\n\n\n\n\n\n\n\n(1)\n\n\n\n\neduc\n0.2214\n\n\n\n(0.0409)\n\n\nexper\n0.1439\n\n\n\n(0.0187)\n\n\nNum.Obs.\n3010\n\n\nR2\n-0.134\n\n\nR2 Adj.\n-0.136\n\n\nAIC\n4043.8\n\n\nBIC\n4085.9\n\n\nRMSE\n0.47"
  },
  {
    "objectID": "diapositivas/variables-instrumentales-practica.html#modelo-exactamente-identificado-1",
    "href": "diapositivas/variables-instrumentales-practica.html#modelo-exactamente-identificado-1",
    "title": "Variables instrumentales en R",
    "section": "Modelo exactamente identificado",
    "text": "Modelo exactamente identificado\nRepliquemos lo anterior con matrices. Primero construimos \\(X\\), \\(Y\\) y \\(Z\\):\n\ndata.ingresos &lt;- data.ingresos %&gt;% \n  mutate(constant=1)\n\nX &lt;- data.matrix(select(data.ingresos, constant, educ, exper, expersq, black,\n              south),\n       rownames.force = T)\n\nY &lt;- data.matrix(select(data.ingresos,lwage),\n       rownames.force = T)\n\nZ &lt;- data.matrix(select(data.ingresos, constant, nearc4, exper, expersq, black,\n              south),\n       rownames.force = T)\n\nN &lt;- nrow(X)\nk &lt;- ncol(X) # incluyendo la constante"
  },
  {
    "objectID": "diapositivas/variables-instrumentales-practica.html#modelo-exactamente-identificado-2",
    "href": "diapositivas/variables-instrumentales-practica.html#modelo-exactamente-identificado-2",
    "title": "Variables instrumentales en R",
    "section": "Modelo exactamente identificado",
    "text": "Modelo exactamente identificado\nEstimamos beta\n\nb &lt;- solve(t(Z) %*% X) %*% t(Z) %*% Y\nb\n\n                lwage\nconstant  2.324805209\neduc      0.221390289\nexper     0.143933017\nexpersq  -0.002401759\nblack    -0.036938558\nsouth    -0.088888463"
  },
  {
    "objectID": "diapositivas/variables-instrumentales-practica.html#modelo-exactamente-identificado-3",
    "href": "diapositivas/variables-instrumentales-practica.html#modelo-exactamente-identificado-3",
    "title": "Variables instrumentales en R",
    "section": "Modelo exactamente identificado",
    "text": "Modelo exactamente identificado\nLa matriz de varianzas, asumiendo homocedasticidad:\n\nu_hat &lt;- Y-X%*%b\nsigma2 &lt;- as.numeric((1/N)*t(u_hat)%*%u_hat)\n\nConstruimos la matriz de proyección\n\nP &lt;- Z%*%(solve(t(Z)%*%Z))%*%t(Z)\n\nLa matriz de varianzas que construye R por defecto multiplica por \\(N/N-k\\):\n\nV=sigma2*solve(t(X)%*%P%*%X)*(N/(N-k))\nsqrt(diag(V))\n\n    constant         educ        exper      expersq        black        south \n0.7071765685 0.0409013368 0.0186980191 0.0004020113 0.0458381726 0.0257238774"
  },
  {
    "objectID": "diapositivas/variables-instrumentales-practica.html#modelo-exactamente-identificado-4",
    "href": "diapositivas/variables-instrumentales-practica.html#modelo-exactamente-identificado-4",
    "title": "Variables instrumentales en R",
    "section": "Modelo exactamente identificado",
    "text": "Modelo exactamente identificado\nComparamos el coeficiente y el de educación con lo obtenido con ivreg:\n\n\nmodelsummary(list(iv_ei),\n          output=\"gt\",\n          coef_map = c(\"educ\", \"exper\"),\n          gof_map = c(\"nobs\"),\n          fmt = 4)\n\n\n\n\n\n\n\n\n\n(1)\n\n\n\n\neduc\n0.2214\n\n\n\n(0.0409)\n\n\nexper\n0.1439\n\n\n\n(0.0187)\n\n\nNum.Obs.\n3010"
  },
  {
    "objectID": "diapositivas/variables-instrumentales-practica.html#modelo-exactamente-identificado-5",
    "href": "diapositivas/variables-instrumentales-practica.html#modelo-exactamente-identificado-5",
    "title": "Variables instrumentales en R",
    "section": "Modelo exactamente identificado",
    "text": "Modelo exactamente identificado\nSi permitimos una heterocedasticidad arbitraria:\n\n\nmodelsummary(list(\"Clásicos\"=iv_ei, \"HC0\"=iv_ei, \"HC3 (default)\"=iv_ei),\n          output=\"gt\",\n          coef_map = c(\"educ\", \"exper\"),\n          gof_map = c(\"nobs\"),\n          vcov = c(\"iid\",\"HC0\", \"robust\"),\n          fmt = 4)\n\n\n\n\n\n\n\n\n\nClásicos\nHC0\nHC3 (default)\n\n\n\n\neduc\n0.2214\n0.2214\n0.2214\n\n\n\n(0.0409)\n(0.0403)\n(0.0404)\n\n\nexper\n0.1439\n0.1439\n0.1439\n\n\n\n(0.0187)\n(0.0185)\n(0.0186)\n\n\nNum.Obs.\n3010\n3010\n3010"
  },
  {
    "objectID": "diapositivas/variables-instrumentales-practica.html#modelo-exactamente-identificado-6",
    "href": "diapositivas/variables-instrumentales-practica.html#modelo-exactamente-identificado-6",
    "title": "Variables instrumentales en R",
    "section": "Modelo exactamente identificado",
    "text": "Modelo exactamente identificado\nRepliquemos esto con matrices, obteniendo primero la matriz \\(D\\), que colecciona los errores ajustados, y luego la matriz \\(S\\):\n\nD &lt;- diag(as.vector((Y-X%*%b)^2))\nS_hat &lt;- (1/(N)) * t(Z) %*% D %*% Z \n\nNoten que HC0 no hace corrección por muestras pequeñas:\n\nVr= N*solve(t(X)%*%Z%*%solve(t(Z)%*%Z)%*%t(Z)%*%X)%*%(t(X)%*%Z%*%solve(t(Z)%*%Z)%*%S_hat%*%solve(t(Z)%*%Z)%*%t(Z)%*%X)%*%solve(t(X)%*%Z%*%solve(t(Z)%*%Z)%*%t(Z)%*%X)\nsqrt(diag(Vr))\n\n    constant         educ        exper      expersq        black        south \n0.6957801830 0.0403033738 0.0185093210 0.0004295362 0.0442735580 0.0253631935"
  },
  {
    "objectID": "diapositivas/variables-instrumentales-practica.html#modelo-exactamente-identificado-7",
    "href": "diapositivas/variables-instrumentales-practica.html#modelo-exactamente-identificado-7",
    "title": "Variables instrumentales en R",
    "section": "Modelo exactamente identificado",
    "text": "Modelo exactamente identificado\nComparamos:\n\n\nmodelsummary(list(\"Clásicos\"=iv_ei, \"HC0\"=iv_ei, \"HC3 (default)\"=iv_ei),\n          output=\"gt\",\n          coef_map = c(\"educ\", \"exper\"),\n          gof_map = c(\"nobs\"),\n          vcov = c(\"iid\",\"HC0\", \"robust\"),\n          fmt = 4)\n\n\n\n\n\n\n\n\n\nClásicos\nHC0\nHC3 (default)\n\n\n\n\neduc\n0.2214\n0.2214\n0.2214\n\n\n\n(0.0409)\n(0.0403)\n(0.0404)\n\n\nexper\n0.1439\n0.1439\n0.1439\n\n\n\n(0.0187)\n(0.0185)\n(0.0186)\n\n\nNum.Obs.\n3010\n3010\n3010"
  },
  {
    "objectID": "diapositivas/variables-instrumentales-practica.html#modelo-sobreidentificado",
    "href": "diapositivas/variables-instrumentales-practica.html#modelo-sobreidentificado",
    "title": "Variables instrumentales en R",
    "section": "Modelo sobreidentificado",
    "text": "Modelo sobreidentificado\nConsideremos ahora el modelo sobreidentificado con dos instrumentos:\n\n\niv_si &lt;- ivreg(lwage ~ educ + exper + expersq + black + south |\n                 . - educ + nearc4 + nearc2, data = data.ingresos)\n\nmodelsummary(list(\"Clásicos\"=iv_si),\n          output=\"gt\",\n          coef_map = c(\"educ\", \"exper\"),\n          gof_map = c(\"nobs\"),\n          fmt = 4)\n\n\n\n\n\n\n\n\n\nClásicos\n\n\n\n\neduc\n0.2403\n\n\n\n(0.0406)\n\n\nexper\n0.1517\n\n\n\n(0.0188)\n\n\nNum.Obs.\n3010"
  },
  {
    "objectID": "diapositivas/variables-instrumentales-practica.html#modelo-sobreidentificado-1",
    "href": "diapositivas/variables-instrumentales-practica.html#modelo-sobreidentificado-1",
    "title": "Variables instrumentales en R",
    "section": "Modelo sobreidentificado",
    "text": "Modelo sobreidentificado\nConstruyamos la nueva matriz de instrumentos y la nueva matriz de proyección para obtener el vector de coeficientes:\n\nZ &lt;- data.matrix(select(data.ingresos, constant, nearc4, nearc2, exper, expersq, black,\n                        south),\n                 rownames.force = T)\n\nP &lt;- Z%*%(solve(t(Z)%*%Z))%*%t(Z)\n\nb &lt;- solve(t(X)%*%P%*%X) %*% t(X)%*%P%*%Y\nb\n\n                lwage\nconstant  1.998075910\neduc      0.240315358\nexper     0.151707063\nexpersq  -0.002409864\nblack    -0.018284247\nsouth    -0.080744611"
  },
  {
    "objectID": "diapositivas/variables-instrumentales-practica.html#modelo-sobreidentificado-2",
    "href": "diapositivas/variables-instrumentales-practica.html#modelo-sobreidentificado-2",
    "title": "Variables instrumentales en R",
    "section": "Modelo sobreidentificado",
    "text": "Modelo sobreidentificado\nLa matriz de varianzas se estima igual que en el caso exactamente identificado:\n\nu_hat &lt;- Y-X%*%b\nsigma2 &lt;- as.numeric((1/N)*t(u_hat)%*%u_hat)\n\nNoten que R hace correción de muestras finitas:\n\nV=sigma2*solve(t(X)%*%P%*%X)*(N/(N-k))\nsqrt(diag(V))\n\n    constant         educ        exper      expersq        black        south \n0.7015640369 0.0405697227 0.0187547264 0.0004214487 0.0460663648 0.0262991839"
  },
  {
    "objectID": "diapositivas/variables-instrumentales-practica.html#modelo-sobreidentificado-3",
    "href": "diapositivas/variables-instrumentales-practica.html#modelo-sobreidentificado-3",
    "title": "Variables instrumentales en R",
    "section": "Modelo sobreidentificado",
    "text": "Modelo sobreidentificado\nComparamos:\n\n\nmodelsummary(list(\"Clásicos\"=iv_si),\n          output=\"gt\",\n          coef_map = c(\"educ\", \"exper\"),\n          gof_map = c(\"nobs\"),\n          fmt = 4)\n\n\n\n\n\n\n\n\n\nClásicos\n\n\n\n\neduc\n0.2403\n\n\n\n(0.0406)\n\n\nexper\n0.1517\n\n\n\n(0.0188)\n\n\nNum.Obs.\n3010"
  },
  {
    "objectID": "diapositivas/variables-instrumentales-practica.html#estimador-de-mgm-óptimo",
    "href": "diapositivas/variables-instrumentales-practica.html#estimador-de-mgm-óptimo",
    "title": "Variables instrumentales en R",
    "section": "Estimador de MGM óptimo",
    "text": "Estimador de MGM óptimo\nPara estimar por el MGM usaremos la librería gmm y la función del mismo nombre. La opción vcov indica que queremos una matriz robusta a heterocedasticidad y wmatrix especifica el estimador óptimo, es decir, donde \\(W=S^{-1}\\).\n\ngmm_opt &lt;- gmm(lwage ~ educ + exper + expersq + black + south,\n               ~ nearc4 + nearc2 + exper + expersq + black + south,\n               vcov = \"HAC\",\n               wmatrix = \"optimal\",\n               type = \"twoStep\",\n               data = data.ingresos)"
  },
  {
    "objectID": "diapositivas/variables-instrumentales-practica.html#estimador-de-mgm-óptimo-1",
    "href": "diapositivas/variables-instrumentales-practica.html#estimador-de-mgm-óptimo-1",
    "title": "Variables instrumentales en R",
    "section": "Estimador de MGM óptimo",
    "text": "Estimador de MGM óptimo\nRepliquemos esto con matrices. Obtenemos el vector de parámetros con alguna matriz subóptima, por ejemplo, la identidad:\n\nr &lt;- k -1 + 2 # 1 endógena y 2 instrumentos\nI &lt;- data.matrix(diag(r))\n\nb1 &lt;- solve(t(X)%*%Z%*%I%*%t(Z)%*%X)%*%t(X)%*%Z%*%I%*%t(Z)%*%Y\n\nUsemos este vector de parámetros para estimar \\(\\hat{S}\\):\n\nD &lt;- diag(as.vector((Y-X%*%b1)^2))\nS_hat &lt;- (1/N) * t(Z) %*% D %*% Z \n\nY volvamos a estimar el vector de parámetros, ahora usando \\(W=\\hat{S}^{-1}\\):\n\nbo &lt;- solve(t(X)%*%Z%*%solve(S_hat)%*%t(Z)%*%X)%*%\n  t(X)%*%Z%*%solve(S_hat)%*%t(Z)%*%Y\nbo\n\n                lwage\nconstant  2.022002957\neduc      0.238977697\nexper     0.150984069\nexpersq  -0.002402233\nblack    -0.021611174\nsouth    -0.081455256"
  },
  {
    "objectID": "diapositivas/variables-instrumentales-practica.html#estimador-de-mgm-óptimo-2",
    "href": "diapositivas/variables-instrumentales-practica.html#estimador-de-mgm-óptimo-2",
    "title": "Variables instrumentales en R",
    "section": "Estimador de MGM óptimo",
    "text": "Estimador de MGM óptimo\nCon este vector de parámetros, obtenemos la matriz de varianzas:\n\nD &lt;- diag(as.vector((Y-X%*%bo)^2))\nS_tilde &lt;- (1/N) * t(Z) %*% D %*% Z \n\nVr &lt;- (N)*solve(t(X) %*% Z %*% solve(S_tilde) %*% t(Z) %*% X)\nsqrt(diag(Vr))\n\n    constant         educ        exper      expersq        black        south \n0.6925828141 0.0401027981 0.0186490442 0.0004500697 0.0448916940 0.0258844828"
  },
  {
    "objectID": "diapositivas/variables-instrumentales-practica.html#estimador-de-mgm-óptimo-3",
    "href": "diapositivas/variables-instrumentales-practica.html#estimador-de-mgm-óptimo-3",
    "title": "Variables instrumentales en R",
    "section": "Estimador de MGM óptimo",
    "text": "Estimador de MGM óptimo\nComparamos:\n\n\nmodelsummary(list(gmm_opt),\n          output=\"gt\",\n          coef_map = c(\"educ\", \"exper\"),\n          gof_map = c(\"nobs\"),\n          fmt = 4)\n\n\n\n\n\n\n\n\n\n(1)\n\n\n\n\neduc\n0.2391\n\n\n\n(0.0446)\n\n\nexper\n0.1506\n\n\n\n(0.0205)\n\n\nNum.Obs.\n3010"
  },
  {
    "objectID": "diapositivas/variables-instrumentales-practica.html#iv-es-el-estimador-de-mgm-para-cualquier-w",
    "href": "diapositivas/variables-instrumentales-practica.html#iv-es-el-estimador-de-mgm-para-cualquier-w",
    "title": "Variables instrumentales en R",
    "section": "IV es el estimador de MGM para cualquier W",
    "text": "IV es el estimador de MGM para cualquier W\nUsemos gmm para estimar el modelo exactamente identificado, usando diferentes matrices \\(W\\):\n\n\ngmm_iv_opt &lt;- gmm(lwage ~ educ + exper + expersq + black + south,\n               ~ nearc4 + exper + expersq + black + south,\n               vcov = \"iid\",\n               wmatrix = \"optimal\",\n               type = \"twoStep\",\n               data = data.ingresos)\n\ngmm_iv_ident &lt;- gmm(lwage ~ educ + exper + expersq + black + south,\n                  ~ nearc4 + exper + expersq + black + south,\n                  vcov = \"iid\",\n                  wmatrix = \"ident\",\n                  type = \"twoStep\",\n                  data = data.ingresos)\n\n\nmodelsummary(list(\"Mátriz óptima\"=gmm_iv_opt,\"Identidad\"= gmm_iv_ident),\n          output=\"gt\",\n          coef_map = c(\"educ\", \"exper\"),\n          gof_map = c(\"nobs\"),\n          fmt = 4)\n\n\n\n\n\n\n\n\n\nMátriz óptima\nIdentidad\n\n\n\n\neduc\n0.2214\n0.2214\n\n\n\n(0.0409)\n(0.0409)\n\n\nexper\n0.1439\n0.1439\n\n\n\n(0.0187)\n(0.0187)\n\n\nNum.Obs.\n3010\n3010"
  },
  {
    "objectID": "diapositivas/variables-instrumentales-practica.html#iv-es-el-estimador-de-mgm-para-cualquier-w-1",
    "href": "diapositivas/variables-instrumentales-practica.html#iv-es-el-estimador-de-mgm-para-cualquier-w-1",
    "title": "Variables instrumentales en R",
    "section": "IV es el estimador de MGM para cualquier W",
    "text": "IV es el estimador de MGM para cualquier W\nRegresamos a la matriz \\(Z\\) con un solo instrumento y estimamos el vector de parámetros:\n\nZ &lt;- data.matrix(select(data.ingresos, constant, nearc4, exper, expersq, black,\n                        south),\n                 rownames.force = T)\n\nEstimamos el vector de coeficientes:\n\nb &lt;- solve(t(Z) %*% X) %*% t(Z) %*% Y\nb\n\n                lwage\nconstant  2.324805209\neduc      0.221390289\nexper     0.143933017\nexpersq  -0.002401759\nblack    -0.036938558\nsouth    -0.088888463"
  },
  {
    "objectID": "diapositivas/variables-instrumentales-practica.html#iv-es-el-estimador-de-mgm-para-cualquier-w-2",
    "href": "diapositivas/variables-instrumentales-practica.html#iv-es-el-estimador-de-mgm-para-cualquier-w-2",
    "title": "Variables instrumentales en R",
    "section": "IV es el estimador de MGM para cualquier W",
    "text": "IV es el estimador de MGM para cualquier W\nEl estimador de VI es el estimador de GMM para cualquier matriz \\(W\\) cuando \\(r=q\\):\n\n\nmodelsummary(list(\"Mátriz óptima\"=gmm_iv_opt,\"Identidad\"= gmm_iv_ident),\n          output=\"gt\",\n          coef_map = c(\"educ\", \"exper\"),\n          gof_map = c(\"nobs\"),\n          fmt = 4)\n\n\n\n\n\n\n\n\n\nMátriz óptima\nIdentidad\n\n\n\n\neduc\n0.2214\n0.2214\n\n\n\n(0.0409)\n(0.0409)\n\n\nexper\n0.1439\n0.1439\n\n\n\n(0.0187)\n(0.0187)\n\n\nNum.Obs.\n3010\n3010"
  },
  {
    "objectID": "diapositivas/variables-instrumentales-practica.html#prueba-de-hausman",
    "href": "diapositivas/variables-instrumentales-practica.html#prueba-de-hausman",
    "title": "Variables instrumentales en R",
    "section": "Prueba de Hausman",
    "text": "Prueba de Hausman\n\nEn general, las pruebas que comparan dos estimadores distintos se conocen como pruebas de Hausman, Wu-Hausman o Durbin-Wu-Hausman\nConsideremos dos estimadores \\(\\tilde{\\theta}\\) y \\(\\hat{\\theta}\\) que tienen la misma probabilidad límite bajo la \\(H_0\\) pero que difieren bajo la \\(H_a\\)\n\n\\[\n\\begin{aligned}\nH_0:\\quad\\quad p\\lim(\\tilde{\\theta}-\\hat{\\theta})=0 \\\\\nH_a:\\quad\\quad p\\lim(\\tilde{\\theta}-\\hat{\\theta})\\neq 0 \\\\\n\\end{aligned}\n\\]\n\nConstruimos el estadístico de prueba \\(H\\):\n\n\\[H=(\\tilde{\\theta}-\\hat{\\theta})'(\\hat{V}(\\tilde{\\theta}-\\hat{\\theta}))^{-1}(\\tilde{\\theta}-\\hat{\\theta})\\stackrel{a}{\\sim}\\chi^2(q)\\]\n\nSe rechaza la \\(H_0\\) si \\(H&gt;\\chi^2_{\\alpha}(q)\\)\nLa implementación es un poco complicada dado que\n\n\\[\\hat{V}(\\tilde{\\theta}-\\hat{\\theta})=\\hat{V}(\\tilde{\\theta})-\\hat{V}(\\hat{\\theta})-2cov(\\tilde{\\theta},\\hat{\\theta})\\]"
  },
  {
    "objectID": "diapositivas/variables-instrumentales-practica.html#prueba-de-hausman-1",
    "href": "diapositivas/variables-instrumentales-practica.html#prueba-de-hausman-1",
    "title": "Variables instrumentales en R",
    "section": "Prueba de Hausman",
    "text": "Prueba de Hausman\n\nCon errores homocedásticos, el estimador de MCO es eficiente\nEn ese caso, se puede mostrar que\n\n\\[H_{h}=(\\tilde{\\theta}-\\hat{\\theta})'(\\hat{V}(\\tilde{\\theta})-\\hat{V}(\\hat{\\theta}))^{-1}(\\tilde{\\theta}-\\hat{\\theta})\\stackrel{a}{\\sim}\\chi^2(q)\\] que es fácil de calcular en el software\n\nSi no estamos dispuestos a asumir homocedasticidad, se requiere estimar \\(cov(\\tilde{\\theta},\\hat{\\theta})\\), que se implementa en R y otros paquetes\nLa prueba de Hausman puede usarse para comparar dos estimadores, uno más eficiente que otro\nLa estimación de la prueba robusta puede complicarse en algunas aplicaciones, aunque como prueba de endogeneidad casi todo está disponible como funciones en R y otros paquetes"
  },
  {
    "objectID": "diapositivas/variables-instrumentales-practica.html#prueba-de-hausman-con-regresión-auxiliar",
    "href": "diapositivas/variables-instrumentales-practica.html#prueba-de-hausman-con-regresión-auxiliar",
    "title": "Variables instrumentales en R",
    "section": "Prueba de Hausman con regresión auxiliar",
    "text": "Prueba de Hausman con regresión auxiliar\n\nUna forma equivalente de realizar el test de Hausman es con una regresión auxiliar\nConsideremos el siguiente modelo:\n\n\\[y=x_1\\beta_1 + x_2\\beta_2  + u\\] con \\(x_1\\) endógna y \\(x_2\\) exógena\n\nLa regresión auxiliar es:\n\n\\[y=x_1\\gamma_21 + x_2\\gamma_2 +\\hat{v} \\gamma_3+ \\varepsilon\\] donde \\(\\hat{v}=x_1-\\hat{x}_1\\) y \\(\\hat{x}_1\\) son los valores ajustados de la primera etapa\n\nEl test de Hausman consiste en evaluar la significancia estadística de \\(\\gamma_3\\)"
  },
  {
    "objectID": "diapositivas/variables-instrumentales-practica.html#prueba-de-hausman-con-regresión-auxiliar-1",
    "href": "diapositivas/variables-instrumentales-practica.html#prueba-de-hausman-con-regresión-auxiliar-1",
    "title": "Variables instrumentales en R",
    "section": "Prueba de Hausman con regresión auxiliar",
    "text": "Prueba de Hausman con regresión auxiliar\n\nConsideremos la primera etapa\n\n\\[x_1 = z\\pi_1 + x_2\\pi + v\\] donde \\(z\\) es un instrumento válido\n\nSi \\(x_1\\) está correlacionado con \\(u\\) en la ecuación estructural entonces \\(\\nu\\) también lo está\nEs decir, \\(u=v\\gamma_3 + \\varepsilon\\)\nPlanteamos la hipótesis nula de que \\(\\gamma_3=0\\)\nSi rechazamos la hipótesis nula, concluimos que hay correlación entre \\(x_1\\) y \\(u\\)"
  },
  {
    "objectID": "diapositivas/variables-instrumentales-practica.html#prueba-de-sobreidentificación",
    "href": "diapositivas/variables-instrumentales-practica.html#prueba-de-sobreidentificación",
    "title": "Variables instrumentales en R",
    "section": "Prueba de sobreidentificación",
    "text": "Prueba de sobreidentificación\n\nTambién conocida como prueba de Hansen, quien propuso la forma general de la prueba, o prueba de Sargan, quien propuso la forma particular para el modelo lineal de VI\nEs una prueba sobre qué tan cerca está de cumplirse la hipótesis nula de que \\(E(h(w,\\theta_0))=0\\)\nHansen (1982) define el estadístico de prueba como\n\n\\[J=\\left(\\frac{1}{N}\\sum_i \\hat{h}_i\\right)'\\hat{S}^{-1}\\left(\\frac{1}{N}\\sum_i \\hat{h}_i\\right)\\stackrel{a}{\\sim}\\chi^2(r-q)\\]\n\nEl estadístico \\(J\\) es la función objetivo de MGM evaluada en \\(\\hat{\\theta}_{MGM}\\)\nSi el estadístico es grande en magnitud, rechazamos la hipótesis de que las condiciones de momentos poblacionales se cumplen y se concluye que el estimador de MGM es inconsistente"
  },
  {
    "objectID": "diapositivas/variables-instrumentales-practica.html#prueba-de-sobreidentificación-1",
    "href": "diapositivas/variables-instrumentales-practica.html#prueba-de-sobreidentificación-1",
    "title": "Variables instrumentales en R",
    "section": "Prueba de sobreidentificación",
    "text": "Prueba de sobreidentificación\n\nEn el caso de variables instrumentales, el estadístico tiene la forma específica:\n\n\\[J=\\hat{u}'Z\\hat{S}^{-1}Z'\\hat{u}\\] donde \\(\\hat{u}=y-X'\\hat{\\beta}_{MGM}\\)\n\nSi se rechaza \\(H_0\\), hay evidencia de que los instrumentos \\(z\\) son endógenos (aunque también podría ser que haya una mala especificación del modelo)\nRechazar la \\(H0\\) indica que debemos replantear el modelo, aunque no nos dice cómo"
  },
  {
    "objectID": "diapositivas/variables-instrumentales-practica.html#instrumentos-débiles-1",
    "href": "diapositivas/variables-instrumentales-practica.html#instrumentos-débiles-1",
    "title": "Variables instrumentales en R",
    "section": "Instrumentos débiles",
    "text": "Instrumentos débiles\n\nDiscusión intuitiva en Angrist & Pischke (MHE, 2009)\nEl estimador de MCO tiene las propiedades de ser consistente e insesgado\n\nEn una muestra de tamaño arbitrario, la distribución del coeficiente de MCO está centrada en el coeficiente de poblacional\n\nEn cambio, el estimador de MC2E, aunque consistente, es sesgado\n\nEn muestras grandes el estimador está cerca del coeficiente poblacional\n\nEsto tiene importantes consecuencias para la estimación y la inferencia"
  },
  {
    "objectID": "diapositivas/variables-instrumentales-practica.html#sesgo-del-estimador-de-mc2e",
    "href": "diapositivas/variables-instrumentales-practica.html#sesgo-del-estimador-de-mc2e",
    "title": "Variables instrumentales en R",
    "section": "Sesgo del estimador de MC2E",
    "text": "Sesgo del estimador de MC2E\n\nConsideremos el modelo simple con un solo regresor endógeno \\(y=\\beta x+ \\eta\\)\nSupongamos que tenemos una matriz de instrumentos \\(Z\\), por lo que la primera etapa es:\n\n\\[x=Z\\pi+\\xi\\]\n\nEl estimador de MCE es:\n\n\\[\\hat{\\beta}_{MC2E}=\\beta+(x'P_Z x)^{-1}x'P_Z\\eta\\]\n\nSustituyendo \\(x\\):\n\n\\[\\hat{\\beta}_{MC2E}-\\beta=(x'P_z x)^{-1}\\pi'Z'\\eta+(x'P_z x)^{-1}\\xi'P_z\\eta=sesgo_{Mc2E}\\]\n\nNo podemos calcular directamente el sesgo pues el operador esperanza es un operador lineal\nAngrist & Pischke (2009) aproximan el sesgo como.\n\n\\[E(\\hat{\\beta}_{MC2E}-\\beta)\\approx(E(x'P_z x))^{-1}E(\\pi'Z'\\eta)+(E(x'P_z x))^{-1}\\xi'P_z\\eta\\]"
  },
  {
    "objectID": "diapositivas/variables-instrumentales-practica.html#sesgo-del-estimador-de-mc2e-1",
    "href": "diapositivas/variables-instrumentales-practica.html#sesgo-del-estimador-de-mc2e-1",
    "title": "Variables instrumentales en R",
    "section": "Sesgo del estimador de MC2E",
    "text": "Sesgo del estimador de MC2E\n\nLa expresión del sesgo puede reescribirse como\n\n\\[E(\\hat{\\beta}_{MC2E}-\\beta)\\approx\\frac{\\sigma_{\\eta\\xi}}{\\sigma_{xi}^2}\\frac{1}{F+1}\\]\ndonde \\(\\frac{\\sigma_{\\eta \\xi}}{\\sigma_{xi}^2}\\) es el sesgo del estimador de MCO\n\nCuando \\(\\pi=0\\), el sesgo de MC2E es el mismo que el de MCO\nEs decir, cuando \\(F\\) es pequeña, el sesgo de MC2E se acerca al sesgo de MCO: el estimador de MC2E está sesgado hacia el de MCO cuando la primera etapa es débil\nStaiger & Stock (1997) mostraron con simulaciones que cuando \\(F&gt;10\\), el sesgo máximo en el estimador de MC2E es de 10%\nDe aquí viene la regla de dedo frecuentemente usada para juzgar instrumentos débiles"
  },
  {
    "objectID": "diapositivas/variables-instrumentales-practica.html#recomendaciones-prácticas",
    "href": "diapositivas/variables-instrumentales-practica.html#recomendaciones-prácticas",
    "title": "Variables instrumentales en R",
    "section": "Recomendaciones prácticas",
    "text": "Recomendaciones prácticas\n\nReportar la primera etapa y ver si los coeficientes tienen sentido económico\nReportar el estadístico \\(F\\) de la primera etapa para los instrumentos excluidos\nReportar los resultados usando un modelo exactamente identificado usando el mejor instrumento\nPoner atención a la forma reducida, recordando que la forma reducida es proporcional al efecto causal de interés\n\n\n“Si no puedes ver la relación causal de interés en la forma reducida es porque probablemente no haya nada ahí.”\n— Angrist & Krueger (2001)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Econometría Aplicada y Ciencia de Datos 2025",
    "section": "",
    "text": "Profesor: Irvin Rojas (irvin.rojas@cide.edu).\nHorario de clases: miércoles y jueves (14:30 a 16:00).\nPlataforma del curso: Microsoft Teams.\nHorario de oficina: martes y jueves (18:00 a 19:00).\n\n¡Bienvenidas, bienvenidos!\nEste es un sitio para las y los estudiantes del curso de Econometría Aplicada y Ciencia de Datos de la Maestría en Economía del CIDE. Sin embargo, otras personas pueden encontrar útiles los recursos de este sitio, como el programa del curso, las tareas y la lista de lecturas.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "materials.html",
    "href": "materials.html",
    "title": "Course Materials",
    "section": "",
    "text": "Datasets\nSlides"
  },
  {
    "objectID": "programa.html",
    "href": "programa.html",
    "title": "Programa",
    "section": "",
    "text": "Conocer la teoría sobre la que se fundamentan los métodos para la estimación de relaciones empíricas y la inferencia usando datos de sección cruzada y de panel.\nDiseñar estrategias econométricas usando los modelos adecuados de acuerdo con la pregunta de investigación.\nEmplear software para estimar los modelos econométricos apropiados de acuerdo con la naturaleza de los datos disponibles.\nConocer e implementar buenas prácticas en el uso de software, orientadas a la transparencia y la replicabilidad en la investigación.\nConocer los métodos que se emplean en la investigación económica actual.",
    "crumbs": [
      "Programa"
    ]
  },
  {
    "objectID": "programa.html#objetivos",
    "href": "programa.html#objetivos",
    "title": "Programa",
    "section": "",
    "text": "Conocer la teoría sobre la que se fundamentan los métodos para la estimación de relaciones empíricas y la inferencia usando datos de sección cruzada y de panel.\nDiseñar estrategias econométricas usando los modelos adecuados de acuerdo con la pregunta de investigación.\nEmplear software para estimar los modelos econométricos apropiados de acuerdo con la naturaleza de los datos disponibles.\nConocer e implementar buenas prácticas en el uso de software, orientadas a la transparencia y la replicabilidad en la investigación.\nConocer los métodos que se emplean en la investigación económica actual.",
    "crumbs": [
      "Programa"
    ]
  },
  {
    "objectID": "programa.html#referencias",
    "href": "programa.html#referencias",
    "title": "Programa",
    "section": "Referencias",
    "text": "Referencias\nEl curso se basa en los siguientes textos:\n\n(MHE) Angrist, J.D. y Pischke, J.S. (2013). Mostly Harmless Econometrics: An Empiricists Companion. Princeton University Press.\n* (CT) Cameron, A.C. y Trivedi, P.K. (2005). Microeconometrics: Methods and applications. Oxford University Press.\nHansen, B. (2022). Econometrics. Princeton University Press.\nHayashi, F. (2000). Econometrics. Princeton University Press.\n* (W) Wooldridge, J.M. (2010). Econometric analysis of cross section and panel data. Segunda edición, MIT Press.",
    "crumbs": [
      "Programa"
    ]
  },
  {
    "objectID": "programa.html#contenido-temático",
    "href": "programa.html#contenido-temático",
    "title": "Programa",
    "section": "Contenido temático",
    "text": "Contenido temático\nUnidad 1. Introducción\n\nRevisión de fundamentos de estadística y regresión lineal\nTeoría asintótica\nEstimadores extremos\nMáxima verosimilitud\nMínimos cuadrados no lineales\nInferencia estadística\n\nUnidad 2. Modelos de variable dependiente no continua\n\nModelos de variable dependiente binaria\n\nModelos de conteo\n\nUnidad 3. Modelos de selección\n\nModelo de Tobit\nModelo de Heckman\n\nUnidad 4. Endogeneidad\n\nVariables instrumentales\nMétodo generalizado de momentos\nEstimación con instrumentos débiles\n\nUnidad 5. Datos de panel\n\nModelos de efectos fijos y de efectos aleatorios\nEstimadores between y within\nEstimadores de primeras diferencias y de efectos aleatorios\nErrores estándar agrupados\n\nUnidad 6. Extensiones\n\nBootstrap\nRegresión por cuantiles\nMétodos semi paramétricos y no paramétricos\nModelos de panel no lineales\nModelos de panel con endogeneidad\nModelos de riesgo y de sobrevivencia",
    "crumbs": [
      "Programa"
    ]
  },
  {
    "objectID": "programa.html#evaluación-del-curso",
    "href": "programa.html#evaluación-del-curso",
    "title": "Programa",
    "section": "Evaluación del curso",
    "text": "Evaluación del curso\n\nExamen parcial: 30%.\nExamen final acumulativo: 45%\nTareas (4): 20% (5% cada una)\nExposición: 5%",
    "crumbs": [
      "Programa"
    ]
  },
  {
    "objectID": "programa.html#tareas",
    "href": "programa.html#tareas",
    "title": "Programa",
    "section": "Tareas",
    "text": "Tareas\nCuatro tareas teórico-prácticas. Las tareas deben entregarse de manera individual, pero se recomienda ampliamente colaborar en grupos de estudio. Las tareas deberán entregarse en Teams antes de la fecha y hora señalada. No se aceptarán tareas fuera de tiempo. Por favor, no comprima los archivos en carpetas comprimidas. Las tareas deberán contener dos archivos:\nUn primer documento de respuestas donde se incluyan las respuestas a las preguntas teóricas y conceptuales. Este documento debe estar en formato pdf y debe ser generado usando un software de procesamiento de textos científicos, por ejemplo, usando los leguajes LaTeX o Markdown. En este documento también se deben incluir las respuestas a preguntas sobre conclusiones que se desprenden de las secciones prácticas. Por ejemplo, si una pregunta pide obtener la media de la variable x en cierta base de datos, entonces el documento de respuestas debe incluir la pregunta y respuesta correspondiente: “la media de la variable x es 32.6”. En este documento también deberán incluirse las tablas y gráficas que se soliciten.\nUn segundo archivo deberá contener el código replicable usado para generar los resultados de la sección práctica. El código debe también crear las tablas y gráficas solicitadas. Los archivos de código se verificarán para comprobar su replicabilidad de manera aleatoria.",
    "crumbs": [
      "Programa"
    ]
  },
  {
    "objectID": "programa.html#software",
    "href": "programa.html#software",
    "title": "Programa",
    "section": "Software",
    "text": "Software\nR será el paquete standard usado en las sesiones prácticas. Más aún, el uso de cualquier software es aceptado siempre que se cumplan con los requisitos de replicabilidad y reportes de las tareas y exámenes. Habrá una sesión especial para introducir el uso de Quarto para la generación de reportes científicos.",
    "crumbs": [
      "Programa"
    ]
  },
  {
    "objectID": "programa.html#exámenes",
    "href": "programa.html#exámenes",
    "title": "Programa",
    "section": "Exámenes",
    "text": "Exámenes\n\nExamen parcial: miércoles 9 de octubre en el horario de clase.\nExamen final: por definir.",
    "crumbs": [
      "Programa"
    ]
  },
  {
    "objectID": "programa.html#exposición",
    "href": "programa.html#exposición",
    "title": "Programa",
    "section": "Exposición",
    "text": "Exposición\nCada alumno realizará una presentación de uno de los artículos aplicados marcados con negritas en la lista de lecturas. Cada presentación deberá ser de máximo 15 minutos y deberá incluir el contenido que el presentador considere más relevante. La presentación deberá abordar, mínimamente: 1) el problema a investigar, 2) la metodología empleada, 3) la relación entre la metodología y la teoría vista en el curso, 4) los datos empleados, 5) los principales resultados, y 6) una crítica sobre la validez y las conclusiones del estudio.",
    "crumbs": [
      "Programa"
    ]
  },
  {
    "objectID": "programa.html#lista-de-lecturas",
    "href": "programa.html#lista-de-lecturas",
    "title": "Programa",
    "section": "Lista de lecturas",
    "text": "Lista de lecturas\nTodas las lecturas de capítulos de libro son obligatorias pues permiten una discusión informada en la clase. Las lecturas marcadas con “*” no serán cubiertas en clase, pero son ampliamente recomendables. En las sesiones de exposiciones cada alumno presentará uno de los artículos enlistados con negritas, por lo que se espera que el resto de la clase tenga el conocimiento suficiente para participar en la discusión.\nLa lista de lecturas está disponible aquí.",
    "crumbs": [
      "Programa"
    ]
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Example schedule:\n\n\n\n\n\n\n\n\n\nMorning\nAfternoon\n\n\n\n\nL\nIntro + Data manipulation\ngit / GitHub\n\n\nM\nGeneralised Linear Models\nData visualisation\n\n\nX\nMixed models / GAM / Bayes\nFunctional programming + Students work\n\n\nJ\nMultivariate analyses\nReproducible workflows\n\n\nV\nUsing R as GIS + Students work\nProject presentations"
  },
  {
    "objectID": "tareas/tarea-1-respuestas.html",
    "href": "tareas/tarea-1-respuestas.html",
    "title": "Respuestas a la tarea 1",
    "section": "",
    "text": "Considere el problema de regresión no lineal en el que la variable dependiente escalar \\(y\\) tiene una media condicional \\(E(y_i)=g(x_i,\\beta)\\), siendo \\(g(\\cdot)\\) una función no lineal. Suponga que:\n\nEl proceso generador de datos es \\(y_i=g(x_i,\\beta_0)+u_i\\).\nEn el proceso generador de datos \\(E(u_i|x_i)=0\\) y \\(E(uu'|X)=\\Omega\\), donde \\(\\Omega_{0,ij}=\\sigma_{ij}\\).\nLa función \\(g(\\cdot)\\) satisface \\(g(x, \\beta^{(1)})=g(x, \\beta^{(2)})\\) si y solo si \\(\\beta^{(1)}=\\beta^{(2)}\\).\nLa matriz \\(A_0\\) existe y es finita y no singular y donde \\(A_0=p\\lim\\frac{1}{N}\\sum_{i=1}^{N}\\left.\\frac{\\partial g_i}{\\partial \\beta}\\right|_{\\beta_0}\\left.\\frac{\\partial g_i}{\\partial \\beta'}\\right|_{\\beta_0}=p\\lim\\frac{1}{N}\\left.\\frac{\\partial g'}{\\partial \\beta}\\frac{\\partial g'}{\\partial \\beta'}\\right|_{\\beta_0}\\)\n\\(N^{-1/2}\\sum_{i=1}^N \\left.\\frac{\\partial g_i}{\\partial \\beta}u_i \\right|_{\\beta_0}\\xrightarrow{d}\\mathcal{N}(0,B_0)\\), donde \\(B_0=p\\lim \\frac{1}{N}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\sigma_{ij}\\left.\\frac{\\partial g_i}{\\partial \\beta}\\frac{\\partial g_i}{\\partial \\beta'}\\right|_{\\beta_0}=p\\lim\\frac{1}{N}\\left.\\frac{\\partial g'}{\\partial \\beta}\\Omega_0 \\frac{\\partial g}{\\partial \\beta'}\\right|_{\\beta_0}\\).\nRespuestas: ver proposición 5.6 en CT.\n\n\n[5 puntos] Plantee el problema de optimización para la minimización de la suma de los errores cuadráticos y obtenga las condiciones de primer orden.\n[10 puntos] Pruebe que \\(\\hat{\\beta}_{MCNL}\\), el estimador de mínimos cuadrados no lineales (MCNL) y definido como una raíz de las condiciones de primer orden, es consistente para \\(\\beta_0\\).\n[10 puntos] Derive una expresión para \\(\\sqrt{N}(\\hat{\\beta}_{MCNL}-\\beta_0)\\) y pruebe que \\(\\sqrt{N}(\\hat{\\beta}_{MCNL}-\\beta_0)\\xrightarrow{d}\\mathcal{N}(0,A_0^{-1}B_0A_0^{-1})\\). Tip: utilice una expansión de Taylor exacta de primer orden.\n[5 puntos] ¿Cómo estimaría \\(V(\\hat{\\beta}_{MCNL})\\)?}\n\n\n\n\nSuponga que está interesado en una variable aleatoria que tiene una distribución Bernoulli con parámetro \\(p\\). La función de densidad está definida como:\n\\[f(x_;p)=\\left\\{\\begin{array} .1 & \\text{con probabilidad } p \\\\ 0 & \\text{con probabilidad } 1-p \\end{array} \\right.\\] Suponga que tiene una muestra de \\(N\\) observaciones independientes e idénticamente distribuidas.\n\n[4 puntos] Plantee la función de log verosimilitud del problema.\nPodemos escribir la función de densidad para la \\(i\\)-ésima observación como\n\\[f(x_i;p)=p^{x_i}(1-p)^{(1-x_i)}\\]\nPor tanto, la función de verosimilitud es\n\\[L_N(p)=\\prod_{i=1}^N f(x;p)=\\prod_{i=1}^N p^{x_i}(1-p)^{(1-x_i)} = p^{\\sum_{i=1}^N x_i}(1-p)^{N-\\sum_{i=1}^N x_i}\\]\nY la función de log verosimilitud será\n\\[\\mathcal{L_N(p)}=\\ln{L_N(p)}=\\sum x_i \\ln(p)-(N-\\sum x_i)\\ln(1-p)\\]\n[4 puntos] Obtenga las condiciones de primer orden y resuelva para \\(\\hat{p}\\).\nDerivando \\(\\mathcal{L}_N\\) con respecto a \\(p\\) obtenemos la condición de primer orden:\n\\[\\frac{d\\mathcal{L}_N(p)}{d p}=\\frac{\\sum x_i}{p}-\\frac{N-\\sum x_i}{1-p}=0\\]\nY resolviendo, obtenemos el estimador de máxima verosimilitud \\[\\hat{p}_{MV}=\\bar{x}\\] es decir, la media muestral.\n[2 puntos] ¿Cuál es la media y la varianza del estimador de máxima verosimilitud que ha encontrado?\nObtenemos directamente la media \\[E(\\hat{p}_{MV})=E(\\bar{x})=\\frac{1}{N}E\\left(\\sum x_i\\right)=\\frac{1}{N}N p=p\\]\nMientras que la varianza es \\[V(\\hat{p}_{MV})=\\frac{1}{N^2}V\\left(\\sum x_i\\right)=\\frac{p(1-p)}{N}\\]\n\n\n\n\nConsidere el modelo logit:\n\\[f(y_i|x_i;\\theta_0)=\\left\\{ \\begin{array} .1 & \\frac{\\exp\\{x_i'\\theta_0\\}}{1+\\exp\\{x_i'\\theta_0\\}}  \\\\ 0 &  \\frac{1}{1+\\exp\\{x_i'\\theta_0\\}} \\end{array} \\right.\\] donde \\(x_i\\) es un vector de variables explicativas, \\(\\theta_0\\) y es el vector de parámetros poblacional. Asuma que dispone de observaciones \\((y_i,x_i)\\) que son iid.\n\n[5 puntos] Escriba la función de log verosimilitud condicional para la observación \\(i\\).\nEs conveniente escribir el problema en término de \\(\\Lambda (x_i'\\theta_0) \\equiv \\frac{\\exp\\{x_i'\\theta_0\\}}{1+\\exp\\{x_i'\\theta_0\\}}\\). Así, la función de verosimilitud para la observación \\(i\\) es:\n\\[f(y_i|x_i;\\theta_0)=\\Lambda (x_i'\\theta_0)^{y_i}(1-\\Lambda (x_i'\\theta_0))^{(1-y_i)}\\]\nTomando logs, la función de log verosimilitud es:\n\\[\\mathcal{l}(y_i|x_i,\\theta)=\\ln f(y_i|x_i;\\theta)=y_i\\ln \\Lambda (x_i'\\theta)+(1-y_i)(1-\\Lambda (x_i'\\theta))\\]\n[5 puntos] Encuentre el vector score para la observación \\(i\\).\nEl vector score es el vector de primeras derivadas parciales de la log verosimilitud. Un pequeño truco facilita las cosas. Se puede mostrar que \\(\\Lambda (\\cdot)'=\\Lambda (\\cdot)(1-\\Lambda (\\cdot))\\). Entonces:\n\\[\\frac{\\partial \\mathcal{l}_i}{\\partial\\theta}=y_i \\frac{1}{\\Lambda (x_i'\\theta)}\\Lambda (x_i'\\theta)(1-\\Lambda (x_i'\\theta))x_i+(1-y_i)\\frac{1}{1-\\Lambda (x_i'\\theta)}\\Lambda (x_i'\\theta)(1-\\Lambda (x_i'\\theta))x_i\\]\nSimplificando:\n\\[\\frac{\\partial \\mathcal{l}_i}{\\partial\\theta}=(y_i-\\Lambda (x_i'\\theta))x_i \\equiv s(y_i,x_i;\\theta)\\]\n[5 puntos] Encuentre la hesiana de la función de log verosimilitud con respecto a \\(\\mathbf{\\theta}\\).\nProcedemos a derivar el score con respecto a \\(\\theta'\\):\n\\[H(y_i,x_i;\\theta)\\equiv \\frac{\\partial s(y_i,x_i;\\theta)}{\\partial \\theta'}= -\\Lambda(x_i'\\theta)(1-\\Lambda(x_i'\\theta))x_ix_i'\\]\n[5 puntos] Obtenga la matriz de información para la observación \\(i\\).\nLa matriz de información es \\(E(s(y_i,x_i;\\theta_0)s(y_i,x_i;\\theta_0)'|x_i)\\)\n\\[I(\\theta_0)=E((y_i-\\Lambda(x_i'\\theta_0))^2x_ix_i')\\]\n\n\n\n\nSuponga una variable aleatoria \\(X_i\\) con distribución desconocida. Sin embargo, sí conocemos que \\(E(X)=\\mu=54\\) y que \\(\\sqrt{V(X)}=\\sigma=6\\). Suponga que se recolecta una muestra de 50 observaciones.\n\n[2 punto] ¿Cuál es la distribución asintótica de la media muestral \\(\\bar{X}\\)?\nSi se puede aplicar un teorema de límite central a la media muestral, sabemos que la nueva variable hereda la media de \\(X_i\\) y la desviación estándar es la desviación estándar de \\(X_i\\) dividida por la raíz del tamaño de la muestra. Es decir:\n\\[\\bar{X}\\sim \\mathcal{N}(54, 6^2/50)\\]\n[4 punto] ¿Cuál es la probabilidad de que \\(\\bar{X}&gt;58\\)?\nSabemos que \\(\\frac{\\bar{X}-54}{6/\\sqrt{50}}\\sim\\mathcal{N}(0,1)\\), por tanto:\n\\[P(\\bar{X}&gt;58)=P\\left(z&gt;\\frac{58-54}{6/\\sqrt{50}}\\right)=P(z&gt;4.714045)=1-\\Phi(4.714045)\\]\nCalculamos la probabilidad usando pnorm, que nos da la función de distribución. La probabilidad es un número muy pequeño:\n\n1-pnorm((58-54)/(6/sqrt(50)), mean = 0, sd = 1)\n\n[1] 1.214234e-06\n\n\n[2 punto] ¿Cuál es la probabilidad de que una observación elegida al azar sea tal que \\(X_i&gt;58\\)?\nEs imposible de determinar porque no sabemos la distribución de \\(X_i\\). Esto es algo muy conveniente de los TLC, pues nos permiten hacer afirmaciones sobre la media muestral sin saber la distribución de la que provienen las observaciones. Solo necesitamos que se cumplan las condiciones sobre las \\(X_i\\) para aplicar los TLC.\n[2 punto] Provea un intervalo de confianza de 99% para la media muestral.\nPor un lado, sabemos que la variable aleatoria \\(Z=\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{N}}\\) tendrá una distribución \\(\\mathcal{N}(0,1)\\). Por otro lado, queremos obtener \\(P(-z_{\\alpha/2}&lt;Z&lt;z_{\\alpha/2})=0.99\\). Manipulando, obtenemos una expresión para el intervalo de confianza:\n\\[\\left(\\bar{X}-z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{N}},\\bar{X}+z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{N}}\\right)\\]\nEn nuestro caso, el intervalo es:\n\\[P\\left(\\bar{X}\\pm 2.5758\\times(6/\\sqrt{50})\\right)=0.99\\]\ndonde obtenemos el 2.5758 como:\n\nqnorm(0.995)\n\n[1] 2.575829\n\n\nqnorm es la función cuantil y está definida como la función inversa de la función de distribución. La función cuantil da el valor de \\(x\\) tal que \\(F(x)=P(X \\leq x)=p\\).\nEntonces, el intervalo de confianza es: \\[P(\\bar{X}\\pm 2.185664)=0.99\\]\n\n\n\n\nEn esta pregunta mostraremos los alcances de los teoremas del límite central. Para esto, generaremos muchas muestras de tamaño \\(N\\) con una distribución \\(Bernoulli\\) con probabilidad de éxito \\(p=0.7\\). Recuerde que cuando realice simulaciones, siempre debe fijar una semilla al inicio para poder replicar su trabajo.\n\n[2 puntos] ¿Cuál es la media y la varianza de una variable aleatoria \\(y_i \\sim Bernoulli(0.3)\\)?\nPara una variable que se distribuye \\(Bernoulli(p)\\), la media es \\(p\\) y la varianza es \\(p(1-p)\\). Para este caso, \\(E(y_i)=0.7\\) y \\(V(y_i)=0.7*0.3=0.21\\).\n[2 puntos] Si \\(y_i\\) son iid y podemos aplicar un teorema de límite central, ¿cuál es la distribución teórica de \\(\\bar{y}\\) cuando \\(N\\to\\infty\\)?\nObtenemos el valor esperado y la varianza de \\(\\bar{y}\\):\n\\[E(\\bar{y})=\\frac{1}{N}E(\\sum_i y_i)  = E(y_i)=p\\]\n\\[V(\\bar{y})=\\frac{1}{N^2}V(\\sum_i y_i) = \\frac{1}{N}V(y_i)=\\frac{p(1-p)}{N}\\]\nEntonces, un TLC nos daría las condiciones para que:\n\\[\\frac{\\bar{y}-0.7}{\\sqrt{0.21/N}}\\sim\\mathcal{N}(0, 1)\\]\n[5 puntos] Realice el siguiente procedimiento \\(J=1,000\\) veces. Obtenga una muestra de tamaño \\(N=3\\) a partir de la distribución \\(Bernoulli(0.7)\\) y calcule la media muestral \\(\\bar{y}\\). Coleccione las \\(J\\) medias muestrales y luego grafique un histograma de las medias muestrales obtenidas junto con una curva teórica normal con la media y varianza obtenida en la parte b. Comente sobre lo que observa.\n\nset.seed(921)\nreps &lt;- 1000\nn &lt;- 3\np &lt;- 0.7\nv &lt;- p*(1-p)/n\n\nymedias3 &lt;- numeric(reps)\nfor (i in 1:reps){\n sample &lt;- rbinom(n, 1, p)\n ymedias3[i]&lt;-mean(sample)\n}\n\nGraficamos junto con una densidad \\(N(0.7, \\sqrt{0.21/3})\\):\n\nhist(ymedias3, breaks=20, prob=TRUE, \n     xlab=\"Medias\")\ncurve(dnorm(x, mean=p, sd=sqrt(v)), \n      col=\"darkblue\", lwd=2, add=TRUE, yaxt=\"n\")\n\n\n\n\n\n\n\n\nEl histograma no se parece nada a la curva normal.\n[3 puntos] Repita lo realizado en la parte b., ahora con \\(N=15\\). Comente sobre lo que observa.\n\nreps &lt;- 1000\nn &lt;- 15\np &lt;- 0.7\nv &lt;- p*(1-p)/n\n\nymedias15 &lt;- numeric(reps)\nfor (i in 1:reps){\n sample &lt;- rbinom(n, 1, p)\n ymedias15[i]&lt;-mean(sample)\n}\n\nGraficamos junto con una densidad \\(N(0.7, \\sqrtt{0.21/15})\\):\n\nhist(ymedias15, breaks=20, prob=TRUE)\ncurve(dnorm(x, mean=p, sd=sqrt(v)), \n      col=\"darkblue\", lwd=2, add=TRUE, yaxt=\"n\")\n\n\n\n\n\n\n\n\nEl histograma comienza a tener una forma normal.\n[3 puntos] Repita lo realizado en la parte b., ahora con \\(N=1,500\\). Comente sobre lo que observa.\n\n#|echo: true\n\nreps &lt;- 1000\nn &lt;- 1500\np &lt;- 0.7\nv &lt;- p*(1-p)/n\n\nymedias1500 &lt;- numeric(reps)\nfor (i in 1:reps){\n sample &lt;- rbinom(n, 1, p)\n ymedias1500[i]&lt;-mean(sample)\n}\n\nGraficamos junto con una densidad \\(N(0.7, \\sqrt{0.21/1500})\\):\n\nhist(ymedias1500, breaks=20, prob=TRUE, \n     xlab=\"Medias\")\ncurve(dnorm(x, mean=p, sd=sqrt(v)), \n      col=\"darkblue\", lwd=2, add=TRUE, yaxt=\"n\")\n\n\n\n\n\n\n\n\nEl histograma se parece ya a la curva normal. Podrían repetir este ejercicio con un tamaño de muestra más grande incluso y ver qué sucede.\n[5 puntos] ¿Cómo usaría este ejercicio con palabras simples para explicar a una persona que no sabe mucho de estadística sobre la importancia de los teoremas de límite central?\nUn TLC nos permite hacer afirmaciones sobre la distribución de un estadístico. Un estadístico es un resumen de los datos, por lo que nos interesa usar dichos estadísticos para describir características de los fenómenos que estudiamos usando datos. Queremos saber cosas como lo que esperamos en promedio que suceda con una variable, o qué tanta variabilidad dicha variable tendrá en la población. Con un TLC podemos hacer afirmaciones sobre cómo lucen promedios muestrales de la variable que estudiamos cuando tenemos suficientes observaciones. Nos dice en particular que va a tener una distribución normal.\n\n\n\n\nSea \\(x_1\\) un vector de variables continuas, \\(x_2\\) una variable continua y \\(d_1\\) una variable dicotómica. Considere el siguiente modelo probit: \\[P(y=1│x_1,x_2 )=\\Phi(x_1'\\alpha+\\beta x_2+\\gamma x_2^2 )\\]\n\n[5 punto] Provea una expresión para el efecto marginal de \\(x_2\\) en la probabilidad. ¿Cómo estimaría este efecto marginal?\nEl efecto marginal de interés es:\n\\[\\frac{\\partial P(y=1|x_1,x_2)}{\\partial x_2}=\\phi(x_1\\alpha+\\beta x_2+\\gamma x_2^2)(\\beta+2\\gamma x_2)\\]\nPara estimarlo, usamos máxima verosimilitud para obtener estimadores consistentes de \\(\\alpha\\), \\(\\beta\\) y \\(\\gamma\\) y empleamos software para obener los valores de las probabilidad usando las características individuales \\(x_1\\) y \\(x_2\\). Luego podemos obtener el efecto marginal promedio como el promedio de los efectos marginales individuales. Alternativamente, se podría calcular el efecto marginal en valores específicos de \\(x_1\\) y \\(x_2\\), como \\(\\bar{x}_1\\) y \\(\\bar{x}_2\\).\n[3 punto] Considere ahora el modelo: \\[P(y=1│x_1  ,x_2 ,d_1)=\\Phi(x_1 '\\delta+\\pi x_2+\\rho d_1+\\nu x_2 d_1 )\\] Provea la nueva expresión para el efecto marginal de \\(x_2\\).\nEl efecto marginal es: \\[\\frac{\\partial P(y=1|x_1,x_2)}{\\partial x_2}=\\phi(x_1\\delta+\\pi x_2+\\rho d_1+  \\nu x_2d_1)(\\pi+\\nu d_1)\\]\n[2 punto] En el modelo de la parte b., ¿cómo evaluaría el efecto de un cambio en \\(d_1\\) en la probabilidad? Provea una expresión para este efecto.\nDado que \\(d_1\\) es una variable dicotómica, el efecto de \\(d_1\\) se mide como la diferencia en probabilidad cuando \\(d_1=1\\) y cuando \\(d_1=0\\):\n\\[P(y=1|x_1,x_2,d_1=1)-P(y=1|x_1,x_2,d_1=0)=\\phi(x_1\\delta+(\\pi+\\nu)x_2+\\rho)-\\phi(x_1\\delta+\\pi x_2)\\]\nPodemos estimar este cambio en probabilidad para cada individuo de la muestra y luego obtener un promedio de los cambios en probabilidad. Por ejemplo, supongamos que estudiamos la probabilidad de estar empleado y que \\(d_1\\) es la pertenencia o no a un sindicato. Para cada individuo en la muestra obtemeos el cambio en probabilidad que resulta de que \\(d_1\\) pase de 1 a 0."
  },
  {
    "objectID": "tareas/tarea-1-respuestas.html#pregunta-1",
    "href": "tareas/tarea-1-respuestas.html#pregunta-1",
    "title": "Respuestas a la tarea 1",
    "section": "",
    "text": "Considere el problema de regresión no lineal en el que la variable dependiente escalar \\(y\\) tiene una media condicional \\(E(y_i)=g(x_i,\\beta)\\), siendo \\(g(\\cdot)\\) una función no lineal. Suponga que:\n\nEl proceso generador de datos es \\(y_i=g(x_i,\\beta_0)+u_i\\).\nEn el proceso generador de datos \\(E(u_i|x_i)=0\\) y \\(E(uu'|X)=\\Omega\\), donde \\(\\Omega_{0,ij}=\\sigma_{ij}\\).\nLa función \\(g(\\cdot)\\) satisface \\(g(x, \\beta^{(1)})=g(x, \\beta^{(2)})\\) si y solo si \\(\\beta^{(1)}=\\beta^{(2)}\\).\nLa matriz \\(A_0\\) existe y es finita y no singular y donde \\(A_0=p\\lim\\frac{1}{N}\\sum_{i=1}^{N}\\left.\\frac{\\partial g_i}{\\partial \\beta}\\right|_{\\beta_0}\\left.\\frac{\\partial g_i}{\\partial \\beta'}\\right|_{\\beta_0}=p\\lim\\frac{1}{N}\\left.\\frac{\\partial g'}{\\partial \\beta}\\frac{\\partial g'}{\\partial \\beta'}\\right|_{\\beta_0}\\)\n\\(N^{-1/2}\\sum_{i=1}^N \\left.\\frac{\\partial g_i}{\\partial \\beta}u_i \\right|_{\\beta_0}\\xrightarrow{d}\\mathcal{N}(0,B_0)\\), donde \\(B_0=p\\lim \\frac{1}{N}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\sigma_{ij}\\left.\\frac{\\partial g_i}{\\partial \\beta}\\frac{\\partial g_i}{\\partial \\beta'}\\right|_{\\beta_0}=p\\lim\\frac{1}{N}\\left.\\frac{\\partial g'}{\\partial \\beta}\\Omega_0 \\frac{\\partial g}{\\partial \\beta'}\\right|_{\\beta_0}\\).\nRespuestas: ver proposición 5.6 en CT.\n\n\n[5 puntos] Plantee el problema de optimización para la minimización de la suma de los errores cuadráticos y obtenga las condiciones de primer orden.\n[10 puntos] Pruebe que \\(\\hat{\\beta}_{MCNL}\\), el estimador de mínimos cuadrados no lineales (MCNL) y definido como una raíz de las condiciones de primer orden, es consistente para \\(\\beta_0\\).\n[10 puntos] Derive una expresión para \\(\\sqrt{N}(\\hat{\\beta}_{MCNL}-\\beta_0)\\) y pruebe que \\(\\sqrt{N}(\\hat{\\beta}_{MCNL}-\\beta_0)\\xrightarrow{d}\\mathcal{N}(0,A_0^{-1}B_0A_0^{-1})\\). Tip: utilice una expansión de Taylor exacta de primer orden.\n[5 puntos] ¿Cómo estimaría \\(V(\\hat{\\beta}_{MCNL})\\)?}"
  },
  {
    "objectID": "tareas/tarea-1-respuestas.html#pregunta-2",
    "href": "tareas/tarea-1-respuestas.html#pregunta-2",
    "title": "Respuestas a la tarea 1",
    "section": "",
    "text": "Suponga que está interesado en una variable aleatoria que tiene una distribución Bernoulli con parámetro \\(p\\). La función de densidad está definida como:\n\\[f(x_;p)=\\left\\{\\begin{array} .1 & \\text{con probabilidad } p \\\\ 0 & \\text{con probabilidad } 1-p \\end{array} \\right.\\] Suponga que tiene una muestra de \\(N\\) observaciones independientes e idénticamente distribuidas.\n\n[4 puntos] Plantee la función de log verosimilitud del problema.\nPodemos escribir la función de densidad para la \\(i\\)-ésima observación como\n\\[f(x_i;p)=p^{x_i}(1-p)^{(1-x_i)}\\]\nPor tanto, la función de verosimilitud es\n\\[L_N(p)=\\prod_{i=1}^N f(x;p)=\\prod_{i=1}^N p^{x_i}(1-p)^{(1-x_i)} = p^{\\sum_{i=1}^N x_i}(1-p)^{N-\\sum_{i=1}^N x_i}\\]\nY la función de log verosimilitud será\n\\[\\mathcal{L_N(p)}=\\ln{L_N(p)}=\\sum x_i \\ln(p)-(N-\\sum x_i)\\ln(1-p)\\]\n[4 puntos] Obtenga las condiciones de primer orden y resuelva para \\(\\hat{p}\\).\nDerivando \\(\\mathcal{L}_N\\) con respecto a \\(p\\) obtenemos la condición de primer orden:\n\\[\\frac{d\\mathcal{L}_N(p)}{d p}=\\frac{\\sum x_i}{p}-\\frac{N-\\sum x_i}{1-p}=0\\]\nY resolviendo, obtenemos el estimador de máxima verosimilitud \\[\\hat{p}_{MV}=\\bar{x}\\] es decir, la media muestral.\n[2 puntos] ¿Cuál es la media y la varianza del estimador de máxima verosimilitud que ha encontrado?\nObtenemos directamente la media \\[E(\\hat{p}_{MV})=E(\\bar{x})=\\frac{1}{N}E\\left(\\sum x_i\\right)=\\frac{1}{N}N p=p\\]\nMientras que la varianza es \\[V(\\hat{p}_{MV})=\\frac{1}{N^2}V\\left(\\sum x_i\\right)=\\frac{p(1-p)}{N}\\]"
  },
  {
    "objectID": "tareas/tarea-1-respuestas.html#pregunta-3",
    "href": "tareas/tarea-1-respuestas.html#pregunta-3",
    "title": "Respuestas a la tarea 1",
    "section": "",
    "text": "Considere el modelo logit:\n\\[f(y_i|x_i;\\theta_0)=\\left\\{ \\begin{array} .1 & \\frac{\\exp\\{x_i'\\theta_0\\}}{1+\\exp\\{x_i'\\theta_0\\}}  \\\\ 0 &  \\frac{1}{1+\\exp\\{x_i'\\theta_0\\}} \\end{array} \\right.\\] donde \\(x_i\\) es un vector de variables explicativas, \\(\\theta_0\\) y es el vector de parámetros poblacional. Asuma que dispone de observaciones \\((y_i,x_i)\\) que son iid.\n\n[5 puntos] Escriba la función de log verosimilitud condicional para la observación \\(i\\).\nEs conveniente escribir el problema en término de \\(\\Lambda (x_i'\\theta_0) \\equiv \\frac{\\exp\\{x_i'\\theta_0\\}}{1+\\exp\\{x_i'\\theta_0\\}}\\). Así, la función de verosimilitud para la observación \\(i\\) es:\n\\[f(y_i|x_i;\\theta_0)=\\Lambda (x_i'\\theta_0)^{y_i}(1-\\Lambda (x_i'\\theta_0))^{(1-y_i)}\\]\nTomando logs, la función de log verosimilitud es:\n\\[\\mathcal{l}(y_i|x_i,\\theta)=\\ln f(y_i|x_i;\\theta)=y_i\\ln \\Lambda (x_i'\\theta)+(1-y_i)(1-\\Lambda (x_i'\\theta))\\]\n[5 puntos] Encuentre el vector score para la observación \\(i\\).\nEl vector score es el vector de primeras derivadas parciales de la log verosimilitud. Un pequeño truco facilita las cosas. Se puede mostrar que \\(\\Lambda (\\cdot)'=\\Lambda (\\cdot)(1-\\Lambda (\\cdot))\\). Entonces:\n\\[\\frac{\\partial \\mathcal{l}_i}{\\partial\\theta}=y_i \\frac{1}{\\Lambda (x_i'\\theta)}\\Lambda (x_i'\\theta)(1-\\Lambda (x_i'\\theta))x_i+(1-y_i)\\frac{1}{1-\\Lambda (x_i'\\theta)}\\Lambda (x_i'\\theta)(1-\\Lambda (x_i'\\theta))x_i\\]\nSimplificando:\n\\[\\frac{\\partial \\mathcal{l}_i}{\\partial\\theta}=(y_i-\\Lambda (x_i'\\theta))x_i \\equiv s(y_i,x_i;\\theta)\\]\n[5 puntos] Encuentre la hesiana de la función de log verosimilitud con respecto a \\(\\mathbf{\\theta}\\).\nProcedemos a derivar el score con respecto a \\(\\theta'\\):\n\\[H(y_i,x_i;\\theta)\\equiv \\frac{\\partial s(y_i,x_i;\\theta)}{\\partial \\theta'}= -\\Lambda(x_i'\\theta)(1-\\Lambda(x_i'\\theta))x_ix_i'\\]\n[5 puntos] Obtenga la matriz de información para la observación \\(i\\).\nLa matriz de información es \\(E(s(y_i,x_i;\\theta_0)s(y_i,x_i;\\theta_0)'|x_i)\\)\n\\[I(\\theta_0)=E((y_i-\\Lambda(x_i'\\theta_0))^2x_ix_i')\\]"
  },
  {
    "objectID": "tareas/tarea-1-respuestas.html#pregunta-4",
    "href": "tareas/tarea-1-respuestas.html#pregunta-4",
    "title": "Respuestas a la tarea 1",
    "section": "",
    "text": "Suponga una variable aleatoria \\(X_i\\) con distribución desconocida. Sin embargo, sí conocemos que \\(E(X)=\\mu=54\\) y que \\(\\sqrt{V(X)}=\\sigma=6\\). Suponga que se recolecta una muestra de 50 observaciones.\n\n[2 punto] ¿Cuál es la distribución asintótica de la media muestral \\(\\bar{X}\\)?\nSi se puede aplicar un teorema de límite central a la media muestral, sabemos que la nueva variable hereda la media de \\(X_i\\) y la desviación estándar es la desviación estándar de \\(X_i\\) dividida por la raíz del tamaño de la muestra. Es decir:\n\\[\\bar{X}\\sim \\mathcal{N}(54, 6^2/50)\\]\n[4 punto] ¿Cuál es la probabilidad de que \\(\\bar{X}&gt;58\\)?\nSabemos que \\(\\frac{\\bar{X}-54}{6/\\sqrt{50}}\\sim\\mathcal{N}(0,1)\\), por tanto:\n\\[P(\\bar{X}&gt;58)=P\\left(z&gt;\\frac{58-54}{6/\\sqrt{50}}\\right)=P(z&gt;4.714045)=1-\\Phi(4.714045)\\]\nCalculamos la probabilidad usando pnorm, que nos da la función de distribución. La probabilidad es un número muy pequeño:\n\n1-pnorm((58-54)/(6/sqrt(50)), mean = 0, sd = 1)\n\n[1] 1.214234e-06\n\n\n[2 punto] ¿Cuál es la probabilidad de que una observación elegida al azar sea tal que \\(X_i&gt;58\\)?\nEs imposible de determinar porque no sabemos la distribución de \\(X_i\\). Esto es algo muy conveniente de los TLC, pues nos permiten hacer afirmaciones sobre la media muestral sin saber la distribución de la que provienen las observaciones. Solo necesitamos que se cumplan las condiciones sobre las \\(X_i\\) para aplicar los TLC.\n[2 punto] Provea un intervalo de confianza de 99% para la media muestral.\nPor un lado, sabemos que la variable aleatoria \\(Z=\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{N}}\\) tendrá una distribución \\(\\mathcal{N}(0,1)\\). Por otro lado, queremos obtener \\(P(-z_{\\alpha/2}&lt;Z&lt;z_{\\alpha/2})=0.99\\). Manipulando, obtenemos una expresión para el intervalo de confianza:\n\\[\\left(\\bar{X}-z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{N}},\\bar{X}+z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{N}}\\right)\\]\nEn nuestro caso, el intervalo es:\n\\[P\\left(\\bar{X}\\pm 2.5758\\times(6/\\sqrt{50})\\right)=0.99\\]\ndonde obtenemos el 2.5758 como:\n\nqnorm(0.995)\n\n[1] 2.575829\n\n\nqnorm es la función cuantil y está definida como la función inversa de la función de distribución. La función cuantil da el valor de \\(x\\) tal que \\(F(x)=P(X \\leq x)=p\\).\nEntonces, el intervalo de confianza es: \\[P(\\bar{X}\\pm 2.185664)=0.99\\]"
  },
  {
    "objectID": "tareas/tarea-1-respuestas.html#pregunta-5",
    "href": "tareas/tarea-1-respuestas.html#pregunta-5",
    "title": "Respuestas a la tarea 1",
    "section": "",
    "text": "En esta pregunta mostraremos los alcances de los teoremas del límite central. Para esto, generaremos muchas muestras de tamaño \\(N\\) con una distribución \\(Bernoulli\\) con probabilidad de éxito \\(p=0.7\\). Recuerde que cuando realice simulaciones, siempre debe fijar una semilla al inicio para poder replicar su trabajo.\n\n[2 puntos] ¿Cuál es la media y la varianza de una variable aleatoria \\(y_i \\sim Bernoulli(0.3)\\)?\nPara una variable que se distribuye \\(Bernoulli(p)\\), la media es \\(p\\) y la varianza es \\(p(1-p)\\). Para este caso, \\(E(y_i)=0.7\\) y \\(V(y_i)=0.7*0.3=0.21\\).\n[2 puntos] Si \\(y_i\\) son iid y podemos aplicar un teorema de límite central, ¿cuál es la distribución teórica de \\(\\bar{y}\\) cuando \\(N\\to\\infty\\)?\nObtenemos el valor esperado y la varianza de \\(\\bar{y}\\):\n\\[E(\\bar{y})=\\frac{1}{N}E(\\sum_i y_i)  = E(y_i)=p\\]\n\\[V(\\bar{y})=\\frac{1}{N^2}V(\\sum_i y_i) = \\frac{1}{N}V(y_i)=\\frac{p(1-p)}{N}\\]\nEntonces, un TLC nos daría las condiciones para que:\n\\[\\frac{\\bar{y}-0.7}{\\sqrt{0.21/N}}\\sim\\mathcal{N}(0, 1)\\]\n[5 puntos] Realice el siguiente procedimiento \\(J=1,000\\) veces. Obtenga una muestra de tamaño \\(N=3\\) a partir de la distribución \\(Bernoulli(0.7)\\) y calcule la media muestral \\(\\bar{y}\\). Coleccione las \\(J\\) medias muestrales y luego grafique un histograma de las medias muestrales obtenidas junto con una curva teórica normal con la media y varianza obtenida en la parte b. Comente sobre lo que observa.\n\nset.seed(921)\nreps &lt;- 1000\nn &lt;- 3\np &lt;- 0.7\nv &lt;- p*(1-p)/n\n\nymedias3 &lt;- numeric(reps)\nfor (i in 1:reps){\n sample &lt;- rbinom(n, 1, p)\n ymedias3[i]&lt;-mean(sample)\n}\n\nGraficamos junto con una densidad \\(N(0.7, \\sqrt{0.21/3})\\):\n\nhist(ymedias3, breaks=20, prob=TRUE, \n     xlab=\"Medias\")\ncurve(dnorm(x, mean=p, sd=sqrt(v)), \n      col=\"darkblue\", lwd=2, add=TRUE, yaxt=\"n\")\n\n\n\n\n\n\n\n\nEl histograma no se parece nada a la curva normal.\n[3 puntos] Repita lo realizado en la parte b., ahora con \\(N=15\\). Comente sobre lo que observa.\n\nreps &lt;- 1000\nn &lt;- 15\np &lt;- 0.7\nv &lt;- p*(1-p)/n\n\nymedias15 &lt;- numeric(reps)\nfor (i in 1:reps){\n sample &lt;- rbinom(n, 1, p)\n ymedias15[i]&lt;-mean(sample)\n}\n\nGraficamos junto con una densidad \\(N(0.7, \\sqrtt{0.21/15})\\):\n\nhist(ymedias15, breaks=20, prob=TRUE)\ncurve(dnorm(x, mean=p, sd=sqrt(v)), \n      col=\"darkblue\", lwd=2, add=TRUE, yaxt=\"n\")\n\n\n\n\n\n\n\n\nEl histograma comienza a tener una forma normal.\n[3 puntos] Repita lo realizado en la parte b., ahora con \\(N=1,500\\). Comente sobre lo que observa.\n\n#|echo: true\n\nreps &lt;- 1000\nn &lt;- 1500\np &lt;- 0.7\nv &lt;- p*(1-p)/n\n\nymedias1500 &lt;- numeric(reps)\nfor (i in 1:reps){\n sample &lt;- rbinom(n, 1, p)\n ymedias1500[i]&lt;-mean(sample)\n}\n\nGraficamos junto con una densidad \\(N(0.7, \\sqrt{0.21/1500})\\):\n\nhist(ymedias1500, breaks=20, prob=TRUE, \n     xlab=\"Medias\")\ncurve(dnorm(x, mean=p, sd=sqrt(v)), \n      col=\"darkblue\", lwd=2, add=TRUE, yaxt=\"n\")\n\n\n\n\n\n\n\n\nEl histograma se parece ya a la curva normal. Podrían repetir este ejercicio con un tamaño de muestra más grande incluso y ver qué sucede.\n[5 puntos] ¿Cómo usaría este ejercicio con palabras simples para explicar a una persona que no sabe mucho de estadística sobre la importancia de los teoremas de límite central?\nUn TLC nos permite hacer afirmaciones sobre la distribución de un estadístico. Un estadístico es un resumen de los datos, por lo que nos interesa usar dichos estadísticos para describir características de los fenómenos que estudiamos usando datos. Queremos saber cosas como lo que esperamos en promedio que suceda con una variable, o qué tanta variabilidad dicha variable tendrá en la población. Con un TLC podemos hacer afirmaciones sobre cómo lucen promedios muestrales de la variable que estudiamos cuando tenemos suficientes observaciones. Nos dice en particular que va a tener una distribución normal."
  },
  {
    "objectID": "tareas/tarea-1-respuestas.html#pregunta-6",
    "href": "tareas/tarea-1-respuestas.html#pregunta-6",
    "title": "Respuestas a la tarea 1",
    "section": "",
    "text": "Sea \\(x_1\\) un vector de variables continuas, \\(x_2\\) una variable continua y \\(d_1\\) una variable dicotómica. Considere el siguiente modelo probit: \\[P(y=1│x_1,x_2 )=\\Phi(x_1'\\alpha+\\beta x_2+\\gamma x_2^2 )\\]\n\n[5 punto] Provea una expresión para el efecto marginal de \\(x_2\\) en la probabilidad. ¿Cómo estimaría este efecto marginal?\nEl efecto marginal de interés es:\n\\[\\frac{\\partial P(y=1|x_1,x_2)}{\\partial x_2}=\\phi(x_1\\alpha+\\beta x_2+\\gamma x_2^2)(\\beta+2\\gamma x_2)\\]\nPara estimarlo, usamos máxima verosimilitud para obtener estimadores consistentes de \\(\\alpha\\), \\(\\beta\\) y \\(\\gamma\\) y empleamos software para obener los valores de las probabilidad usando las características individuales \\(x_1\\) y \\(x_2\\). Luego podemos obtener el efecto marginal promedio como el promedio de los efectos marginales individuales. Alternativamente, se podría calcular el efecto marginal en valores específicos de \\(x_1\\) y \\(x_2\\), como \\(\\bar{x}_1\\) y \\(\\bar{x}_2\\).\n[3 punto] Considere ahora el modelo: \\[P(y=1│x_1  ,x_2 ,d_1)=\\Phi(x_1 '\\delta+\\pi x_2+\\rho d_1+\\nu x_2 d_1 )\\] Provea la nueva expresión para el efecto marginal de \\(x_2\\).\nEl efecto marginal es: \\[\\frac{\\partial P(y=1|x_1,x_2)}{\\partial x_2}=\\phi(x_1\\delta+\\pi x_2+\\rho d_1+  \\nu x_2d_1)(\\pi+\\nu d_1)\\]\n[2 punto] En el modelo de la parte b., ¿cómo evaluaría el efecto de un cambio en \\(d_1\\) en la probabilidad? Provea una expresión para este efecto.\nDado que \\(d_1\\) es una variable dicotómica, el efecto de \\(d_1\\) se mide como la diferencia en probabilidad cuando \\(d_1=1\\) y cuando \\(d_1=0\\):\n\\[P(y=1|x_1,x_2,d_1=1)-P(y=1|x_1,x_2,d_1=0)=\\phi(x_1\\delta+(\\pi+\\nu)x_2+\\rho)-\\phi(x_1\\delta+\\pi x_2)\\]\nPodemos estimar este cambio en probabilidad para cada individuo de la muestra y luego obtener un promedio de los cambios en probabilidad. Por ejemplo, supongamos que estudiamos la probabilidad de estar empleado y que \\(d_1\\) es la pertenencia o no a un sindicato. Para cada individuo en la muestra obtemeos el cambio en probabilidad que resulta de que \\(d_1\\) pase de 1 a 0."
  },
  {
    "objectID": "tareas/tarea-2-respuestas.html",
    "href": "tareas/tarea-2-respuestas.html",
    "title": "Respuestas a la tarea 2",
    "section": "",
    "text": "Use los datos en el archivo motral2012.csv, que incluye una muestra de individuos con sus características socioeconómicas. Nos interesa conocer los factores que afectan la probabilidad de que los individuos tengan ahorros formales. Considere lo siguiente sobre las opciones de ahorro de los entrevistados, contenida en la variable p14:\n\np14 igual a 1 significa cuentas de ahorro bancarias\np14 igual a 2 significa cuenta de inversión bancaria\np14 igual a 3 significa inversiones en bienes raíces\np14 igual a 4 significa caja de ahorro en su trabajo\np14 igual a 5 significa caja de ahorro con sus amigos\np14 igual a 6 significa tandas\np14 igual a 7 significa que ahorra en su casa o alcancías\np14 igual a 8 significa otro lugar\np14 NA significa que no ahorra\n\n\n[2 puntos] Comience generando una variable binaria ahorra_inf que tome el valor de 1 para las personas que ahorran en instrumentos informales y 0 en otro caso. Se consideran instrumentos informales las cajas de ahorro en el trabajo o amigos, las tandas y el ahorro en casa o alcancías . Construya también la variable mujer que tome el valor de 1 cuando sex toma el valor de 2 y 0 en otro caso. Luego, estime un modelo de probabilidad lineal que relacione ahorra_inf como variable dependiente con eda (edad), anios_esc (años de escolaridad) y mujer. Reporte los errores que asumen homocedasticidad y los errores robustos a heteroscedasticidad. ¿Qué observa respecto a los errores y por qué sucede?\nGeneramos variables:\n\ndata.financiero &lt;- read_csv(\"../files/motral2012.csv\",\n                          locale = locale(encoding = \"latin1\")) %&gt;%\n  clean_names() %&gt;% \n  mutate(ahorra_inf = case_when(p14 %in% c(4,5,6,7) ~ 1,\n                                .default = 0),\n         mujer=ifelse(sex==2,1,0))\n\nEstimamos el modelo lineal y obtenemos la matriz de varianzas robusta usando vcovHC:\n\nsummary(reg.lineal &lt;- lm(ahorra_inf ~ eda + anios_esc + mujer,\n                         data = data.financiero))\n\n\nCall:\nlm(formula = ahorra_inf ~ eda + anios_esc + mujer, data = data.financiero)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.3365 -0.2580 -0.1844 -0.1092  0.9467 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.4519266  0.0254686  17.744   &lt;2e-16 ***\neda         -0.0063753  0.0005491 -11.610   &lt;2e-16 ***\nanios_esc   -0.0006780  0.0012186  -0.556    0.578    \nmujer       -0.0006674  0.0113305  -0.059    0.953    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4107 on 5260 degrees of freedom\nMultiple R-squared:  0.02507,    Adjusted R-squared:  0.02452 \nF-statistic: 45.09 on 3 and 5260 DF,  p-value: &lt; 2.2e-16\n\n#Matriz robusta\nv_rob &lt;- vcovHC(reg.lineal, type = \"HC0\")\nse_rob    &lt;- sqrt(diag(v_rob))\n\nPresentamos usando modelsummary.\nmodelsummary(list(reg.lineal, reg.lineal),\n             vcov = c(\"iid\", \"HC0\"),\n             stars = c('*'=.1, '**'=.05, '***'=.01))\n \n\n  \n    \n    \n    tinytable_e59qvmmsbodj5hhcbo5l\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n\n              \n                 \n                (1)\n                (2)\n              \n        \n        * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n        \n                \n                  (Intercept)\n                  0.452*** \n                  0.452*** \n                \n                \n                             \n                  (0.025)  \n                  (0.029)  \n                \n                \n                  eda        \n                  -0.006***\n                  -0.006***\n                \n                \n                             \n                  (0.001)  \n                  (0.001)  \n                \n                \n                  anios_esc  \n                  -0.001   \n                  -0.001   \n                \n                \n                             \n                  (0.001)  \n                  (0.002)  \n                \n                \n                  mujer      \n                  -0.001   \n                  -0.001   \n                \n                \n                             \n                  (0.011)  \n                  (0.011)  \n                \n                \n                  Num.Obs.   \n                  5264     \n                  5264     \n                \n                \n                  R2         \n                  0.025    \n                  0.025    \n                \n                \n                  R2 Adj.    \n                  0.025    \n                  0.025    \n                \n                \n                  AIC        \n                  5575.3   \n                  5575.3   \n                \n                \n                  BIC        \n                  5608.1   \n                  5608.1   \n                \n                \n                  Log.Lik.   \n                  -2782.626\n                  -2782.626\n                \n                \n                  F          \n                  45.091   \n                  46.854   \n                \n                \n                  RMSE       \n                  0.41     \n                  0.41     \n                \n                \n                  Std.Errors \n                  IID      \n                  HC0      \n                \n        \n      \n    \n\n    \n\n  \n\n\nEn en problema binario, la media condicional está bien planteada. Y dado que la densidad pertenece a la familia lineal exponencial, basta con que la media condicional esté bien planteada para la consistencia de \\(\\beta\\). Sin embargo, en el caso de los modelos binarios, la varianza condicional también está siempre bien planteada, pues \\(V(y)=p(1-p)\\). Esto implica que no hay ninguna ganancia en usar la matriz de varianzas robustas. Ver CT, p. 469 para una discusión.\n[3 puntos] ¿Cuál es el efecto en la probabilidad de ahorrar informalmente si los años de educación se incrementan en una unidad, pasando de 4 a 5 años de educación?\nUna reducción de 0.1 puntos porcentuales (0.001/100), estadísticamente no significativa.\n[2 puntos] Realice una prueba de significancia conjunta de eda y anios_esc. ¿Qué concluye?\nPodemos usar la función linearHypothesis:\n\ncar::linearHypothesis(reg.lineal, c(\"eda=0\", \"anios_esc=0\"))\n\nLinear hypothesis test\n\nHypothesis:\neda = 0\nanios_esc = 0\n\nModel 1: restricted model\nModel 2: ahorra_inf ~ eda + anios_esc + mujer\n\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1   5262 909.91                                  \n2   5260 887.14  2    22.773 67.512 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nConcluimos que no hay evidencia para afirmar que \\(\\beta_{eda}=\\beta_{anios\\_esc}=0\\).\n[3 puntos] Estime un modelo logit relacionando las mismas variables. Use la función avg_slopes del paquete marginaleffects para obtener los efectos marginales promedio de un cambio en cada uno de los regresores. ¿Por qué difiere la magnitud de este efecto marginal con respecto a la parte b.?\nEstimamos el modelo probit:\n\nreg.logit &lt;- glm(ahorra_inf ~ eda + anios_esc + mujer,\n                  family = binomial(link = \"logit\"),\n                  data = data.financiero)\n\nsummary(reg.logit)$coef\n\n                Estimate  Std. Error      z value     Pr(&gt;|z|)\n(Intercept)  0.081021587 0.150793629   0.53730113 5.910596e-01\neda         -0.038339619 0.003385824 -11.32357193 1.003001e-29\nanios_esc   -0.003787198 0.007627194  -0.49653889 6.195143e-01\nmujer       -0.001539617 0.067250054  -0.02289392 9.817349e-01\n\n\nNoten que el signo de los coeficientes coinciden con el promedio de los efectos marginales:\n\navg_slopes(reg.logit)\n\n\n      Term Contrast  Estimate Std. Error        z Pr(&gt;|z|)     S    2.5 %\n anios_esc    dY/dX -0.000638   0.001285  -0.4966    0.619   0.7 -0.00316\n eda          dY/dX -0.006459   0.000554 -11.6533   &lt;0.001 101.8 -0.00755\n mujer        1 - 0 -0.000259   0.011331  -0.0229    0.982   0.0 -0.02247\n   97.5 %\n  0.00188\n -0.00537\n  0.02195\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n\nEl promedio del efecto marginal de un cambio en los años de educación es de una reducción de 0.06 puntos porcentuales.\n[2 puntos] Ahora estime el efecto marginal en la media para eda y anios_esc y para los hombres, usando la función slopes. ¿Por qué difiere la magnitud de este efecto marginal respecto a la parte b. y la d.?\nPara obtener los efectos marginales evaluados en algún valor \\(X_i\\) de los covariables, debemos especificar estos valores usando datagrid:\n\navg_slopes(reg.logit,\n           newdata = datagrid(eda = mean(data.financiero$eda),\n                              anios_esc = mean(data.financiero$anios_esc),\n                              mujer = 1))\n\n\n      Term Contrast  Estimate Std. Error        z Pr(&gt;|z|)    S    2.5 %\n anios_esc    dY/dX -0.000639   0.001287  -0.4963    0.620  0.7 -0.00316\n eda          dY/dX -0.006465   0.000575 -11.2472   &lt;0.001 95.1 -0.00759\n mujer        1 - 0 -0.000260   0.011346  -0.0229    0.982  0.0 -0.02250\n   97.5 %\n  0.00188\n -0.00534\n  0.02198\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n\nEl efecto marginal de un cambio en los años de escolaridad evaluados en la media de los años de educación y edad, para las mujeres, es de -0.06 puntos. Esto difiere del modelo lineal porque en el modelo lineal los efectos marginales son constantes, mientras que los efectos marginal del modelo no lineal dependen del punto de evaluación. También difiere de los efectos marginales promedio pues aquí solo hemos calculado el efecto marginal una sola vez, para un valor \\(X_i\\), mientras que el promedio de efectos marginales implica calcular el efecto marginal para cada individuo y luego obtener el promedio.\nEn clase les pregunté cómo estimarían el error estándar de los cambios marginales y brevemente mencioné que una forma muy usada es el Método Delta, el cual se basa en que los efectos marginales son funciones no lineales de los parámetros. Esto es lo que efectivamente se usa en la función avg_slopes para obtener los errores estándar y los intervalos de confianza. Aquí pueden leer al respecto.\n[3 puntos] Provea una expresión para la maginitud de:\n\n\\[\\frac{\\frac{\\partial P(y=1)}{\\partial \\; anios\\_esc}}{\\frac{\\partial P(y=1)}{\\partial \\; eda}}\\]\nLa razón de efectos marginales es la razón de coeficientes:\n\\[\\frac{\\frac{\\partial P(y=1)}{\\partial \\; anios\\_esc}}{\\frac{\\partial P(y=1)}{\\partial \\; eda}}=\\frac{\\beta_{anios\\_esc}}{\\beta_{eda}}=\\frac{0.0038 }{0.0383}\\]\n\n\n\nAhora estimará un modelo multinomial empleando los mismos datos en motral2012.csv. El propósito será ahora estudiar los factores relevantes para predecir la forma de ahorro que tienen las personas que ahorran.\n\n[2 puntos] Genere una variable categórica llamada ahorro que sea igual a 1 cuando p14 sea igual a 1 o 2, igual a 2 cuando p14 sea igual a 7, e igual a 3 cuando p14 sea igual a 3, 4, 5, 6 u 8. Haga que esa variable sea missing cuando p14 sea missing. Posteriormente, convierta esta nueva variable en una de factores de forma que el valor 1 tenga la etiqueta “Banco”, el valor 2 tenga la etiqueta “Casa” y el valor 3 tenga la etiqueta “Otro”.\nConstruimos la variable dependiente:\n\ndata.financiero &lt;- read_csv(\"../files/motral2012.csv\",\n                            locale = locale(encoding = \"latin1\")) %&gt;%\n  clean_names() %&gt;% \n  mutate(ahorro=NA) %&gt;% \n  mutate(ahorro=ifelse(p14%in%c(1,2),1,ahorro)) %&gt;%\n  mutate(ahorro=ifelse(p14==7,2,ahorro)) %&gt;% \n  mutate(ahorro=ifelse(p14%in%c(3,4,5,6,8),3,ahorro)) %&gt;% \n  mutate(ahorro=factor(ahorro,\n                   levels=c(1,2,3), labels=c(\"Banco\",\"Casa\",\"Otro\"))) %&gt;%\n  mutate(mujer=ifelse(sex==2,1,0))\n\n[4 puntos] Estime un modelo logit multinomial (regresores invariantes a la alternativa) con la opción de ahorro como variable dependiente y los mismos regresores de la pregunta 1. Hay varios paquetes para hacer esto, pero recomiendo usar la función multinom del paquete nnet. ¿Qué puede decir sobre el coeficiente de años de educación en la alternativa “Casa”?\nUsamos multinom para estimar el modelo logit multinomial:\n\nmultilogit &lt;- nnet::multinom(ahorro~ eda + anios_esc + mujer,\n                              data=data.financiero)\n\n# weights:  15 (8 variable)\ninitial  value 2727.854313 \niter  10 value 2546.085070\nfinal  value 2545.712541 \nconverged\n\nsummary(multilogit)\n\nCall:\nnnet::multinom(formula = ahorro ~ eda + anios_esc + mujer, data = data.financiero)\n\nCoefficients:\n     (Intercept)          eda   anios_esc       mujer\nCasa    3.026880 -0.052310196 -0.14829719  0.09265405\nOtro    0.206704 -0.003501367 -0.04628175 -0.06459305\n\nStd. Errors:\n     (Intercept)         eda  anios_esc      mujer\nCasa   0.2487107 0.005207439 0.01355319 0.09956544\nOtro   0.2476498 0.004964123 0.01285100 0.09995715\n\nResidual Deviance: 5091.425 \nAIC: 5107.425 \n\n\nEn el logit multinominal (regresores invariantes) el coeficiente se interpreta con respecto a una categoría base. En este caso, la categoría base es Banco. El modelo implica que la probabilidad de ahorrar en casa disminuye con un año más de educación, en comparación con la probabilidad de ahorrar en el banco. En particular, sabemos que podemos escribir el log del cociente de la probabilidad de las categorías \\(j\\) y \\(k\\) sean escogidas, normalizando \\(k\\) a ser la base, como:\n\\[\\ln\\left(\\frac{P(y=Casa)}{P(y=Banco)}\\right)=x'\\beta=\\beta_0+\\beta_1 edad + \\beta_2 educación + \\beta_3 mujer \\]\nEs decir, un año más de educación se asocia con una reducción en el log de la razón de momios de 0.15.\n[4 puntos] Calcule los efectos marginales promedio sobre la probabilidad de ahorrar en el banco. Al considerar el cambio en la probabilidad para el caso de las mujeres (cuando la variable mujer pasa de 0 a 1), ¿de qué tamaño es el efecto predicho en la probabilidad de ahorrar en el banco?\nUsamos avg_slopes:\n\navg_slopes(multilogit)\n\n\n Group      Term Contrast Estimate Std. Error       z Pr(&gt;|z|)    S    2.5 %\n Banco anios_esc    dY/dX  0.02285   0.002381   9.595   &lt;0.001 70.0  0.01818\n Banco eda          dY/dX  0.00657   0.000935   7.027   &lt;0.001 38.8  0.00474\n Banco mujer        1 - 0 -0.00343   0.019432  -0.177    0.860  0.2 -0.04152\n Casa  anios_esc    dY/dX -0.02512   0.002231 -11.262   &lt;0.001 95.3 -0.02949\n Casa  eda          dY/dX -0.00982   0.000860 -11.422   &lt;0.001 98.0 -0.01151\n Casa  mujer        1 - 0  0.02271   0.017662   1.286    0.199  2.3 -0.01191\n Otro  anios_esc    dY/dX  0.00228   0.002174   1.046    0.295  1.8 -0.00199\n Otro  eda          dY/dX  0.00325   0.000842   3.863   &lt;0.001 13.1  0.00160\n Otro  mujer        1 - 0 -0.01927   0.017549  -1.098    0.272  1.9 -0.05367\n   97.5 %\n  0.02751\n  0.00840\n  0.03465\n -0.02075\n -0.00814\n  0.05732\n  0.00654\n  0.00490\n  0.01512\n\nColumns: term, group, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  probs \n\n\nEl efecto de ser mujer es de una reducción de 0.3 puntos en la probabilidad de ahorrar en el banco al estimar el promedio de los efectos marginales.\n[3 puntos] Calcule los cocientes de riesgo relativo (relative risk ratios o RRR). ¿Qué significa el hecho de que el RRR asociado a ser mujer sea mayor que 1 en la alternativa “Casa”?\n\n(multilogit_rrr = exp(coef(multilogit)))\n\n     (Intercept)       eda anios_esc     mujer\nCasa   20.632752 0.9490344 0.8621748 1.0970821\nOtro    1.229619 0.9965048 0.9547729 0.9374489\n\n\nLos coeficientes en forma de RRR tienen la interpretación del cambio en el riesgo relativo que una categoría sea elegida con relación al riesgo de escoger la categoría base. En este caso, el ser mujer está asociado con una probabilidad de ahorrar en “Casa” 1.097 veces mayor de que la de ahorrar en “Banco”.\n[2 puntos] Estime nuevamente el modelo, pero ahora, especifique que la alternativa “Casa” sea la alternativa base. ¿Cómo es el RRR de la edad en la alternativa “Banco”? ¿Es esto congruente con lo que obtuvo en la parte d. de esta pregunta?\nPrimero tenemos que cambiar la base. Para esto hacemos uso de que ahorro es una variable de factores. Luego estimamos:\n\ndata.financiero &lt;- data.financiero %&gt;% \n  mutate(ahorro = relevel(ahorro, ref = \"Casa\"))\n\nmultilogit2 &lt;- nnet::multinom(ahorro ~ eda + anios_esc + mujer,\n                              data=data.financiero)\n\n# weights:  15 (8 variable)\ninitial  value 2727.854313 \niter  10 value 2552.696634\nfinal  value 2545.712541 \nconverged\n\n\nObtenemos el RRR:\n\n(multilogit2_rrr = exp(coef(multilogit2)))\n\n      (Intercept)      eda anios_esc     mujer\nBanco  0.04846638 1.053703  1.159857 0.9115111\nOtro   0.05959489 1.050020  1.107401 0.8544952\n\n\nAl cambiar la categoría base a Casa solo se modifica la interpretación relativa. En la parte d. el RRR de la edad para la opción de Casa era 0.949, es decir, si la edad se incrementa en una unidad, la probabilidad de ahorrar en Casa es 0.949 veces la de ahorrar en Banco. Con la nueva categoría base, el RRR de la edad para ahorrar en Banco es 1.054, es decir, si la edad se incrementa en un año, la probabilidad de ahorrar en Banco es 1.054 veces más probable que la probabilidad de ahorrar en Casa. La parte d. implica que \\(P(Casa)=0.949(Banco)\\). Mientras que estimando el modelo con la nueva categoría, \\(P(Banco)=1.054(Casa)\\), o \\(P(Casa)=1/1.054(Banco)\\). Empleando todos los decimales en R se puede notar que 1/1.054≅0.949 Ambos resultados son consistentes.\n\n\n\n\nOtra manera de resolver el problema del exceso de ceros que a veces nos molesta en los modelos Poisson es usar un modelo Poisson inflado en cero (CT, p. 681). La idea es introducir un proceso binario con densidad \\(f_1(\\cdot)\\) para modelar la probabilidad de que \\(y=0\\) y luego una densidad de conteo \\(f_2(\\cdot)\\). Si el proceso binario toma el valor de 0, con probabilidad \\(f_1(0)\\), entonces \\(y=0\\), pero si el proceso binario toma el valor de 1, entonces \\(y={0,1,2,\\ldots}\\). Note que podemos entonces observar ceros por dos razones, por el proceso binomial o por el conteo.\nUn modelo inflado en cero tendrá como densidad:\n\\[\ng(y)=\n\\begin{cases}\nf_1(0)+(1-f_1(0))f_2(0) & \\text{si }y=0 \\\\\n(1-f_1(0))f_2(y)& \\text{si }y\\geq 1\n\\end{cases}\n\\]\nConsidere la variable aleatoria \\(Y\\) con observaciones iid que sigue una distribución Poisson con parámetro \\(\\lambda\\). Y considere una variable un proceso binomial tal que \\(\\pi\\) es la probabilidad de que el conteo no se realice. Entonces:\n\\[\ng(y)=\n\\begin{cases}\n\\pi+(1-\\pi)f_2(0) & \\text{si }y=0 \\\\\n(1-\\pi)f_2(y)& \\text{si }y\\geq 1\n\\end{cases}\n\\]\n\n[4 puntos] Termine de especializar la expresión anterior unsando la distribución Poisson para \\(f_2(\\cdot)\\) para obtener la función de masa de probabilidad del modelo Poisson inflado en cero \\(g(y|\\lambda, \\pi)\\).\nEn el caso particular de un modelo Poisson, sabemos que \\(f_2(0)=P(Y=0)=exp(-\\lambda)\\). Definiendo la probabilidad de observar un conteo cero como \\(\\pi\\), la función de masa de probabilidad del modelo inflado en cero es:\n\\[g(y)=\n\\begin{cases}\n\\pi+(1-\\pi)exp(-\\lambda) \\quad\\text{si }y=0 \\\\\n(1-\\pi)\\frac{\\lambda^y exp(-\\lambda)}{y!} \\quad\\text{si } y \\geq1 \\\\\n\\end{cases}\n\\]\n[5 puntos] Provea una expresión para la función de verosimilitud \\(L(\\lambda,\\pi)=\\prod_{i=1}^N g(y_i|\\lambda, \\pi)\\). Una sugerencia para simplificar sus cálculos es definir una variable \\(X\\) igual al numero de veces que \\(Y_i\\) que toma el valor de cero.\nLa función de verosimilitud del problema es:\n\\[L(\\pi,\\lambda,y_i)=\\prod_i P(Y_i=y_i)\\]\nCon las formas específicas para el modelo Poisson inflado en cero:\n\\[L(\\pi,\\lambda,y_i)=\\prod_{i\\in y_i=0}\\left(\\pi+(1-\\pi)exp(-\\lambda) \\right) \\prod_{i\\in y_i&gt;0}\\left((1-\\pi)\\frac{\\lambda^{y_i} exp(-\\lambda)}{y!}\\right)\\]\nHaciendo \\(X\\) el número de veces que \\(y_i\\) toma el valor de cero, el primer producto es \\(\\left(\\pi+(1-\\pi)exp(-\\lambda) \\right)\\) elevado a la potencia \\(X\\).\n¿Cuántos términos distintos de cero quedan? Quedan \\(n-X\\). El segundo producto en la verosimilitud es:\n\\[\\left((1-\\pi)exp(-\\lambda)\\right)^{n-X}\\frac{\\lambda^{\\sum_i y_i}}{\\prod_{i\\in y_i&gt;0} y!}\\]\nLa verosimilitud es por tanto:\n\\[L(\\pi,\\lambda,y_i)=\\left(\\pi+(1-\\pi)exp(-\\lambda) \\right)^X \\left((1-\\pi)exp(-\\lambda)\\right)^{n-X}\\frac{\\lambda^{\\sum_i y_i}}{\\prod_{i\\in y_i&gt;0} y!}\\]\n[3 puntos] Provea una expresión para la log verosimilitud del problema, \\(\\mathcal{L}(\\lambda,\\pi)\\).\nDada la verosimilitud planteada en la parte anterior, la log verosimilitud es:\n\\[\\mathcal{L}(\\pi,\\lambda,y_i)=X\\ln \\left(\\pi+(1-\\pi)exp(-\\lambda) \\right)+(n-X)\\ln(1-\\pi)-(n-X)\\lambda+n\\bar{Y}\\ln (\\lambda)- \\ln\\left(\\prod_{i\\in y_i&gt;0} y! \\right)\\]\n[3 puntos] Obtenga las condiciones de primer orden que caracterizan la solución del problema de máxima verosimilitud, derivando la log verosimilitud con respecto a \\(\\lambda\\) y a \\(\\pi\\).\nTenemos dos parámetros, así que tenemos dos condiciones de primer orden. Derivando la log verosimilitud con respecto a \\(\\pi\\) obtenemos:\n\\[\\frac{\\partial \\mathcal{L}}{\\partial \\pi}=\\frac{X}{\\pi+(1-\\pi)exp(-\\lambda)}(1-exp(-\\lambda))-\\frac{n-X}{1-\\pi}=0\\]\nLa primer condición (A) es:\n\\[\\frac{X(1-exp(-\\lambda))(1-\\pi)}{\\pi+(1-\\pi)exp(-\\lambda)}=n-X\\quad\\quad\\ldots(A)\\]\nAhora derivando la log verosimilitud con respecto a \\(\\lambda\\):\n\\[\\frac{\\partial \\mathcal{L}}{\\partial \\lambda}=-\\frac{X}{\\pi+(1-\\pi)exp(-\\lambda)}(1-\\pi)exp(-\\lambda)-(n-X)+\\frac{n\\bar{Y}}{\\lambda}=0\\]\nLa segunda condición (B) es:\n\\[\\frac{X(1-\\pi)exp(-\\lambda)}{\\pi+(1-\\pi)exp(-\\lambda)}+(n-X)=\\frac{n\\bar{Y}}{\\lambda}\\quad\\quad\\ldots(B)\\]\n\\((\\hat{\\pi}_{MV},\\hat{\\lambda}_{MV})\\) son los valores de los parámetros que resulven el sistema dado por (A) y (B).\n\n\n\n\nUse los datos phd_articulos.csv, los cuales contienen información sobre el número de artículos publicados para una muestra de entonces estudiantes de doctorado. Nuestra variable de interés será el número de artículos art.\n\n[4 puntos] Estime un modelo Poisson que incluya variables dicotómicas para estudiantes mujeres (female) y para estudiantes casadas o casados (married), el número de hijos mejores de cinco años (kid5), el ranking de prestigio del doctorado (phd) y el número de artículos publicados por su mentor (mentor). Realice la estimación de la matriz de varianzas primero a partir de la varianza teórica que resulta de la igualdad de la matriz de información y luego usando una matriz de sándwich. Interprete los coeficientes estimados.\n\ndata.phd&lt;-read_csv(\"../files/phd_articulos.csv\",\n                          locale = locale(encoding =                \"latin1\"))\n\ndata.phd &lt;- data.phd %&gt;% \n  mutate(female=factor(female,\n                       levels=c('Male','Female')))\n\nmpoisson &lt;- glm(art ~ factor(female) + factor(married) + kid5 + phd + mentor,\n                family=\"poisson\",\n                data=data.phd)\n\nsummary(mpoisson)\n\n\nCall:\nglm(formula = art ~ factor(female) + factor(married) + kid5 + \n    phd + mentor, family = \"poisson\", data = data.phd)\n\nCoefficients:\n                       Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)            0.459860   0.093335   4.927 8.35e-07 ***\nfactor(female)Female  -0.224594   0.054613  -4.112 3.92e-05 ***\nfactor(married)Single -0.155243   0.061374  -2.529   0.0114 *  \nkid5                  -0.184883   0.040127  -4.607 4.08e-06 ***\nphd                    0.012823   0.026397   0.486   0.6271    \nmentor                 0.025543   0.002006  12.733  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 1817.4  on 914  degrees of freedom\nResidual deviance: 1634.4  on 909  degrees of freedom\nAIC: 3314.1\n\nNumber of Fisher Scoring iterations: 5\n\n\nPresentamos errores heterocedásticos y robustos a la heterocedasticidad. Aquí les muestro otro paquete que puede servirles para presentar resultados en trabajos y tesinas, alterntivo a stargazer, modelsummary:\n\nmodelsummary(list(mpoisson, mpoisson),\n             vcov = list('classical', 'robust'),\n             stars = c('***' = 0.01, '**' = 0.05, '*' = 0.1))\n\n \n\n  \n    \n    \n    tinytable_5aw84vld86yzsiomg0ub\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n\n              \n                 \n                (1)\n                (2)\n              \n        \n        * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n        \n                \n                  (Intercept)          \n                  0.460*** \n                  0.460*** \n                \n                \n                                       \n                  (0.093)  \n                  (0.151)  \n                \n                \n                  factor(female)Female \n                  -0.225***\n                  -0.225***\n                \n                \n                                       \n                  (0.055)  \n                  (0.072)  \n                \n                \n                  factor(married)Single\n                  -0.155** \n                  -0.155*  \n                \n                \n                                       \n                  (0.061)  \n                  (0.083)  \n                \n                \n                  kid5                 \n                  -0.185***\n                  -0.185***\n                \n                \n                                       \n                  (0.040)  \n                  (0.057)  \n                \n                \n                  phd                  \n                  0.013    \n                  0.013    \n                \n                \n                                       \n                  (0.026)  \n                  (0.044)  \n                \n                \n                  mentor               \n                  0.026*** \n                  0.026*** \n                \n                \n                                       \n                  (0.002)  \n                  (0.004)  \n                \n                \n                  Num.Obs.             \n                  915      \n                  915      \n                \n                \n                  AIC                  \n                  3314.1   \n                  3314.1   \n                \n                \n                  BIC                  \n                  3343.0   \n                  3343.0   \n                \n                \n                  Log.Lik.             \n                  -1651.056\n                  -1651.056\n                \n                \n                  F                    \n                  43.333   \n                  14.855   \n                \n                \n                  RMSE                 \n                  1.84     \n                  1.84     \n                \n                \n                  Std.Errors           \n                  IID      \n                  HC3      \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nPara las variables continuas, como el número de artículos publicados por el mentor, la interpretación es el cambio en el log conteo esperado. En este caso, un artículo más publicado por el mentor incrementa el log conteo esperado en 0.026. También sabemos que los coeficientes tienen una interpretación de semielasticidad; en este caso, la semielasticidad del conteo con respecto al número de artículos publicados es 0.026. Para las variables dicotómicas, por ejemplo female, la interpretación es la diferencia entre el log conteo esperado entre mujeres y la categoría base (hombres).\n[3 puntos] Obtenga la razón de tasas de incidencia (IRR) para los coeficientes e interprete los resultados.\n\nexp(summary(mpoisson)$coef)\n\n                       Estimate Std. Error      z value Pr(&gt;|z|)\n(Intercept)           1.5838526   1.097829 1.379638e+02 1.000001\nfactor(female)Female  0.7988403   1.056132 1.636793e-02 1.000039\nfactor(married)Single 0.8562068   1.063297 7.970295e-02 1.011490\nkid5                  0.8312018   1.040943 9.977222e-03 1.000004\nphd                   1.0129051   1.026749 1.625407e+00 1.872246\nmentor                1.0258718   1.002008 3.386456e+05 1.000000\n\n\nAunque esto también puede hacerse directamente en modelsummary:\n\nmodelsummary(list(mpoisson, mpoisson),\n             exponentiate = TRUE,\n             vcov = list('classical', 'robust'),\n             stars = c('***' = 0.01, '**' = 0.05, '*' = 0.1))\n\n \n\n  \n    \n    \n    tinytable_m86x4p1kd6srn0483ja3\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n\n              \n                 \n                (1)\n                (2)\n              \n        \n        * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n        \n                \n                  (Intercept)          \n                  1.584*** \n                  1.584*** \n                \n                \n                                       \n                  (0.148)  \n                  (0.240)  \n                \n                \n                  factor(female)Female \n                  0.799*** \n                  0.799*** \n                \n                \n                                       \n                  (0.044)  \n                  (0.058)  \n                \n                \n                  factor(married)Single\n                  0.856**  \n                  0.856*   \n                \n                \n                                       \n                  (0.053)  \n                  (0.071)  \n                \n                \n                  kid5                 \n                  0.831*** \n                  0.831*** \n                \n                \n                                       \n                  (0.033)  \n                  (0.047)  \n                \n                \n                  phd                  \n                  1.013    \n                  1.013    \n                \n                \n                                       \n                  (0.027)  \n                  (0.045)  \n                \n                \n                  mentor               \n                  1.026*** \n                  1.026*** \n                \n                \n                                       \n                  (0.002)  \n                  (0.004)  \n                \n                \n                  Num.Obs.             \n                  915      \n                  915      \n                \n                \n                  AIC                  \n                  3314.1   \n                  3314.1   \n                \n                \n                  BIC                  \n                  3343.0   \n                  3343.0   \n                \n                \n                  Log.Lik.             \n                  -1651.056\n                  -1651.056\n                \n                \n                  F                    \n                  43.333   \n                  14.855   \n                \n                \n                  RMSE                 \n                  1.84     \n                  1.84     \n                \n                \n                  Std.Errors           \n                  IID      \n                  HC3      \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nLa interpretación de los coeficientes se vuelve más sencilla usando irr. Para la variable continua mentor, un artículo más publicado por el mentor está asociado con 1.026 veces más artículos publicados por el estudiante, es decir, un 2.6% más artículos. En cambio, la variable dicotómica para mujeres indica que las mujeres publican 0.8 veces el número de artículos que los hombres.\n[2 puntos] Considere ahora que las mujeres han tenido carreras profesionales más cortas que los hombres, es decir, han estado menos expuestas a la ocurrencia de los eventos publicar. Incorpore esto al análisis y reinterprete los resultados. Pista: explore la opción offeset en glm de R. La columna profage mide la duración efectiva de las carreras profesionales de cada individuo.\nEl razonamiento es que ahora queremos conocer cuál es la tasa de publicación, es decir, \\(art/profage\\). Pero como nuestro podemos Poisson solo puede manejar conteos, podemos modificar el modelo para pasar la edad de la carrera del lado derecho:\n\\[\\begin{aligned}ln(art/profage)&=x'\\beta \\\\ ln(art)&=x'\\beta+\\ln(profage) \\end{aligned}\\]\n\nmpoisson_duracion &lt;- glm(art ~\n                  factor(female) + factor(married) + kid5 + phd + mentor,\n                  offset = log(profage),\n                  family=\"poisson\",\n                  data=data.phd)\n\nsummary(mpoisson_duracion)$coef\n\n                         Estimate  Std. Error     z value      Pr(&gt;|z|)\n(Intercept)           -2.95404558 0.093812104 -31.4889600 1.230266e-217\nfactor(female)Female   0.45874678 0.054721432   8.3833109  5.145931e-17\nfactor(married)Single -0.15598278 0.061347334  -2.5426171  1.100257e-02\nkid5                  -0.18643454 0.040135522  -4.6451256  3.398696e-06\nphd                    0.01801602 0.026428953   0.6816773  4.954430e-01\nmentor                 0.02573493 0.002001731  12.8563329  7.924799e-38\n\n\nHasta ahora hemos asumido que cada individuo ha estado “en riesgo” de publicar por el mismo periodo de tiempo, lo cual puede ser no cierto si, por ejemplo, algunos estudiantes se graduaron antes, o si otros han tenido pausas en sus carreras. Al controlar por el hecho de que las mujeres han tenido carreras más cortas, la variable female deja de ser negativa y se convierte en positiva. Las mujeres publican más que los hombres al tomar en cuenta la duración de las carreras.\nComparando los tres modelos:\n\nmodelsummary(list(mpoisson, mpoisson, mpoisson_duracion),\n             vcov = list('classical', 'robust', 'robust'),\n             stars = c('***' = 0.01, '**' = 0.05, '*' = 0.1))\n\n \n\n  \n    \n    \n    tinytable_1hcxsutk66jsbyopzrux\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n\n              \n                 \n                (1)\n                (2)\n                (3)\n              \n        \n        * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n        \n                \n                  (Intercept)          \n                  0.460*** \n                  0.460*** \n                  -2.954***\n                \n                \n                                       \n                  (0.093)  \n                  (0.151)  \n                  (0.155)  \n                \n                \n                  factor(female)Female \n                  -0.225***\n                  -0.225***\n                  0.459*** \n                \n                \n                                       \n                  (0.055)  \n                  (0.072)  \n                  (0.073)  \n                \n                \n                  factor(married)Single\n                  -0.155** \n                  -0.155*  \n                  -0.156*  \n                \n                \n                                       \n                  (0.061)  \n                  (0.083)  \n                  (0.083)  \n                \n                \n                  kid5                 \n                  -0.185***\n                  -0.185***\n                  -0.186***\n                \n                \n                                       \n                  (0.040)  \n                  (0.057)  \n                  (0.057)  \n                \n                \n                  phd                  \n                  0.013    \n                  0.013    \n                  0.018    \n                \n                \n                                       \n                  (0.026)  \n                  (0.044)  \n                  (0.045)  \n                \n                \n                  mentor               \n                  0.026*** \n                  0.026*** \n                  0.026*** \n                \n                \n                                       \n                  (0.002)  \n                  (0.004)  \n                  (0.005)  \n                \n                \n                  Num.Obs.             \n                  915      \n                  915      \n                  915      \n                \n                \n                  AIC                  \n                  3314.1   \n                  3314.1   \n                  3322.8   \n                \n                \n                  BIC                  \n                  3343.0   \n                  3343.0   \n                  3351.7   \n                \n                \n                  Log.Lik.             \n                  -1651.056\n                  -1651.056\n                  -1655.393\n                \n                \n                  F                    \n                  43.333   \n                  14.855   \n                  20.311   \n                \n                \n                  RMSE                 \n                  1.84     \n                  1.84     \n                  1.85     \n                \n                \n                  Std.Errors           \n                  IID      \n                  HC3      \n                  HC3      \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\n[2 puntos] Implemente la prueba de dispersión de Cameron y Trivedi (1990) usando una regresión auxiliar y los coeficientes estimados en la parte a. ¿Qué concluye?\nSeguimos a CT, p671 y construimos:\n\\[\\frac{(y_i-\\hat{\\mu}_i)^2}{\\hat{\\mu}_i}=\\alpha\\frac{ g(\\hat{\\mu}_i)}{\\hat{\\mu}_i}+u_i\\] Creamos el lado izquierdo:\n\ndata.phd &lt;- data.phd %&gt;% \n  mutate(xb_hat = predict(mpoisson),\n         mu_hat = exp(xb_hat),\n         lado_izq = (art-mu_hat)^2/mu_hat)\n\nNoten que si especificamos \\(g(\\hat{\\mu}_i)=\\hat{\\mu}^2_i\\), el lado derecho simplemente es \\(\\alpha \\hat{\\mu}_i+u_i\\). Estimamos entonces la regresión, sin constante:\nCorremos la regresión:\n\nsummary(lm(lado_izq ~ -1 + mu_hat,\n    data = data.phd))\n\n\nCall:\nlm(formula = lado_izq ~ -1 + mu_hat, data = data.phd)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-5.570 -1.431 -0.798 -0.027 69.967 \n\nCoefficients:\n       Estimate Std. Error t value Pr(&gt;|t|)    \nmu_hat  1.02020    0.08866   11.51   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.881 on 914 degrees of freedom\nMultiple R-squared:  0.1265, Adjusted R-squared:  0.1256 \nF-statistic: 132.4 on 1 and 914 DF,  p-value: &lt; 2.2e-16\n\n\nEl coeficiente sobre \\(\\alpha\\) es estadísticamente significativo, sugiriendo una relación entre la media y la varianza.\n[5 puntos] Emplee ahora un modelo negativo binomial con sobredispersión cuadrática en la media para estimar la relación entre el número de artículos publicados y las variables explicativas antes enumeradas. Interprete el coeficiente asociado al número de hijos y a la variable dicotómica para estudiantes mujeres. ¿Qué puede decir sobre la significancia del \\(\\alpha\\) estimado?\n\nmnb2 &lt;- MASS::glm.nb(art ~\n                 factor(female) + factor(married) + kid5 + phd + mentor,\n                 data = data.phd)\nsummary(mnb2)\n\n\nCall:\nMASS::glm.nb(formula = art ~ factor(female) + factor(married) + \n    kid5 + phd + mentor, data = data.phd, init.theta = 2.264387695, \n    link = log)\n\nCoefficients:\n                       Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)            0.406633   0.125778   3.233 0.001225 ** \nfactor(female)Female  -0.216418   0.072636  -2.979 0.002887 ** \nfactor(married)Single -0.150489   0.082097  -1.833 0.066791 .  \nkid5                  -0.176415   0.052813  -3.340 0.000837 ***\nphd                    0.015271   0.035873   0.426 0.670326    \nmentor                 0.029082   0.003214   9.048  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(2.2644) family taken to be 1)\n\n    Null deviance: 1109.0  on 914  degrees of freedom\nResidual deviance: 1004.3  on 909  degrees of freedom\nAIC: 3135.9\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  2.264 \n          Std. Err.:  0.271 \n\n 2 x log-likelihood:  -3121.917 \n\n\nPonemos todo junto:\n\nmodelsummary(list(mpoisson, mpoisson, mpoisson_duracion, mnb2),\n             vcov = list('classical', 'robust', 'robust', 'robust'),\n             stars = c('***' = 0.01, '**' = 0.05, '*' = 0.1))\n\n \n\n  \n    \n    \n    tinytable_brgvkfr7asijx0qzv9d6\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n\n              \n                 \n                (1)\n                (2)\n                (3)\n                (4)\n              \n        \n        * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n        \n                \n                  (Intercept)          \n                  0.460*** \n                  0.460*** \n                  -2.954***\n                  0.407*** \n                \n                \n                                       \n                  (0.093)  \n                  (0.151)  \n                  (0.155)  \n                  (0.135)  \n                \n                \n                  factor(female)Female \n                  -0.225***\n                  -0.225***\n                  0.459*** \n                  -0.216***\n                \n                \n                                       \n                  (0.055)  \n                  (0.072)  \n                  (0.073)  \n                  (0.071)  \n                \n                \n                  factor(married)Single\n                  -0.155** \n                  -0.155*  \n                  -0.156*  \n                  -0.150*  \n                \n                \n                                       \n                  (0.061)  \n                  (0.083)  \n                  (0.083)  \n                  (0.081)  \n                \n                \n                  kid5                 \n                  -0.185***\n                  -0.185***\n                  -0.186***\n                  -0.176***\n                \n                \n                                       \n                  (0.040)  \n                  (0.057)  \n                  (0.057)  \n                  (0.053)  \n                \n                \n                  phd                  \n                  0.013    \n                  0.013    \n                  0.018    \n                  0.015    \n                \n                \n                                       \n                  (0.026)  \n                  (0.044)  \n                  (0.045)  \n                  (0.038)  \n                \n                \n                  mentor               \n                  0.026*** \n                  0.026*** \n                  0.026*** \n                  0.029*** \n                \n                \n                                       \n                  (0.002)  \n                  (0.004)  \n                  (0.005)  \n                  (0.003)  \n                \n                \n                  Num.Obs.             \n                  915      \n                  915      \n                  915      \n                  915      \n                \n                \n                  AIC                  \n                  3314.1   \n                  3314.1   \n                  3322.8   \n                  3135.9   \n                \n                \n                  BIC                  \n                  3343.0   \n                  3343.0   \n                  3351.7   \n                  3169.6   \n                \n                \n                  Log.Lik.             \n                  -1651.056\n                  -1651.056\n                  -1655.393\n                  -1560.958\n                \n                \n                  F                    \n                  43.333   \n                  14.855   \n                  20.311   \n                  20.935   \n                \n                \n                  RMSE                 \n                  1.84     \n                  1.84     \n                  1.85     \n                  1.86     \n                \n                \n                  Std.Errors           \n                  IID      \n                  HC3      \n                  HC3      \n                  HC3      \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nA diferencia de otros paquetes, glm.nb reporta \\(\\theta=1/\\alpha\\):\n\n(alpha &lt;- 1/summary(mnb2)$theta)        \n\n[1] 0.4416205\n\n\nEste es el modelo NB2 visto en clase y la forma más usada para implementar un modelo negativo binomial. Se asume una sobredispersión cuadrática en la media, con la varianza parametrizada usando \\(\\alpha\\). La interpretación de los coeficientes se mantiene con respecto al modelo Poisson. Los coeficientes tienen magnitudes similares, pero se prefiere el modelo NB2 si el propósito es pronóstico pues toma en cuenta la sobredispersión y le da suficiente flexibilidad a la varianza para depender de manera cuadrática de la media.\nUn poco más cuidado hay que poner en \\(\\alpha\\). En este caso, \\(\\hat{\\alpha}=0.44\\). Pero noten que lo que se reporta es el error estándar de \\(\\theta\\). Como platicamos en clase, con un estadístico podemos hacer un test y obtener un valor \\(p\\), pero una función no lineal del mismo puede que no tenga el mismo valor \\(p\\). Esto ocurre aquí, deberíamos recurrir al método delta para calcular el error estándar de \\(\\alpha\\).\n\n\n\n\nRetome los datos del archivo motral2012.csv usado en la Tarea 1. Estimará un modelo Tobit para explicar los factores que afectan la oferta laboral femenina. En este archivo de datos la variable hrsocup registra las horas trabajadas a la semana.\n\n[2 punto] ¿Qué proporción de la muestra femenina reporta horas trabajadas iguales a cero?\nSi hacemos una dummy de horas positivas, al sacarle la media obtenemos la proporción.\n\ndata.salarios&lt;-read_csv(\"../files/motral2012.csv\",\n                          locale = locale(encoding = \"latin1\")) \n\ndata.salarios &lt;- data.salarios %&gt;% \n  filter(sex==2) %&gt;% \n  mutate(zerohrs=ifelse(hrsocup==0,1,0))\n\nsummary(data.salarios$zerohrs)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0000  0.0000  0.0000  0.3528  1.0000  1.0000 \n\n\nEl 35% de las observaciones tienen cero horas trabajadas.\n[3 puntos] Se desea estimar el efecto de los años de educación (anios_esc) sobre la oferta laboral femenina controlando por el estado marital (casada), la edad (eda) y el número de hijos (n_hij) como una variable continua. En la base, e_con toma el valor de 5 para las personas casadas. Genere la variable dummy casada que tome el valor de 1 para las mujeres casadas y cero en otro caso. Estime un modelo de MCO para hrsocup mayor que cero, usando solo la población femenina. Reporte errores robustos. ¿Cuál es la interpretación sobre el coeficiente de los años de escolaridad?\nEl estimar por MCO, un año más de escolaridad se asocia con 0.17 horas trabajadas más a la semana. Sin embargo, este efecto no es estadísticamente significativo.\n\ndata.salarios &lt;- data.salarios %&gt;% \n  mutate(casada=ifelse(e_con==5,1,0))\n\nreg_mco &lt;- lm(hrsocup ~ anios_esc+casada+eda+n_hij,\n          data=filter(data.salarios,hrsocup&gt;0))\n\ncoeftest(reg_mco,\n         vcov = vcovHC(reg_mco, \"HC1\"))[1:4,]\n\n               Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 36.70129720 1.99116828 18.432042 2.742336e-69\nanios_esc    0.17465627 0.10353350  1.686954 9.179628e-02\ncasada      -3.52571327 0.89724706 -3.929479 8.855253e-05\neda          0.06949593 0.04914655  1.414055 1.575295e-01\n\n\nCon modelsummary podemos hacer pedir la tabla de coeficientes. Podemos especificar qué tipo de errores robustos queremos en la opción vcov:\n\nmodelsummary(list(reg_mco, reg_mco),\n             vcov = list('classical', 'HC1'),\n             stars = c('***' = 0.01, '**' = 0.05, '*' = 0.1))\n\n \n\n  \n    \n    \n    tinytable_pywnwis5vj2flnx51ip4\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n\n              \n                 \n                (1)\n                (2)\n              \n        \n        * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n        \n                \n                  (Intercept)\n                  36.701***\n                  36.701***\n                \n                \n                             \n                  (1.959)  \n                  (1.991)  \n                \n                \n                  anios_esc  \n                  0.175*   \n                  0.175*   \n                \n                \n                             \n                  (0.104)  \n                  (0.104)  \n                \n                \n                  casada     \n                  -3.526***\n                  -3.526***\n                \n                \n                             \n                  (0.862)  \n                  (0.897)  \n                \n                \n                  eda        \n                  0.069    \n                  0.069    \n                \n                \n                             \n                  (0.047)  \n                  (0.049)  \n                \n                \n                  n_hij      \n                  -1.149***\n                  -1.149***\n                \n                \n                             \n                  (0.336)  \n                  (0.372)  \n                \n                \n                  Num.Obs.   \n                  1699     \n                  1699     \n                \n                \n                  R2         \n                  0.030    \n                  0.030    \n                \n                \n                  R2 Adj.    \n                  0.028    \n                  0.028    \n                \n                \n                  AIC        \n                  14234.8  \n                  14234.8  \n                \n                \n                  BIC        \n                  14267.4  \n                  14267.4  \n                \n                \n                  Log.Lik.   \n                  -7111.383\n                  -7111.383\n                \n                \n                  F          \n                  13.171   \n                  12.526   \n                \n                \n                  RMSE       \n                  15.91    \n                  15.91    \n                \n                \n                  Std.Errors \n                  IID      \n                  HC1      \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\n[3 puntos] ¿Qué problema existe con el modelo planteado en el punto anterior en términos de la selección? ¿Considera que se trata de un caso de censura o de truncamiento?\nPodemos racionalizar las horas trabajadas en un modelo microeconómico de oferta laboral. Las horas trabajadas observadas son positivas cuando la solución óptima es una cantidad positiva de horas. Sin embargo, si la solución óptima implicara horas negativas, las horas observadas serían cero. En este caso tenemos datos censurados en cero. Si existe una relación positiva entre educación y horas trabajadas, al estimar un modelo por MCO usando solo los datos con horas positivas estamos sobreestimando la media condicional pues se habrán omitido del análisis aquellas mujeres cuya solución a su problema de optimización eran horas iguales a cero o negativas.\n[8 puntos] Estime un modelo Tobit de datos censurados. ¿Qué resuelve el modelo Tobit en este caso? Interprete nuevamente el coeficiente sobre los años de escolaridad.\nLa función tobit permite hacer esto muy fácilmente. Noten que left especifica dónde está la censura. La opción gaussian pone explícito uno de los supuestos críticos del modelo tobit visto en clase: errores normales. Además, se asume homocedasticidad.\n\nreg_tobit &lt;- tobit(hrsocup ~ anios_esc+casada+eda+n_hij,\n               left = 0,\n               right = Inf,\n               dist = \"gaussian\",\n               data = data.salarios)\n\nsummary(reg_tobit)\n\n\nCall:\ntobit(formula = hrsocup ~ anios_esc + casada + eda + n_hij, left = 0, \n    right = Inf, dist = \"gaussian\", data = data.salarios)\n\nObservations:\n         Total  Left-censored     Uncensored Right-censored \n          2625            926           1699              0 \n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   0.88236    3.19905   0.276  0.78269    \nanios_esc     0.85530    0.17509   4.885 1.04e-06 ***\ncasada      -10.99515    1.43025  -7.688 1.50e-14 ***\neda           0.41621    0.07665   5.430 5.64e-08 ***\nn_hij        -1.73840    0.55887  -3.111  0.00187 ** \nLog(scale)    3.44512    0.01887 182.608  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nScale: 31.35 \n\nGaussian distribution\nNumber of Newton-Raphson Iterations: 3 \nLog-likelihood: -9086 on 6 Df\nWald-statistic: 127.9 on 4 Df, p-value: &lt; 2.22e-16 \n\n\nEl modelo tobit para datos censurados toma en cuenta que hay una masa de ceros en las horas trabajadas para individuos para los que disponemos de sus características en la base de datos. El modelo tobit ajusta la probabilidad de observar esta masa de ceros. El coeficiente estimado será ahora consistente si el modelo está bien especificado, es decir, si el proceso subyacente es lineal en los parámetros y con un error normal homoscedástico (los supuestos de tobit básico). En este caso, un año más de educación se asocia con 0.85 más horas semanales trabajadas, un efecto estadísticamente significativo. Usar MCO subestimaba el efecto de la escolaridad.\nmodelsummary acepta la salida de la función tobit:\n\nmodelsummary(list(reg_mco, reg_mco, reg_tobit),\n             vcov = list('classical', 'HC1', 'classical'),\n             stars = c('***' = 0.01, '**' = 0.05, '*' = 0.1))\n\n \n\n  \n    \n    \n    tinytable_rrkgmst5iynzapwea4g5\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n\n              \n                 \n                (1)\n                (2)\n                (3)\n              \n        \n        * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n        \n                \n                  (Intercept)\n                  36.701***\n                  36.701***\n                  0.882     \n                \n                \n                             \n                  (1.959)  \n                  (1.991)  \n                  (3.199)   \n                \n                \n                  anios_esc  \n                  0.175*   \n                  0.175*   \n                  0.855***  \n                \n                \n                             \n                  (0.104)  \n                  (0.104)  \n                  (0.175)   \n                \n                \n                  casada     \n                  -3.526***\n                  -3.526***\n                  -10.995***\n                \n                \n                             \n                  (0.862)  \n                  (0.897)  \n                  (1.430)   \n                \n                \n                  eda        \n                  0.069    \n                  0.069    \n                  0.416***  \n                \n                \n                             \n                  (0.047)  \n                  (0.049)  \n                  (0.077)   \n                \n                \n                  n_hij      \n                  -1.149***\n                  -1.149***\n                  -1.738*** \n                \n                \n                             \n                  (0.336)  \n                  (0.372)  \n                  (0.559)   \n                \n                \n                  Num.Obs.   \n                  1699     \n                  1699     \n                  2625      \n                \n                \n                  R2         \n                  0.030    \n                  0.030    \n                            \n                \n                \n                  R2 Adj.    \n                  0.028    \n                  0.028    \n                            \n                \n                \n                  AIC        \n                  14234.8  \n                  14234.8  \n                  18184.9   \n                \n                \n                  BIC        \n                  14267.4  \n                  14267.4  \n                  18220.1   \n                \n                \n                  Log.Lik.   \n                  -7111.383\n                  -7111.383\n                            \n                \n                \n                  F          \n                  13.171   \n                  12.526   \n                            \n                \n                \n                  RMSE       \n                  15.91    \n                  15.91    \n                  23.02     \n                \n                \n                  Std.Errors \n                  IID      \n                  HC1      \n                  IID       \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\n[4 puntos] ¿Cuál es el efecto marginal de un incremento de un año de educación en la oferta laboral? ¿Cómo cambia su respuesta si, en lugar de considerar la variable latente, considera la variable censurada?\nEl efecto marginal en la variable latente es directamente el coficiente estimado en la parte d., es decir 0.855.\nEl efecto marginal en la media censurada está dado por:\n\\[\\frac{\\partial E(y|x)}{\\partial x_j}=\\beta_j\\Phi(x_i'\\beta)\\]\nLo que hice aquí fue calcular este efecto marginal para cada individuo y luego obtener el promedio de los efectos marginales en aquellos individuos con horas positivas.\n\ndata.salarios &lt;- data.salarios %&gt;%\n  mutate(index1=predict(reg_tobit,.)) %&gt;% \n  mutate(phi=pnorm(index1/reg_tobit$scale)) %&gt;% \n  mutate(mfx_anis_esc=reg_tobit$coefficients[2]*phi,\n         mfx_eda=reg_tobit$coefficients[4]*phi,\n         mfx_n_hij=reg_tobit$coefficients[5]*phi)\n\ndata.salarios %&gt;%\n  filter(hrsocup&gt;0) %&gt;% \n  summarise(mfx_anis_esc=mean(mfx_anis_esc)) \n\n# A tibble: 1 × 1\n  mfx_anis_esc\n         &lt;dbl&gt;\n1        0.612\n\n\n\n\n\n\nUsando los mismos datos del archivo motral2012.csv implementará un ejercicio en el mismo espíritu del famoso estudio de Mroz (1987)1 sobre la oferta laboral femenina. El propósito es estimar la relación entre el salario y el número de horas trabajadas, concentrándonos en la muestra de mujeres.\n\n[5 puntos] El primer problema al que nos enfrentamos es que el salario no se observa para las mujeres que no trabajan. Estime un modelo lineal para el log del salario por hora, ing_x_hrs, usando las variables anios_esc, eda, n_hij, el cuadrado de n_hij, busqueda y casada, usando la submuestra de mujeres con salario por hora positivo. Dichas variables representan los años de escolaridad, la edad, el número de hijos, el cuadrado del número de hijos, si la persona buscó trabajo recientemente y si la persona es casada, respectivamente. Use los coeficientes estimados para imputar el ingreso por hora, faltante para las mujeres que reportan 0 en las horas trabajadas.\nImputamos el salario faltante:\n\ndata.salarios&lt;-read_csv(\"../files/motral2012.csv\",\n                        locale = locale(encoding = \"latin1\")) %&gt;%\n  filter(sex==2) %&gt;% \n  mutate(casada=ifelse(e_con==5,1,0))\n\ndata.salarios &lt;- data.salarios %&gt;% \n  mutate(ly=ifelse(ing_x_hrs&gt;0,log(ing_x_hrs),NA)) \n\nreg_imput &lt;- lm(ly ~ anios_esc+casada+eda+n_hij+n_hij^2+busqueda,\n              data = data.salarios)\n\ndata.salarios &lt;- data.salarios %&gt;% \n  mutate(lyhat = predict(reg_imput, .)) %&gt;% \n  mutate(ly = ifelse(is.na(ly), lyhat, ly))\n\nAquí tomé en cuenta que hay personas con horas trabajadas positivas e ingreso cero. En ese caso puse un NA al log del salario. Luego, en la imputación, le asigné el valor ajustado a estas observaciones junto con todas las que tienen el log del salario faltante.\n[5 puntos] Use heckit de la librería sampleSelection para estimar por máxima verosimilitud un heckit para las horas trabajadas hrsocup. En la ecuación de selección (si la persona trabaja o no) incluya como variable explicativa el salario por hora (imputado para las mujeres que no trabajan), además de anios_esc, eda, n_hij, el cuadrado de n_hij, casada y busqueda (esta última es un indicador de si se buscó trabajo en la última semana). En la ecuación de horas, incluya los mismos regresores, excepto n_hij, su cuadrado y busqueda.\nLa función heckit permite estimar el modelo de Heckman por máxima verosimilitud de manera muy simple. Hay que especificar method=“ml” para que la estimación sea por máxima verosimilitud:\n\ndata.salarios &lt;- data.salarios %&gt;% \n  mutate(trabaja = ifelse(hrsocup&gt;0,1,0)) %&gt;% \n  mutate(trabaja = factor(trabaja,levels=c(0,1)))\n\nreg_heckit_mv &lt;- heckit(trabaja ~ anios_esc+casada+eda+ly+n_hij+n_hij^2+busqueda,\n                hrsocup ~ anios_esc+casada+eda+ly,\n                method=\"ml\",\n                data = data.salarios)\n\nsummary(reg_heckit_mv)\n\n--------------------------------------------\nTobit 2 model (sample selection model)\nMaximum Likelihood estimation\nNewton-Raphson maximisation, 3 iterations\nReturn code 8: successive function values within relative tolerance limit (reltol)\nLog-Likelihood: -7181.675 \n2625 observations (926 censored and 1699 observed)\n14 free parameters (df = 2611)\nProbit selection equation:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2.583614   0.448320  -5.763 9.24e-09 ***\nanios_esc    0.005346   0.020341   0.263    0.793    \ncasada      -0.213125   0.145135  -1.468    0.142    \neda         -0.003391   0.008137  -0.417    0.677    \nly          -0.004236   0.133344  -0.032    0.975    \nn_hij        0.023985   0.058900   0.407    0.684    \nbusqueda     2.406669   0.104595  23.009  &lt; 2e-16 ***\nOutcome equation:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 55.62469    2.17656  25.556  &lt; 2e-16 ***\nanios_esc    1.04819    0.09995  10.487  &lt; 2e-16 ***\ncasada      -3.58856    0.77967  -4.603 4.37e-06 ***\neda          0.11614    0.03902   2.977  0.00294 ** \nly          -9.83418    0.60389 -16.285  &lt; 2e-16 ***\n   Error terms:\n      Estimate Std. Error t value Pr(&gt;|t|)    \nsigma  14.8579     0.2591  57.350   &lt;2e-16 ***\nrho    -0.1606     0.1964  -0.818    0.414    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n--------------------------------------------\n\n\nPodemos reportar con modelsummary, aunque realmente lo hace muy mal:\n\nmodelsummary(list(reg_heckit_mv),\n             stars = c('***' = 0.01, '**' = 0.05, '*' = 0.1))\n\n \n\n  \n    \n    \n    tinytable_701ffeft592akf3uti3x\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n\n              \n                 \n                (1)\n              \n        \n        * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n        \n                \n                  (Intercept)\n                  -2.584***\n                \n                \n                             \n                  55.625***\n                \n                \n                             \n                  (0.448)  \n                \n                \n                             \n                  (2.177)  \n                \n                \n                  anios_esc  \n                  0.005    \n                \n                \n                             \n                  1.048*** \n                \n                \n                             \n                  (0.020)  \n                \n                \n                             \n                  (0.100)  \n                \n                \n                  casada     \n                  -0.213   \n                \n                \n                             \n                  -3.589***\n                \n                \n                             \n                  (0.145)  \n                \n                \n                             \n                  (0.780)  \n                \n                \n                  eda        \n                  -0.003   \n                \n                \n                             \n                  0.116*** \n                \n                \n                             \n                  (0.008)  \n                \n                \n                             \n                  (0.039)  \n                \n                \n                  ly         \n                  -0.004   \n                \n                \n                             \n                  -9.834***\n                \n                \n                             \n                  (0.133)  \n                \n                \n                             \n                  (0.604)  \n                \n                \n                  n_hij      \n                  0.024    \n                \n                \n                             \n                  (0.059)  \n                \n                \n                  busqueda   \n                  2.407*** \n                \n                \n                             \n                  (0.105)  \n                \n                \n                  sigma      \n                  14.858***\n                \n                \n                             \n                  (0.259)  \n                \n                \n                  rho        \n                  -0.161   \n                \n                \n                             \n                  (0.196)  \n                \n                \n                  Num.Obs.   \n                  2625     \n                \n                \n                  AIC        \n                  14391.4  \n                \n                \n                  BIC        \n                  14473.6  \n                \n                \n                  RMSE       \n                  14.84    \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\n[10 puntos] Estime ahora el heckit en dos pasos, a mano. Es decir, siga los siguientes pasos: i) estime un probit para la ecuación de selección y obtenga el índice \\(x_i'\\hat{\\beta}\\); ii) calcule el inverso de la razón de Mills \\(\\lambda_i(x_i'\\hat{\\beta})\\); y iii) estime por MCO la ecuación para las horas trabajadas con la submuestra que tiene horas trabajadas positivas, incluyendo como regresor el inverso de la razón de Mills estimado y el resto de los regresores. Compare los coeficientes y los errores estándar obtenidos en esta parte con los de la parte b. ¿Por qué son iguales o por qué difieren?\nEstimamos ahora el heckit a mano. Estimamos el probit y obtenemos el valor ajustado del IMR:\n\nreg_heckit_pe &lt;- glm(trabaja ~ anios_esc+casada+eda+ly+n_hij+n_hij^2+busqueda,\n                  family = binomial(link = \"probit\"),\n                  data = data.salarios)\n\ndata.salarios &lt;- data.salarios %&gt;% \n  mutate(index = predict(reg_heckit_pe, .)) %&gt;% \n  mutate(imr = dnorm(index)/pnorm(index))\n\n\nreg_heckit_se &lt;- lm(hrsocup ~ anios_esc+casada+eda+ly+imr,\n            data=filter(data.salarios,trabaja==1))\n\nsummary(reg_heckit_se)\n\n\nCall:\nlm(formula = hrsocup ~ anios_esc + casada + eda + ly + imr, data = filter(data.salarios, \n    trabaja == 1))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-46.172 -10.085   1.915   9.253  57.689 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 55.68676    2.17948  25.550  &lt; 2e-16 ***\nanios_esc    1.04814    0.10000  10.481  &lt; 2e-16 ***\ncasada      -3.56927    0.78057  -4.573 5.17e-06 ***\neda          0.11621    0.03904   2.977  0.00296 ** \nly          -9.82971    0.60406 -16.273  &lt; 2e-16 ***\nimr         -3.94669    3.62684  -1.088  0.27667    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.86 on 1693 degrees of freedom\nMultiple R-squared:  0.1563, Adjusted R-squared:  0.1539 \nF-statistic: 62.75 on 5 and 1693 DF,  p-value: &lt; 2.2e-16\n\n\nComparamos coeficientes (aquí stargazer lo hace mejor):\nstargazer(reg_heckit_mv, reg_heckit_se,\n          type=\"html\", \n          df=FALSE,\n          digits=4)\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nhrsocup\n\n\n\n\n\n\nHeckman\n\n\nOLS\n\n\n\n\n\n\nselection\n\n\n\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nanios_esc\n\n\n1.0482***\n\n\n1.0481***\n\n\n\n\n\n\n(0.0999)\n\n\n(0.1000)\n\n\n\n\n\n\n\n\n\n\n\n\ncasada\n\n\n-3.5886***\n\n\n-3.5693***\n\n\n\n\n\n\n(0.7797)\n\n\n(0.7806)\n\n\n\n\n\n\n\n\n\n\n\n\neda\n\n\n0.1161***\n\n\n0.1162***\n\n\n\n\n\n\n(0.0390)\n\n\n(0.0390)\n\n\n\n\n\n\n\n\n\n\n\n\nly\n\n\n-9.8342***\n\n\n-9.8297***\n\n\n\n\n\n\n(0.6039)\n\n\n(0.6041)\n\n\n\n\n\n\n\n\n\n\n\n\nimr\n\n\n\n\n-3.9467\n\n\n\n\n\n\n\n\n(3.6268)\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n55.6247***\n\n\n55.6868***\n\n\n\n\n\n\n(2.1766)\n\n\n(2.1795)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n2,625\n\n\n1,699\n\n\n\n\nR2\n\n\n0.1563\n\n\n\n\n\n\nAdjusted R2\n\n\n0.1539\n\n\n\n\n\n\nResidual Std. Error\n\n\n\n\n14.8614\n\n\n\n\nF Statistic\n\n\n\n\n62.7490***\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01\n\n\n\nLa magnitud de los coeficientes es práctiamente la misma entre el modelo estimado por máxima verosimilitud y con un procedimiento en dos etapas a mano. En este ejemplo las diferencias son sutiles, aunque recordemos que en general la estimación por MV es más eficiente si la verosimilitud está bien planteada."
  },
  {
    "objectID": "tareas/tarea-2-respuestas.html#pregunta-1",
    "href": "tareas/tarea-2-respuestas.html#pregunta-1",
    "title": "Respuestas a la tarea 2",
    "section": "",
    "text": "Use los datos en el archivo motral2012.csv, que incluye una muestra de individuos con sus características socioeconómicas. Nos interesa conocer los factores que afectan la probabilidad de que los individuos tengan ahorros formales. Considere lo siguiente sobre las opciones de ahorro de los entrevistados, contenida en la variable p14:\n\np14 igual a 1 significa cuentas de ahorro bancarias\np14 igual a 2 significa cuenta de inversión bancaria\np14 igual a 3 significa inversiones en bienes raíces\np14 igual a 4 significa caja de ahorro en su trabajo\np14 igual a 5 significa caja de ahorro con sus amigos\np14 igual a 6 significa tandas\np14 igual a 7 significa que ahorra en su casa o alcancías\np14 igual a 8 significa otro lugar\np14 NA significa que no ahorra\n\n\n[2 puntos] Comience generando una variable binaria ahorra_inf que tome el valor de 1 para las personas que ahorran en instrumentos informales y 0 en otro caso. Se consideran instrumentos informales las cajas de ahorro en el trabajo o amigos, las tandas y el ahorro en casa o alcancías . Construya también la variable mujer que tome el valor de 1 cuando sex toma el valor de 2 y 0 en otro caso. Luego, estime un modelo de probabilidad lineal que relacione ahorra_inf como variable dependiente con eda (edad), anios_esc (años de escolaridad) y mujer. Reporte los errores que asumen homocedasticidad y los errores robustos a heteroscedasticidad. ¿Qué observa respecto a los errores y por qué sucede?\nGeneramos variables:\n\ndata.financiero &lt;- read_csv(\"../files/motral2012.csv\",\n                          locale = locale(encoding = \"latin1\")) %&gt;%\n  clean_names() %&gt;% \n  mutate(ahorra_inf = case_when(p14 %in% c(4,5,6,7) ~ 1,\n                                .default = 0),\n         mujer=ifelse(sex==2,1,0))\n\nEstimamos el modelo lineal y obtenemos la matriz de varianzas robusta usando vcovHC:\n\nsummary(reg.lineal &lt;- lm(ahorra_inf ~ eda + anios_esc + mujer,\n                         data = data.financiero))\n\n\nCall:\nlm(formula = ahorra_inf ~ eda + anios_esc + mujer, data = data.financiero)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.3365 -0.2580 -0.1844 -0.1092  0.9467 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.4519266  0.0254686  17.744   &lt;2e-16 ***\neda         -0.0063753  0.0005491 -11.610   &lt;2e-16 ***\nanios_esc   -0.0006780  0.0012186  -0.556    0.578    \nmujer       -0.0006674  0.0113305  -0.059    0.953    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4107 on 5260 degrees of freedom\nMultiple R-squared:  0.02507,    Adjusted R-squared:  0.02452 \nF-statistic: 45.09 on 3 and 5260 DF,  p-value: &lt; 2.2e-16\n\n#Matriz robusta\nv_rob &lt;- vcovHC(reg.lineal, type = \"HC0\")\nse_rob    &lt;- sqrt(diag(v_rob))\n\nPresentamos usando modelsummary.\nmodelsummary(list(reg.lineal, reg.lineal),\n             vcov = c(\"iid\", \"HC0\"),\n             stars = c('*'=.1, '**'=.05, '***'=.01))\n \n\n  \n    \n    \n    tinytable_e59qvmmsbodj5hhcbo5l\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n\n              \n                 \n                (1)\n                (2)\n              \n        \n        * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n        \n                \n                  (Intercept)\n                  0.452*** \n                  0.452*** \n                \n                \n                             \n                  (0.025)  \n                  (0.029)  \n                \n                \n                  eda        \n                  -0.006***\n                  -0.006***\n                \n                \n                             \n                  (0.001)  \n                  (0.001)  \n                \n                \n                  anios_esc  \n                  -0.001   \n                  -0.001   \n                \n                \n                             \n                  (0.001)  \n                  (0.002)  \n                \n                \n                  mujer      \n                  -0.001   \n                  -0.001   \n                \n                \n                             \n                  (0.011)  \n                  (0.011)  \n                \n                \n                  Num.Obs.   \n                  5264     \n                  5264     \n                \n                \n                  R2         \n                  0.025    \n                  0.025    \n                \n                \n                  R2 Adj.    \n                  0.025    \n                  0.025    \n                \n                \n                  AIC        \n                  5575.3   \n                  5575.3   \n                \n                \n                  BIC        \n                  5608.1   \n                  5608.1   \n                \n                \n                  Log.Lik.   \n                  -2782.626\n                  -2782.626\n                \n                \n                  F          \n                  45.091   \n                  46.854   \n                \n                \n                  RMSE       \n                  0.41     \n                  0.41     \n                \n                \n                  Std.Errors \n                  IID      \n                  HC0      \n                \n        \n      \n    \n\n    \n\n  \n\n\nEn en problema binario, la media condicional está bien planteada. Y dado que la densidad pertenece a la familia lineal exponencial, basta con que la media condicional esté bien planteada para la consistencia de \\(\\beta\\). Sin embargo, en el caso de los modelos binarios, la varianza condicional también está siempre bien planteada, pues \\(V(y)=p(1-p)\\). Esto implica que no hay ninguna ganancia en usar la matriz de varianzas robustas. Ver CT, p. 469 para una discusión.\n[3 puntos] ¿Cuál es el efecto en la probabilidad de ahorrar informalmente si los años de educación se incrementan en una unidad, pasando de 4 a 5 años de educación?\nUna reducción de 0.1 puntos porcentuales (0.001/100), estadísticamente no significativa.\n[2 puntos] Realice una prueba de significancia conjunta de eda y anios_esc. ¿Qué concluye?\nPodemos usar la función linearHypothesis:\n\ncar::linearHypothesis(reg.lineal, c(\"eda=0\", \"anios_esc=0\"))\n\nLinear hypothesis test\n\nHypothesis:\neda = 0\nanios_esc = 0\n\nModel 1: restricted model\nModel 2: ahorra_inf ~ eda + anios_esc + mujer\n\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1   5262 909.91                                  \n2   5260 887.14  2    22.773 67.512 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nConcluimos que no hay evidencia para afirmar que \\(\\beta_{eda}=\\beta_{anios\\_esc}=0\\).\n[3 puntos] Estime un modelo logit relacionando las mismas variables. Use la función avg_slopes del paquete marginaleffects para obtener los efectos marginales promedio de un cambio en cada uno de los regresores. ¿Por qué difiere la magnitud de este efecto marginal con respecto a la parte b.?\nEstimamos el modelo probit:\n\nreg.logit &lt;- glm(ahorra_inf ~ eda + anios_esc + mujer,\n                  family = binomial(link = \"logit\"),\n                  data = data.financiero)\n\nsummary(reg.logit)$coef\n\n                Estimate  Std. Error      z value     Pr(&gt;|z|)\n(Intercept)  0.081021587 0.150793629   0.53730113 5.910596e-01\neda         -0.038339619 0.003385824 -11.32357193 1.003001e-29\nanios_esc   -0.003787198 0.007627194  -0.49653889 6.195143e-01\nmujer       -0.001539617 0.067250054  -0.02289392 9.817349e-01\n\n\nNoten que el signo de los coeficientes coinciden con el promedio de los efectos marginales:\n\navg_slopes(reg.logit)\n\n\n      Term Contrast  Estimate Std. Error        z Pr(&gt;|z|)     S    2.5 %\n anios_esc    dY/dX -0.000638   0.001285  -0.4966    0.619   0.7 -0.00316\n eda          dY/dX -0.006459   0.000554 -11.6533   &lt;0.001 101.8 -0.00755\n mujer        1 - 0 -0.000259   0.011331  -0.0229    0.982   0.0 -0.02247\n   97.5 %\n  0.00188\n -0.00537\n  0.02195\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n\nEl promedio del efecto marginal de un cambio en los años de educación es de una reducción de 0.06 puntos porcentuales.\n[2 puntos] Ahora estime el efecto marginal en la media para eda y anios_esc y para los hombres, usando la función slopes. ¿Por qué difiere la magnitud de este efecto marginal respecto a la parte b. y la d.?\nPara obtener los efectos marginales evaluados en algún valor \\(X_i\\) de los covariables, debemos especificar estos valores usando datagrid:\n\navg_slopes(reg.logit,\n           newdata = datagrid(eda = mean(data.financiero$eda),\n                              anios_esc = mean(data.financiero$anios_esc),\n                              mujer = 1))\n\n\n      Term Contrast  Estimate Std. Error        z Pr(&gt;|z|)    S    2.5 %\n anios_esc    dY/dX -0.000639   0.001287  -0.4963    0.620  0.7 -0.00316\n eda          dY/dX -0.006465   0.000575 -11.2472   &lt;0.001 95.1 -0.00759\n mujer        1 - 0 -0.000260   0.011346  -0.0229    0.982  0.0 -0.02250\n   97.5 %\n  0.00188\n -0.00534\n  0.02198\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n\nEl efecto marginal de un cambio en los años de escolaridad evaluados en la media de los años de educación y edad, para las mujeres, es de -0.06 puntos. Esto difiere del modelo lineal porque en el modelo lineal los efectos marginales son constantes, mientras que los efectos marginal del modelo no lineal dependen del punto de evaluación. También difiere de los efectos marginales promedio pues aquí solo hemos calculado el efecto marginal una sola vez, para un valor \\(X_i\\), mientras que el promedio de efectos marginales implica calcular el efecto marginal para cada individuo y luego obtener el promedio.\nEn clase les pregunté cómo estimarían el error estándar de los cambios marginales y brevemente mencioné que una forma muy usada es el Método Delta, el cual se basa en que los efectos marginales son funciones no lineales de los parámetros. Esto es lo que efectivamente se usa en la función avg_slopes para obtener los errores estándar y los intervalos de confianza. Aquí pueden leer al respecto.\n[3 puntos] Provea una expresión para la maginitud de:\n\n\\[\\frac{\\frac{\\partial P(y=1)}{\\partial \\; anios\\_esc}}{\\frac{\\partial P(y=1)}{\\partial \\; eda}}\\]\nLa razón de efectos marginales es la razón de coeficientes:\n\\[\\frac{\\frac{\\partial P(y=1)}{\\partial \\; anios\\_esc}}{\\frac{\\partial P(y=1)}{\\partial \\; eda}}=\\frac{\\beta_{anios\\_esc}}{\\beta_{eda}}=\\frac{0.0038 }{0.0383}\\]"
  },
  {
    "objectID": "tareas/tarea-2-respuestas.html#pregunta-2",
    "href": "tareas/tarea-2-respuestas.html#pregunta-2",
    "title": "Respuestas a la tarea 2",
    "section": "",
    "text": "Ahora estimará un modelo multinomial empleando los mismos datos en motral2012.csv. El propósito será ahora estudiar los factores relevantes para predecir la forma de ahorro que tienen las personas que ahorran.\n\n[2 puntos] Genere una variable categórica llamada ahorro que sea igual a 1 cuando p14 sea igual a 1 o 2, igual a 2 cuando p14 sea igual a 7, e igual a 3 cuando p14 sea igual a 3, 4, 5, 6 u 8. Haga que esa variable sea missing cuando p14 sea missing. Posteriormente, convierta esta nueva variable en una de factores de forma que el valor 1 tenga la etiqueta “Banco”, el valor 2 tenga la etiqueta “Casa” y el valor 3 tenga la etiqueta “Otro”.\nConstruimos la variable dependiente:\n\ndata.financiero &lt;- read_csv(\"../files/motral2012.csv\",\n                            locale = locale(encoding = \"latin1\")) %&gt;%\n  clean_names() %&gt;% \n  mutate(ahorro=NA) %&gt;% \n  mutate(ahorro=ifelse(p14%in%c(1,2),1,ahorro)) %&gt;%\n  mutate(ahorro=ifelse(p14==7,2,ahorro)) %&gt;% \n  mutate(ahorro=ifelse(p14%in%c(3,4,5,6,8),3,ahorro)) %&gt;% \n  mutate(ahorro=factor(ahorro,\n                   levels=c(1,2,3), labels=c(\"Banco\",\"Casa\",\"Otro\"))) %&gt;%\n  mutate(mujer=ifelse(sex==2,1,0))\n\n[4 puntos] Estime un modelo logit multinomial (regresores invariantes a la alternativa) con la opción de ahorro como variable dependiente y los mismos regresores de la pregunta 1. Hay varios paquetes para hacer esto, pero recomiendo usar la función multinom del paquete nnet. ¿Qué puede decir sobre el coeficiente de años de educación en la alternativa “Casa”?\nUsamos multinom para estimar el modelo logit multinomial:\n\nmultilogit &lt;- nnet::multinom(ahorro~ eda + anios_esc + mujer,\n                              data=data.financiero)\n\n# weights:  15 (8 variable)\ninitial  value 2727.854313 \niter  10 value 2546.085070\nfinal  value 2545.712541 \nconverged\n\nsummary(multilogit)\n\nCall:\nnnet::multinom(formula = ahorro ~ eda + anios_esc + mujer, data = data.financiero)\n\nCoefficients:\n     (Intercept)          eda   anios_esc       mujer\nCasa    3.026880 -0.052310196 -0.14829719  0.09265405\nOtro    0.206704 -0.003501367 -0.04628175 -0.06459305\n\nStd. Errors:\n     (Intercept)         eda  anios_esc      mujer\nCasa   0.2487107 0.005207439 0.01355319 0.09956544\nOtro   0.2476498 0.004964123 0.01285100 0.09995715\n\nResidual Deviance: 5091.425 \nAIC: 5107.425 \n\n\nEn el logit multinominal (regresores invariantes) el coeficiente se interpreta con respecto a una categoría base. En este caso, la categoría base es Banco. El modelo implica que la probabilidad de ahorrar en casa disminuye con un año más de educación, en comparación con la probabilidad de ahorrar en el banco. En particular, sabemos que podemos escribir el log del cociente de la probabilidad de las categorías \\(j\\) y \\(k\\) sean escogidas, normalizando \\(k\\) a ser la base, como:\n\\[\\ln\\left(\\frac{P(y=Casa)}{P(y=Banco)}\\right)=x'\\beta=\\beta_0+\\beta_1 edad + \\beta_2 educación + \\beta_3 mujer \\]\nEs decir, un año más de educación se asocia con una reducción en el log de la razón de momios de 0.15.\n[4 puntos] Calcule los efectos marginales promedio sobre la probabilidad de ahorrar en el banco. Al considerar el cambio en la probabilidad para el caso de las mujeres (cuando la variable mujer pasa de 0 a 1), ¿de qué tamaño es el efecto predicho en la probabilidad de ahorrar en el banco?\nUsamos avg_slopes:\n\navg_slopes(multilogit)\n\n\n Group      Term Contrast Estimate Std. Error       z Pr(&gt;|z|)    S    2.5 %\n Banco anios_esc    dY/dX  0.02285   0.002381   9.595   &lt;0.001 70.0  0.01818\n Banco eda          dY/dX  0.00657   0.000935   7.027   &lt;0.001 38.8  0.00474\n Banco mujer        1 - 0 -0.00343   0.019432  -0.177    0.860  0.2 -0.04152\n Casa  anios_esc    dY/dX -0.02512   0.002231 -11.262   &lt;0.001 95.3 -0.02949\n Casa  eda          dY/dX -0.00982   0.000860 -11.422   &lt;0.001 98.0 -0.01151\n Casa  mujer        1 - 0  0.02271   0.017662   1.286    0.199  2.3 -0.01191\n Otro  anios_esc    dY/dX  0.00228   0.002174   1.046    0.295  1.8 -0.00199\n Otro  eda          dY/dX  0.00325   0.000842   3.863   &lt;0.001 13.1  0.00160\n Otro  mujer        1 - 0 -0.01927   0.017549  -1.098    0.272  1.9 -0.05367\n   97.5 %\n  0.02751\n  0.00840\n  0.03465\n -0.02075\n -0.00814\n  0.05732\n  0.00654\n  0.00490\n  0.01512\n\nColumns: term, group, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  probs \n\n\nEl efecto de ser mujer es de una reducción de 0.3 puntos en la probabilidad de ahorrar en el banco al estimar el promedio de los efectos marginales.\n[3 puntos] Calcule los cocientes de riesgo relativo (relative risk ratios o RRR). ¿Qué significa el hecho de que el RRR asociado a ser mujer sea mayor que 1 en la alternativa “Casa”?\n\n(multilogit_rrr = exp(coef(multilogit)))\n\n     (Intercept)       eda anios_esc     mujer\nCasa   20.632752 0.9490344 0.8621748 1.0970821\nOtro    1.229619 0.9965048 0.9547729 0.9374489\n\n\nLos coeficientes en forma de RRR tienen la interpretación del cambio en el riesgo relativo que una categoría sea elegida con relación al riesgo de escoger la categoría base. En este caso, el ser mujer está asociado con una probabilidad de ahorrar en “Casa” 1.097 veces mayor de que la de ahorrar en “Banco”.\n[2 puntos] Estime nuevamente el modelo, pero ahora, especifique que la alternativa “Casa” sea la alternativa base. ¿Cómo es el RRR de la edad en la alternativa “Banco”? ¿Es esto congruente con lo que obtuvo en la parte d. de esta pregunta?\nPrimero tenemos que cambiar la base. Para esto hacemos uso de que ahorro es una variable de factores. Luego estimamos:\n\ndata.financiero &lt;- data.financiero %&gt;% \n  mutate(ahorro = relevel(ahorro, ref = \"Casa\"))\n\nmultilogit2 &lt;- nnet::multinom(ahorro ~ eda + anios_esc + mujer,\n                              data=data.financiero)\n\n# weights:  15 (8 variable)\ninitial  value 2727.854313 \niter  10 value 2552.696634\nfinal  value 2545.712541 \nconverged\n\n\nObtenemos el RRR:\n\n(multilogit2_rrr = exp(coef(multilogit2)))\n\n      (Intercept)      eda anios_esc     mujer\nBanco  0.04846638 1.053703  1.159857 0.9115111\nOtro   0.05959489 1.050020  1.107401 0.8544952\n\n\nAl cambiar la categoría base a Casa solo se modifica la interpretación relativa. En la parte d. el RRR de la edad para la opción de Casa era 0.949, es decir, si la edad se incrementa en una unidad, la probabilidad de ahorrar en Casa es 0.949 veces la de ahorrar en Banco. Con la nueva categoría base, el RRR de la edad para ahorrar en Banco es 1.054, es decir, si la edad se incrementa en un año, la probabilidad de ahorrar en Banco es 1.054 veces más probable que la probabilidad de ahorrar en Casa. La parte d. implica que \\(P(Casa)=0.949(Banco)\\). Mientras que estimando el modelo con la nueva categoría, \\(P(Banco)=1.054(Casa)\\), o \\(P(Casa)=1/1.054(Banco)\\). Empleando todos los decimales en R se puede notar que 1/1.054≅0.949 Ambos resultados son consistentes."
  },
  {
    "objectID": "tareas/tarea-2-respuestas.html#pregunta-3-modelo-poisson-inflado-en-cero",
    "href": "tareas/tarea-2-respuestas.html#pregunta-3-modelo-poisson-inflado-en-cero",
    "title": "Respuestas a la tarea 2",
    "section": "",
    "text": "Otra manera de resolver el problema del exceso de ceros que a veces nos molesta en los modelos Poisson es usar un modelo Poisson inflado en cero (CT, p. 681). La idea es introducir un proceso binario con densidad \\(f_1(\\cdot)\\) para modelar la probabilidad de que \\(y=0\\) y luego una densidad de conteo \\(f_2(\\cdot)\\). Si el proceso binario toma el valor de 0, con probabilidad \\(f_1(0)\\), entonces \\(y=0\\), pero si el proceso binario toma el valor de 1, entonces \\(y={0,1,2,\\ldots}\\). Note que podemos entonces observar ceros por dos razones, por el proceso binomial o por el conteo.\nUn modelo inflado en cero tendrá como densidad:\n\\[\ng(y)=\n\\begin{cases}\nf_1(0)+(1-f_1(0))f_2(0) & \\text{si }y=0 \\\\\n(1-f_1(0))f_2(y)& \\text{si }y\\geq 1\n\\end{cases}\n\\]\nConsidere la variable aleatoria \\(Y\\) con observaciones iid que sigue una distribución Poisson con parámetro \\(\\lambda\\). Y considere una variable un proceso binomial tal que \\(\\pi\\) es la probabilidad de que el conteo no se realice. Entonces:\n\\[\ng(y)=\n\\begin{cases}\n\\pi+(1-\\pi)f_2(0) & \\text{si }y=0 \\\\\n(1-\\pi)f_2(y)& \\text{si }y\\geq 1\n\\end{cases}\n\\]\n\n[4 puntos] Termine de especializar la expresión anterior unsando la distribución Poisson para \\(f_2(\\cdot)\\) para obtener la función de masa de probabilidad del modelo Poisson inflado en cero \\(g(y|\\lambda, \\pi)\\).\nEn el caso particular de un modelo Poisson, sabemos que \\(f_2(0)=P(Y=0)=exp(-\\lambda)\\). Definiendo la probabilidad de observar un conteo cero como \\(\\pi\\), la función de masa de probabilidad del modelo inflado en cero es:\n\\[g(y)=\n\\begin{cases}\n\\pi+(1-\\pi)exp(-\\lambda) \\quad\\text{si }y=0 \\\\\n(1-\\pi)\\frac{\\lambda^y exp(-\\lambda)}{y!} \\quad\\text{si } y \\geq1 \\\\\n\\end{cases}\n\\]\n[5 puntos] Provea una expresión para la función de verosimilitud \\(L(\\lambda,\\pi)=\\prod_{i=1}^N g(y_i|\\lambda, \\pi)\\). Una sugerencia para simplificar sus cálculos es definir una variable \\(X\\) igual al numero de veces que \\(Y_i\\) que toma el valor de cero.\nLa función de verosimilitud del problema es:\n\\[L(\\pi,\\lambda,y_i)=\\prod_i P(Y_i=y_i)\\]\nCon las formas específicas para el modelo Poisson inflado en cero:\n\\[L(\\pi,\\lambda,y_i)=\\prod_{i\\in y_i=0}\\left(\\pi+(1-\\pi)exp(-\\lambda) \\right) \\prod_{i\\in y_i&gt;0}\\left((1-\\pi)\\frac{\\lambda^{y_i} exp(-\\lambda)}{y!}\\right)\\]\nHaciendo \\(X\\) el número de veces que \\(y_i\\) toma el valor de cero, el primer producto es \\(\\left(\\pi+(1-\\pi)exp(-\\lambda) \\right)\\) elevado a la potencia \\(X\\).\n¿Cuántos términos distintos de cero quedan? Quedan \\(n-X\\). El segundo producto en la verosimilitud es:\n\\[\\left((1-\\pi)exp(-\\lambda)\\right)^{n-X}\\frac{\\lambda^{\\sum_i y_i}}{\\prod_{i\\in y_i&gt;0} y!}\\]\nLa verosimilitud es por tanto:\n\\[L(\\pi,\\lambda,y_i)=\\left(\\pi+(1-\\pi)exp(-\\lambda) \\right)^X \\left((1-\\pi)exp(-\\lambda)\\right)^{n-X}\\frac{\\lambda^{\\sum_i y_i}}{\\prod_{i\\in y_i&gt;0} y!}\\]\n[3 puntos] Provea una expresión para la log verosimilitud del problema, \\(\\mathcal{L}(\\lambda,\\pi)\\).\nDada la verosimilitud planteada en la parte anterior, la log verosimilitud es:\n\\[\\mathcal{L}(\\pi,\\lambda,y_i)=X\\ln \\left(\\pi+(1-\\pi)exp(-\\lambda) \\right)+(n-X)\\ln(1-\\pi)-(n-X)\\lambda+n\\bar{Y}\\ln (\\lambda)- \\ln\\left(\\prod_{i\\in y_i&gt;0} y! \\right)\\]\n[3 puntos] Obtenga las condiciones de primer orden que caracterizan la solución del problema de máxima verosimilitud, derivando la log verosimilitud con respecto a \\(\\lambda\\) y a \\(\\pi\\).\nTenemos dos parámetros, así que tenemos dos condiciones de primer orden. Derivando la log verosimilitud con respecto a \\(\\pi\\) obtenemos:\n\\[\\frac{\\partial \\mathcal{L}}{\\partial \\pi}=\\frac{X}{\\pi+(1-\\pi)exp(-\\lambda)}(1-exp(-\\lambda))-\\frac{n-X}{1-\\pi}=0\\]\nLa primer condición (A) es:\n\\[\\frac{X(1-exp(-\\lambda))(1-\\pi)}{\\pi+(1-\\pi)exp(-\\lambda)}=n-X\\quad\\quad\\ldots(A)\\]\nAhora derivando la log verosimilitud con respecto a \\(\\lambda\\):\n\\[\\frac{\\partial \\mathcal{L}}{\\partial \\lambda}=-\\frac{X}{\\pi+(1-\\pi)exp(-\\lambda)}(1-\\pi)exp(-\\lambda)-(n-X)+\\frac{n\\bar{Y}}{\\lambda}=0\\]\nLa segunda condición (B) es:\n\\[\\frac{X(1-\\pi)exp(-\\lambda)}{\\pi+(1-\\pi)exp(-\\lambda)}+(n-X)=\\frac{n\\bar{Y}}{\\lambda}\\quad\\quad\\ldots(B)\\]\n\\((\\hat{\\pi}_{MV},\\hat{\\lambda}_{MV})\\) son los valores de los parámetros que resulven el sistema dado por (A) y (B)."
  },
  {
    "objectID": "tareas/tarea-2-respuestas.html#pregunta-4",
    "href": "tareas/tarea-2-respuestas.html#pregunta-4",
    "title": "Respuestas a la tarea 2",
    "section": "",
    "text": "Use los datos phd_articulos.csv, los cuales contienen información sobre el número de artículos publicados para una muestra de entonces estudiantes de doctorado. Nuestra variable de interés será el número de artículos art.\n\n[4 puntos] Estime un modelo Poisson que incluya variables dicotómicas para estudiantes mujeres (female) y para estudiantes casadas o casados (married), el número de hijos mejores de cinco años (kid5), el ranking de prestigio del doctorado (phd) y el número de artículos publicados por su mentor (mentor). Realice la estimación de la matriz de varianzas primero a partir de la varianza teórica que resulta de la igualdad de la matriz de información y luego usando una matriz de sándwich. Interprete los coeficientes estimados.\n\ndata.phd&lt;-read_csv(\"../files/phd_articulos.csv\",\n                          locale = locale(encoding =                \"latin1\"))\n\ndata.phd &lt;- data.phd %&gt;% \n  mutate(female=factor(female,\n                       levels=c('Male','Female')))\n\nmpoisson &lt;- glm(art ~ factor(female) + factor(married) + kid5 + phd + mentor,\n                family=\"poisson\",\n                data=data.phd)\n\nsummary(mpoisson)\n\n\nCall:\nglm(formula = art ~ factor(female) + factor(married) + kid5 + \n    phd + mentor, family = \"poisson\", data = data.phd)\n\nCoefficients:\n                       Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)            0.459860   0.093335   4.927 8.35e-07 ***\nfactor(female)Female  -0.224594   0.054613  -4.112 3.92e-05 ***\nfactor(married)Single -0.155243   0.061374  -2.529   0.0114 *  \nkid5                  -0.184883   0.040127  -4.607 4.08e-06 ***\nphd                    0.012823   0.026397   0.486   0.6271    \nmentor                 0.025543   0.002006  12.733  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 1817.4  on 914  degrees of freedom\nResidual deviance: 1634.4  on 909  degrees of freedom\nAIC: 3314.1\n\nNumber of Fisher Scoring iterations: 5\n\n\nPresentamos errores heterocedásticos y robustos a la heterocedasticidad. Aquí les muestro otro paquete que puede servirles para presentar resultados en trabajos y tesinas, alterntivo a stargazer, modelsummary:\n\nmodelsummary(list(mpoisson, mpoisson),\n             vcov = list('classical', 'robust'),\n             stars = c('***' = 0.01, '**' = 0.05, '*' = 0.1))\n\n \n\n  \n    \n    \n    tinytable_5aw84vld86yzsiomg0ub\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n\n              \n                 \n                (1)\n                (2)\n              \n        \n        * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n        \n                \n                  (Intercept)          \n                  0.460*** \n                  0.460*** \n                \n                \n                                       \n                  (0.093)  \n                  (0.151)  \n                \n                \n                  factor(female)Female \n                  -0.225***\n                  -0.225***\n                \n                \n                                       \n                  (0.055)  \n                  (0.072)  \n                \n                \n                  factor(married)Single\n                  -0.155** \n                  -0.155*  \n                \n                \n                                       \n                  (0.061)  \n                  (0.083)  \n                \n                \n                  kid5                 \n                  -0.185***\n                  -0.185***\n                \n                \n                                       \n                  (0.040)  \n                  (0.057)  \n                \n                \n                  phd                  \n                  0.013    \n                  0.013    \n                \n                \n                                       \n                  (0.026)  \n                  (0.044)  \n                \n                \n                  mentor               \n                  0.026*** \n                  0.026*** \n                \n                \n                                       \n                  (0.002)  \n                  (0.004)  \n                \n                \n                  Num.Obs.             \n                  915      \n                  915      \n                \n                \n                  AIC                  \n                  3314.1   \n                  3314.1   \n                \n                \n                  BIC                  \n                  3343.0   \n                  3343.0   \n                \n                \n                  Log.Lik.             \n                  -1651.056\n                  -1651.056\n                \n                \n                  F                    \n                  43.333   \n                  14.855   \n                \n                \n                  RMSE                 \n                  1.84     \n                  1.84     \n                \n                \n                  Std.Errors           \n                  IID      \n                  HC3      \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nPara las variables continuas, como el número de artículos publicados por el mentor, la interpretación es el cambio en el log conteo esperado. En este caso, un artículo más publicado por el mentor incrementa el log conteo esperado en 0.026. También sabemos que los coeficientes tienen una interpretación de semielasticidad; en este caso, la semielasticidad del conteo con respecto al número de artículos publicados es 0.026. Para las variables dicotómicas, por ejemplo female, la interpretación es la diferencia entre el log conteo esperado entre mujeres y la categoría base (hombres).\n[3 puntos] Obtenga la razón de tasas de incidencia (IRR) para los coeficientes e interprete los resultados.\n\nexp(summary(mpoisson)$coef)\n\n                       Estimate Std. Error      z value Pr(&gt;|z|)\n(Intercept)           1.5838526   1.097829 1.379638e+02 1.000001\nfactor(female)Female  0.7988403   1.056132 1.636793e-02 1.000039\nfactor(married)Single 0.8562068   1.063297 7.970295e-02 1.011490\nkid5                  0.8312018   1.040943 9.977222e-03 1.000004\nphd                   1.0129051   1.026749 1.625407e+00 1.872246\nmentor                1.0258718   1.002008 3.386456e+05 1.000000\n\n\nAunque esto también puede hacerse directamente en modelsummary:\n\nmodelsummary(list(mpoisson, mpoisson),\n             exponentiate = TRUE,\n             vcov = list('classical', 'robust'),\n             stars = c('***' = 0.01, '**' = 0.05, '*' = 0.1))\n\n \n\n  \n    \n    \n    tinytable_m86x4p1kd6srn0483ja3\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n\n              \n                 \n                (1)\n                (2)\n              \n        \n        * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n        \n                \n                  (Intercept)          \n                  1.584*** \n                  1.584*** \n                \n                \n                                       \n                  (0.148)  \n                  (0.240)  \n                \n                \n                  factor(female)Female \n                  0.799*** \n                  0.799*** \n                \n                \n                                       \n                  (0.044)  \n                  (0.058)  \n                \n                \n                  factor(married)Single\n                  0.856**  \n                  0.856*   \n                \n                \n                                       \n                  (0.053)  \n                  (0.071)  \n                \n                \n                  kid5                 \n                  0.831*** \n                  0.831*** \n                \n                \n                                       \n                  (0.033)  \n                  (0.047)  \n                \n                \n                  phd                  \n                  1.013    \n                  1.013    \n                \n                \n                                       \n                  (0.027)  \n                  (0.045)  \n                \n                \n                  mentor               \n                  1.026*** \n                  1.026*** \n                \n                \n                                       \n                  (0.002)  \n                  (0.004)  \n                \n                \n                  Num.Obs.             \n                  915      \n                  915      \n                \n                \n                  AIC                  \n                  3314.1   \n                  3314.1   \n                \n                \n                  BIC                  \n                  3343.0   \n                  3343.0   \n                \n                \n                  Log.Lik.             \n                  -1651.056\n                  -1651.056\n                \n                \n                  F                    \n                  43.333   \n                  14.855   \n                \n                \n                  RMSE                 \n                  1.84     \n                  1.84     \n                \n                \n                  Std.Errors           \n                  IID      \n                  HC3      \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nLa interpretación de los coeficientes se vuelve más sencilla usando irr. Para la variable continua mentor, un artículo más publicado por el mentor está asociado con 1.026 veces más artículos publicados por el estudiante, es decir, un 2.6% más artículos. En cambio, la variable dicotómica para mujeres indica que las mujeres publican 0.8 veces el número de artículos que los hombres.\n[2 puntos] Considere ahora que las mujeres han tenido carreras profesionales más cortas que los hombres, es decir, han estado menos expuestas a la ocurrencia de los eventos publicar. Incorpore esto al análisis y reinterprete los resultados. Pista: explore la opción offeset en glm de R. La columna profage mide la duración efectiva de las carreras profesionales de cada individuo.\nEl razonamiento es que ahora queremos conocer cuál es la tasa de publicación, es decir, \\(art/profage\\). Pero como nuestro podemos Poisson solo puede manejar conteos, podemos modificar el modelo para pasar la edad de la carrera del lado derecho:\n\\[\\begin{aligned}ln(art/profage)&=x'\\beta \\\\ ln(art)&=x'\\beta+\\ln(profage) \\end{aligned}\\]\n\nmpoisson_duracion &lt;- glm(art ~\n                  factor(female) + factor(married) + kid5 + phd + mentor,\n                  offset = log(profage),\n                  family=\"poisson\",\n                  data=data.phd)\n\nsummary(mpoisson_duracion)$coef\n\n                         Estimate  Std. Error     z value      Pr(&gt;|z|)\n(Intercept)           -2.95404558 0.093812104 -31.4889600 1.230266e-217\nfactor(female)Female   0.45874678 0.054721432   8.3833109  5.145931e-17\nfactor(married)Single -0.15598278 0.061347334  -2.5426171  1.100257e-02\nkid5                  -0.18643454 0.040135522  -4.6451256  3.398696e-06\nphd                    0.01801602 0.026428953   0.6816773  4.954430e-01\nmentor                 0.02573493 0.002001731  12.8563329  7.924799e-38\n\n\nHasta ahora hemos asumido que cada individuo ha estado “en riesgo” de publicar por el mismo periodo de tiempo, lo cual puede ser no cierto si, por ejemplo, algunos estudiantes se graduaron antes, o si otros han tenido pausas en sus carreras. Al controlar por el hecho de que las mujeres han tenido carreras más cortas, la variable female deja de ser negativa y se convierte en positiva. Las mujeres publican más que los hombres al tomar en cuenta la duración de las carreras.\nComparando los tres modelos:\n\nmodelsummary(list(mpoisson, mpoisson, mpoisson_duracion),\n             vcov = list('classical', 'robust', 'robust'),\n             stars = c('***' = 0.01, '**' = 0.05, '*' = 0.1))\n\n \n\n  \n    \n    \n    tinytable_1hcxsutk66jsbyopzrux\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n\n              \n                 \n                (1)\n                (2)\n                (3)\n              \n        \n        * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n        \n                \n                  (Intercept)          \n                  0.460*** \n                  0.460*** \n                  -2.954***\n                \n                \n                                       \n                  (0.093)  \n                  (0.151)  \n                  (0.155)  \n                \n                \n                  factor(female)Female \n                  -0.225***\n                  -0.225***\n                  0.459*** \n                \n                \n                                       \n                  (0.055)  \n                  (0.072)  \n                  (0.073)  \n                \n                \n                  factor(married)Single\n                  -0.155** \n                  -0.155*  \n                  -0.156*  \n                \n                \n                                       \n                  (0.061)  \n                  (0.083)  \n                  (0.083)  \n                \n                \n                  kid5                 \n                  -0.185***\n                  -0.185***\n                  -0.186***\n                \n                \n                                       \n                  (0.040)  \n                  (0.057)  \n                  (0.057)  \n                \n                \n                  phd                  \n                  0.013    \n                  0.013    \n                  0.018    \n                \n                \n                                       \n                  (0.026)  \n                  (0.044)  \n                  (0.045)  \n                \n                \n                  mentor               \n                  0.026*** \n                  0.026*** \n                  0.026*** \n                \n                \n                                       \n                  (0.002)  \n                  (0.004)  \n                  (0.005)  \n                \n                \n                  Num.Obs.             \n                  915      \n                  915      \n                  915      \n                \n                \n                  AIC                  \n                  3314.1   \n                  3314.1   \n                  3322.8   \n                \n                \n                  BIC                  \n                  3343.0   \n                  3343.0   \n                  3351.7   \n                \n                \n                  Log.Lik.             \n                  -1651.056\n                  -1651.056\n                  -1655.393\n                \n                \n                  F                    \n                  43.333   \n                  14.855   \n                  20.311   \n                \n                \n                  RMSE                 \n                  1.84     \n                  1.84     \n                  1.85     \n                \n                \n                  Std.Errors           \n                  IID      \n                  HC3      \n                  HC3      \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\n[2 puntos] Implemente la prueba de dispersión de Cameron y Trivedi (1990) usando una regresión auxiliar y los coeficientes estimados en la parte a. ¿Qué concluye?\nSeguimos a CT, p671 y construimos:\n\\[\\frac{(y_i-\\hat{\\mu}_i)^2}{\\hat{\\mu}_i}=\\alpha\\frac{ g(\\hat{\\mu}_i)}{\\hat{\\mu}_i}+u_i\\] Creamos el lado izquierdo:\n\ndata.phd &lt;- data.phd %&gt;% \n  mutate(xb_hat = predict(mpoisson),\n         mu_hat = exp(xb_hat),\n         lado_izq = (art-mu_hat)^2/mu_hat)\n\nNoten que si especificamos \\(g(\\hat{\\mu}_i)=\\hat{\\mu}^2_i\\), el lado derecho simplemente es \\(\\alpha \\hat{\\mu}_i+u_i\\). Estimamos entonces la regresión, sin constante:\nCorremos la regresión:\n\nsummary(lm(lado_izq ~ -1 + mu_hat,\n    data = data.phd))\n\n\nCall:\nlm(formula = lado_izq ~ -1 + mu_hat, data = data.phd)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-5.570 -1.431 -0.798 -0.027 69.967 \n\nCoefficients:\n       Estimate Std. Error t value Pr(&gt;|t|)    \nmu_hat  1.02020    0.08866   11.51   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.881 on 914 degrees of freedom\nMultiple R-squared:  0.1265, Adjusted R-squared:  0.1256 \nF-statistic: 132.4 on 1 and 914 DF,  p-value: &lt; 2.2e-16\n\n\nEl coeficiente sobre \\(\\alpha\\) es estadísticamente significativo, sugiriendo una relación entre la media y la varianza.\n[5 puntos] Emplee ahora un modelo negativo binomial con sobredispersión cuadrática en la media para estimar la relación entre el número de artículos publicados y las variables explicativas antes enumeradas. Interprete el coeficiente asociado al número de hijos y a la variable dicotómica para estudiantes mujeres. ¿Qué puede decir sobre la significancia del \\(\\alpha\\) estimado?\n\nmnb2 &lt;- MASS::glm.nb(art ~\n                 factor(female) + factor(married) + kid5 + phd + mentor,\n                 data = data.phd)\nsummary(mnb2)\n\n\nCall:\nMASS::glm.nb(formula = art ~ factor(female) + factor(married) + \n    kid5 + phd + mentor, data = data.phd, init.theta = 2.264387695, \n    link = log)\n\nCoefficients:\n                       Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)            0.406633   0.125778   3.233 0.001225 ** \nfactor(female)Female  -0.216418   0.072636  -2.979 0.002887 ** \nfactor(married)Single -0.150489   0.082097  -1.833 0.066791 .  \nkid5                  -0.176415   0.052813  -3.340 0.000837 ***\nphd                    0.015271   0.035873   0.426 0.670326    \nmentor                 0.029082   0.003214   9.048  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(2.2644) family taken to be 1)\n\n    Null deviance: 1109.0  on 914  degrees of freedom\nResidual deviance: 1004.3  on 909  degrees of freedom\nAIC: 3135.9\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  2.264 \n          Std. Err.:  0.271 \n\n 2 x log-likelihood:  -3121.917 \n\n\nPonemos todo junto:\n\nmodelsummary(list(mpoisson, mpoisson, mpoisson_duracion, mnb2),\n             vcov = list('classical', 'robust', 'robust', 'robust'),\n             stars = c('***' = 0.01, '**' = 0.05, '*' = 0.1))\n\n \n\n  \n    \n    \n    tinytable_brgvkfr7asijx0qzv9d6\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n\n              \n                 \n                (1)\n                (2)\n                (3)\n                (4)\n              \n        \n        * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n        \n                \n                  (Intercept)          \n                  0.460*** \n                  0.460*** \n                  -2.954***\n                  0.407*** \n                \n                \n                                       \n                  (0.093)  \n                  (0.151)  \n                  (0.155)  \n                  (0.135)  \n                \n                \n                  factor(female)Female \n                  -0.225***\n                  -0.225***\n                  0.459*** \n                  -0.216***\n                \n                \n                                       \n                  (0.055)  \n                  (0.072)  \n                  (0.073)  \n                  (0.071)  \n                \n                \n                  factor(married)Single\n                  -0.155** \n                  -0.155*  \n                  -0.156*  \n                  -0.150*  \n                \n                \n                                       \n                  (0.061)  \n                  (0.083)  \n                  (0.083)  \n                  (0.081)  \n                \n                \n                  kid5                 \n                  -0.185***\n                  -0.185***\n                  -0.186***\n                  -0.176***\n                \n                \n                                       \n                  (0.040)  \n                  (0.057)  \n                  (0.057)  \n                  (0.053)  \n                \n                \n                  phd                  \n                  0.013    \n                  0.013    \n                  0.018    \n                  0.015    \n                \n                \n                                       \n                  (0.026)  \n                  (0.044)  \n                  (0.045)  \n                  (0.038)  \n                \n                \n                  mentor               \n                  0.026*** \n                  0.026*** \n                  0.026*** \n                  0.029*** \n                \n                \n                                       \n                  (0.002)  \n                  (0.004)  \n                  (0.005)  \n                  (0.003)  \n                \n                \n                  Num.Obs.             \n                  915      \n                  915      \n                  915      \n                  915      \n                \n                \n                  AIC                  \n                  3314.1   \n                  3314.1   \n                  3322.8   \n                  3135.9   \n                \n                \n                  BIC                  \n                  3343.0   \n                  3343.0   \n                  3351.7   \n                  3169.6   \n                \n                \n                  Log.Lik.             \n                  -1651.056\n                  -1651.056\n                  -1655.393\n                  -1560.958\n                \n                \n                  F                    \n                  43.333   \n                  14.855   \n                  20.311   \n                  20.935   \n                \n                \n                  RMSE                 \n                  1.84     \n                  1.84     \n                  1.85     \n                  1.86     \n                \n                \n                  Std.Errors           \n                  IID      \n                  HC3      \n                  HC3      \n                  HC3      \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nA diferencia de otros paquetes, glm.nb reporta \\(\\theta=1/\\alpha\\):\n\n(alpha &lt;- 1/summary(mnb2)$theta)        \n\n[1] 0.4416205\n\n\nEste es el modelo NB2 visto en clase y la forma más usada para implementar un modelo negativo binomial. Se asume una sobredispersión cuadrática en la media, con la varianza parametrizada usando \\(\\alpha\\). La interpretación de los coeficientes se mantiene con respecto al modelo Poisson. Los coeficientes tienen magnitudes similares, pero se prefiere el modelo NB2 si el propósito es pronóstico pues toma en cuenta la sobredispersión y le da suficiente flexibilidad a la varianza para depender de manera cuadrática de la media.\nUn poco más cuidado hay que poner en \\(\\alpha\\). En este caso, \\(\\hat{\\alpha}=0.44\\). Pero noten que lo que se reporta es el error estándar de \\(\\theta\\). Como platicamos en clase, con un estadístico podemos hacer un test y obtener un valor \\(p\\), pero una función no lineal del mismo puede que no tenga el mismo valor \\(p\\). Esto ocurre aquí, deberíamos recurrir al método delta para calcular el error estándar de \\(\\alpha\\)."
  },
  {
    "objectID": "tareas/tarea-2-respuestas.html#pregunta-5",
    "href": "tareas/tarea-2-respuestas.html#pregunta-5",
    "title": "Respuestas a la tarea 2",
    "section": "",
    "text": "Retome los datos del archivo motral2012.csv usado en la Tarea 1. Estimará un modelo Tobit para explicar los factores que afectan la oferta laboral femenina. En este archivo de datos la variable hrsocup registra las horas trabajadas a la semana.\n\n[2 punto] ¿Qué proporción de la muestra femenina reporta horas trabajadas iguales a cero?\nSi hacemos una dummy de horas positivas, al sacarle la media obtenemos la proporción.\n\ndata.salarios&lt;-read_csv(\"../files/motral2012.csv\",\n                          locale = locale(encoding = \"latin1\")) \n\ndata.salarios &lt;- data.salarios %&gt;% \n  filter(sex==2) %&gt;% \n  mutate(zerohrs=ifelse(hrsocup==0,1,0))\n\nsummary(data.salarios$zerohrs)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0000  0.0000  0.0000  0.3528  1.0000  1.0000 \n\n\nEl 35% de las observaciones tienen cero horas trabajadas.\n[3 puntos] Se desea estimar el efecto de los años de educación (anios_esc) sobre la oferta laboral femenina controlando por el estado marital (casada), la edad (eda) y el número de hijos (n_hij) como una variable continua. En la base, e_con toma el valor de 5 para las personas casadas. Genere la variable dummy casada que tome el valor de 1 para las mujeres casadas y cero en otro caso. Estime un modelo de MCO para hrsocup mayor que cero, usando solo la población femenina. Reporte errores robustos. ¿Cuál es la interpretación sobre el coeficiente de los años de escolaridad?\nEl estimar por MCO, un año más de escolaridad se asocia con 0.17 horas trabajadas más a la semana. Sin embargo, este efecto no es estadísticamente significativo.\n\ndata.salarios &lt;- data.salarios %&gt;% \n  mutate(casada=ifelse(e_con==5,1,0))\n\nreg_mco &lt;- lm(hrsocup ~ anios_esc+casada+eda+n_hij,\n          data=filter(data.salarios,hrsocup&gt;0))\n\ncoeftest(reg_mco,\n         vcov = vcovHC(reg_mco, \"HC1\"))[1:4,]\n\n               Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 36.70129720 1.99116828 18.432042 2.742336e-69\nanios_esc    0.17465627 0.10353350  1.686954 9.179628e-02\ncasada      -3.52571327 0.89724706 -3.929479 8.855253e-05\neda          0.06949593 0.04914655  1.414055 1.575295e-01\n\n\nCon modelsummary podemos hacer pedir la tabla de coeficientes. Podemos especificar qué tipo de errores robustos queremos en la opción vcov:\n\nmodelsummary(list(reg_mco, reg_mco),\n             vcov = list('classical', 'HC1'),\n             stars = c('***' = 0.01, '**' = 0.05, '*' = 0.1))\n\n \n\n  \n    \n    \n    tinytable_pywnwis5vj2flnx51ip4\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n\n              \n                 \n                (1)\n                (2)\n              \n        \n        * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n        \n                \n                  (Intercept)\n                  36.701***\n                  36.701***\n                \n                \n                             \n                  (1.959)  \n                  (1.991)  \n                \n                \n                  anios_esc  \n                  0.175*   \n                  0.175*   \n                \n                \n                             \n                  (0.104)  \n                  (0.104)  \n                \n                \n                  casada     \n                  -3.526***\n                  -3.526***\n                \n                \n                             \n                  (0.862)  \n                  (0.897)  \n                \n                \n                  eda        \n                  0.069    \n                  0.069    \n                \n                \n                             \n                  (0.047)  \n                  (0.049)  \n                \n                \n                  n_hij      \n                  -1.149***\n                  -1.149***\n                \n                \n                             \n                  (0.336)  \n                  (0.372)  \n                \n                \n                  Num.Obs.   \n                  1699     \n                  1699     \n                \n                \n                  R2         \n                  0.030    \n                  0.030    \n                \n                \n                  R2 Adj.    \n                  0.028    \n                  0.028    \n                \n                \n                  AIC        \n                  14234.8  \n                  14234.8  \n                \n                \n                  BIC        \n                  14267.4  \n                  14267.4  \n                \n                \n                  Log.Lik.   \n                  -7111.383\n                  -7111.383\n                \n                \n                  F          \n                  13.171   \n                  12.526   \n                \n                \n                  RMSE       \n                  15.91    \n                  15.91    \n                \n                \n                  Std.Errors \n                  IID      \n                  HC1      \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\n[3 puntos] ¿Qué problema existe con el modelo planteado en el punto anterior en términos de la selección? ¿Considera que se trata de un caso de censura o de truncamiento?\nPodemos racionalizar las horas trabajadas en un modelo microeconómico de oferta laboral. Las horas trabajadas observadas son positivas cuando la solución óptima es una cantidad positiva de horas. Sin embargo, si la solución óptima implicara horas negativas, las horas observadas serían cero. En este caso tenemos datos censurados en cero. Si existe una relación positiva entre educación y horas trabajadas, al estimar un modelo por MCO usando solo los datos con horas positivas estamos sobreestimando la media condicional pues se habrán omitido del análisis aquellas mujeres cuya solución a su problema de optimización eran horas iguales a cero o negativas.\n[8 puntos] Estime un modelo Tobit de datos censurados. ¿Qué resuelve el modelo Tobit en este caso? Interprete nuevamente el coeficiente sobre los años de escolaridad.\nLa función tobit permite hacer esto muy fácilmente. Noten que left especifica dónde está la censura. La opción gaussian pone explícito uno de los supuestos críticos del modelo tobit visto en clase: errores normales. Además, se asume homocedasticidad.\n\nreg_tobit &lt;- tobit(hrsocup ~ anios_esc+casada+eda+n_hij,\n               left = 0,\n               right = Inf,\n               dist = \"gaussian\",\n               data = data.salarios)\n\nsummary(reg_tobit)\n\n\nCall:\ntobit(formula = hrsocup ~ anios_esc + casada + eda + n_hij, left = 0, \n    right = Inf, dist = \"gaussian\", data = data.salarios)\n\nObservations:\n         Total  Left-censored     Uncensored Right-censored \n          2625            926           1699              0 \n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   0.88236    3.19905   0.276  0.78269    \nanios_esc     0.85530    0.17509   4.885 1.04e-06 ***\ncasada      -10.99515    1.43025  -7.688 1.50e-14 ***\neda           0.41621    0.07665   5.430 5.64e-08 ***\nn_hij        -1.73840    0.55887  -3.111  0.00187 ** \nLog(scale)    3.44512    0.01887 182.608  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nScale: 31.35 \n\nGaussian distribution\nNumber of Newton-Raphson Iterations: 3 \nLog-likelihood: -9086 on 6 Df\nWald-statistic: 127.9 on 4 Df, p-value: &lt; 2.22e-16 \n\n\nEl modelo tobit para datos censurados toma en cuenta que hay una masa de ceros en las horas trabajadas para individuos para los que disponemos de sus características en la base de datos. El modelo tobit ajusta la probabilidad de observar esta masa de ceros. El coeficiente estimado será ahora consistente si el modelo está bien especificado, es decir, si el proceso subyacente es lineal en los parámetros y con un error normal homoscedástico (los supuestos de tobit básico). En este caso, un año más de educación se asocia con 0.85 más horas semanales trabajadas, un efecto estadísticamente significativo. Usar MCO subestimaba el efecto de la escolaridad.\nmodelsummary acepta la salida de la función tobit:\n\nmodelsummary(list(reg_mco, reg_mco, reg_tobit),\n             vcov = list('classical', 'HC1', 'classical'),\n             stars = c('***' = 0.01, '**' = 0.05, '*' = 0.1))\n\n \n\n  \n    \n    \n    tinytable_rrkgmst5iynzapwea4g5\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n\n              \n                 \n                (1)\n                (2)\n                (3)\n              \n        \n        * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n        \n                \n                  (Intercept)\n                  36.701***\n                  36.701***\n                  0.882     \n                \n                \n                             \n                  (1.959)  \n                  (1.991)  \n                  (3.199)   \n                \n                \n                  anios_esc  \n                  0.175*   \n                  0.175*   \n                  0.855***  \n                \n                \n                             \n                  (0.104)  \n                  (0.104)  \n                  (0.175)   \n                \n                \n                  casada     \n                  -3.526***\n                  -3.526***\n                  -10.995***\n                \n                \n                             \n                  (0.862)  \n                  (0.897)  \n                  (1.430)   \n                \n                \n                  eda        \n                  0.069    \n                  0.069    \n                  0.416***  \n                \n                \n                             \n                  (0.047)  \n                  (0.049)  \n                  (0.077)   \n                \n                \n                  n_hij      \n                  -1.149***\n                  -1.149***\n                  -1.738*** \n                \n                \n                             \n                  (0.336)  \n                  (0.372)  \n                  (0.559)   \n                \n                \n                  Num.Obs.   \n                  1699     \n                  1699     \n                  2625      \n                \n                \n                  R2         \n                  0.030    \n                  0.030    \n                            \n                \n                \n                  R2 Adj.    \n                  0.028    \n                  0.028    \n                            \n                \n                \n                  AIC        \n                  14234.8  \n                  14234.8  \n                  18184.9   \n                \n                \n                  BIC        \n                  14267.4  \n                  14267.4  \n                  18220.1   \n                \n                \n                  Log.Lik.   \n                  -7111.383\n                  -7111.383\n                            \n                \n                \n                  F          \n                  13.171   \n                  12.526   \n                            \n                \n                \n                  RMSE       \n                  15.91    \n                  15.91    \n                  23.02     \n                \n                \n                  Std.Errors \n                  IID      \n                  HC1      \n                  IID       \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\n[4 puntos] ¿Cuál es el efecto marginal de un incremento de un año de educación en la oferta laboral? ¿Cómo cambia su respuesta si, en lugar de considerar la variable latente, considera la variable censurada?\nEl efecto marginal en la variable latente es directamente el coficiente estimado en la parte d., es decir 0.855.\nEl efecto marginal en la media censurada está dado por:\n\\[\\frac{\\partial E(y|x)}{\\partial x_j}=\\beta_j\\Phi(x_i'\\beta)\\]\nLo que hice aquí fue calcular este efecto marginal para cada individuo y luego obtener el promedio de los efectos marginales en aquellos individuos con horas positivas.\n\ndata.salarios &lt;- data.salarios %&gt;%\n  mutate(index1=predict(reg_tobit,.)) %&gt;% \n  mutate(phi=pnorm(index1/reg_tobit$scale)) %&gt;% \n  mutate(mfx_anis_esc=reg_tobit$coefficients[2]*phi,\n         mfx_eda=reg_tobit$coefficients[4]*phi,\n         mfx_n_hij=reg_tobit$coefficients[5]*phi)\n\ndata.salarios %&gt;%\n  filter(hrsocup&gt;0) %&gt;% \n  summarise(mfx_anis_esc=mean(mfx_anis_esc)) \n\n# A tibble: 1 × 1\n  mfx_anis_esc\n         &lt;dbl&gt;\n1        0.612"
  },
  {
    "objectID": "tareas/tarea-2-respuestas.html#pregunta-6",
    "href": "tareas/tarea-2-respuestas.html#pregunta-6",
    "title": "Respuestas a la tarea 2",
    "section": "",
    "text": "Usando los mismos datos del archivo motral2012.csv implementará un ejercicio en el mismo espíritu del famoso estudio de Mroz (1987)1 sobre la oferta laboral femenina. El propósito es estimar la relación entre el salario y el número de horas trabajadas, concentrándonos en la muestra de mujeres.\n\n[5 puntos] El primer problema al que nos enfrentamos es que el salario no se observa para las mujeres que no trabajan. Estime un modelo lineal para el log del salario por hora, ing_x_hrs, usando las variables anios_esc, eda, n_hij, el cuadrado de n_hij, busqueda y casada, usando la submuestra de mujeres con salario por hora positivo. Dichas variables representan los años de escolaridad, la edad, el número de hijos, el cuadrado del número de hijos, si la persona buscó trabajo recientemente y si la persona es casada, respectivamente. Use los coeficientes estimados para imputar el ingreso por hora, faltante para las mujeres que reportan 0 en las horas trabajadas.\nImputamos el salario faltante:\n\ndata.salarios&lt;-read_csv(\"../files/motral2012.csv\",\n                        locale = locale(encoding = \"latin1\")) %&gt;%\n  filter(sex==2) %&gt;% \n  mutate(casada=ifelse(e_con==5,1,0))\n\ndata.salarios &lt;- data.salarios %&gt;% \n  mutate(ly=ifelse(ing_x_hrs&gt;0,log(ing_x_hrs),NA)) \n\nreg_imput &lt;- lm(ly ~ anios_esc+casada+eda+n_hij+n_hij^2+busqueda,\n              data = data.salarios)\n\ndata.salarios &lt;- data.salarios %&gt;% \n  mutate(lyhat = predict(reg_imput, .)) %&gt;% \n  mutate(ly = ifelse(is.na(ly), lyhat, ly))\n\nAquí tomé en cuenta que hay personas con horas trabajadas positivas e ingreso cero. En ese caso puse un NA al log del salario. Luego, en la imputación, le asigné el valor ajustado a estas observaciones junto con todas las que tienen el log del salario faltante.\n[5 puntos] Use heckit de la librería sampleSelection para estimar por máxima verosimilitud un heckit para las horas trabajadas hrsocup. En la ecuación de selección (si la persona trabaja o no) incluya como variable explicativa el salario por hora (imputado para las mujeres que no trabajan), además de anios_esc, eda, n_hij, el cuadrado de n_hij, casada y busqueda (esta última es un indicador de si se buscó trabajo en la última semana). En la ecuación de horas, incluya los mismos regresores, excepto n_hij, su cuadrado y busqueda.\nLa función heckit permite estimar el modelo de Heckman por máxima verosimilitud de manera muy simple. Hay que especificar method=“ml” para que la estimación sea por máxima verosimilitud:\n\ndata.salarios &lt;- data.salarios %&gt;% \n  mutate(trabaja = ifelse(hrsocup&gt;0,1,0)) %&gt;% \n  mutate(trabaja = factor(trabaja,levels=c(0,1)))\n\nreg_heckit_mv &lt;- heckit(trabaja ~ anios_esc+casada+eda+ly+n_hij+n_hij^2+busqueda,\n                hrsocup ~ anios_esc+casada+eda+ly,\n                method=\"ml\",\n                data = data.salarios)\n\nsummary(reg_heckit_mv)\n\n--------------------------------------------\nTobit 2 model (sample selection model)\nMaximum Likelihood estimation\nNewton-Raphson maximisation, 3 iterations\nReturn code 8: successive function values within relative tolerance limit (reltol)\nLog-Likelihood: -7181.675 \n2625 observations (926 censored and 1699 observed)\n14 free parameters (df = 2611)\nProbit selection equation:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2.583614   0.448320  -5.763 9.24e-09 ***\nanios_esc    0.005346   0.020341   0.263    0.793    \ncasada      -0.213125   0.145135  -1.468    0.142    \neda         -0.003391   0.008137  -0.417    0.677    \nly          -0.004236   0.133344  -0.032    0.975    \nn_hij        0.023985   0.058900   0.407    0.684    \nbusqueda     2.406669   0.104595  23.009  &lt; 2e-16 ***\nOutcome equation:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 55.62469    2.17656  25.556  &lt; 2e-16 ***\nanios_esc    1.04819    0.09995  10.487  &lt; 2e-16 ***\ncasada      -3.58856    0.77967  -4.603 4.37e-06 ***\neda          0.11614    0.03902   2.977  0.00294 ** \nly          -9.83418    0.60389 -16.285  &lt; 2e-16 ***\n   Error terms:\n      Estimate Std. Error t value Pr(&gt;|t|)    \nsigma  14.8579     0.2591  57.350   &lt;2e-16 ***\nrho    -0.1606     0.1964  -0.818    0.414    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n--------------------------------------------\n\n\nPodemos reportar con modelsummary, aunque realmente lo hace muy mal:\n\nmodelsummary(list(reg_heckit_mv),\n             stars = c('***' = 0.01, '**' = 0.05, '*' = 0.1))\n\n \n\n  \n    \n    \n    tinytable_701ffeft592akf3uti3x\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n\n              \n                 \n                (1)\n              \n        \n        * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n        \n                \n                  (Intercept)\n                  -2.584***\n                \n                \n                             \n                  55.625***\n                \n                \n                             \n                  (0.448)  \n                \n                \n                             \n                  (2.177)  \n                \n                \n                  anios_esc  \n                  0.005    \n                \n                \n                             \n                  1.048*** \n                \n                \n                             \n                  (0.020)  \n                \n                \n                             \n                  (0.100)  \n                \n                \n                  casada     \n                  -0.213   \n                \n                \n                             \n                  -3.589***\n                \n                \n                             \n                  (0.145)  \n                \n                \n                             \n                  (0.780)  \n                \n                \n                  eda        \n                  -0.003   \n                \n                \n                             \n                  0.116*** \n                \n                \n                             \n                  (0.008)  \n                \n                \n                             \n                  (0.039)  \n                \n                \n                  ly         \n                  -0.004   \n                \n                \n                             \n                  -9.834***\n                \n                \n                             \n                  (0.133)  \n                \n                \n                             \n                  (0.604)  \n                \n                \n                  n_hij      \n                  0.024    \n                \n                \n                             \n                  (0.059)  \n                \n                \n                  busqueda   \n                  2.407*** \n                \n                \n                             \n                  (0.105)  \n                \n                \n                  sigma      \n                  14.858***\n                \n                \n                             \n                  (0.259)  \n                \n                \n                  rho        \n                  -0.161   \n                \n                \n                             \n                  (0.196)  \n                \n                \n                  Num.Obs.   \n                  2625     \n                \n                \n                  AIC        \n                  14391.4  \n                \n                \n                  BIC        \n                  14473.6  \n                \n                \n                  RMSE       \n                  14.84    \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\n[10 puntos] Estime ahora el heckit en dos pasos, a mano. Es decir, siga los siguientes pasos: i) estime un probit para la ecuación de selección y obtenga el índice \\(x_i'\\hat{\\beta}\\); ii) calcule el inverso de la razón de Mills \\(\\lambda_i(x_i'\\hat{\\beta})\\); y iii) estime por MCO la ecuación para las horas trabajadas con la submuestra que tiene horas trabajadas positivas, incluyendo como regresor el inverso de la razón de Mills estimado y el resto de los regresores. Compare los coeficientes y los errores estándar obtenidos en esta parte con los de la parte b. ¿Por qué son iguales o por qué difieren?\nEstimamos ahora el heckit a mano. Estimamos el probit y obtenemos el valor ajustado del IMR:\n\nreg_heckit_pe &lt;- glm(trabaja ~ anios_esc+casada+eda+ly+n_hij+n_hij^2+busqueda,\n                  family = binomial(link = \"probit\"),\n                  data = data.salarios)\n\ndata.salarios &lt;- data.salarios %&gt;% \n  mutate(index = predict(reg_heckit_pe, .)) %&gt;% \n  mutate(imr = dnorm(index)/pnorm(index))\n\n\nreg_heckit_se &lt;- lm(hrsocup ~ anios_esc+casada+eda+ly+imr,\n            data=filter(data.salarios,trabaja==1))\n\nsummary(reg_heckit_se)\n\n\nCall:\nlm(formula = hrsocup ~ anios_esc + casada + eda + ly + imr, data = filter(data.salarios, \n    trabaja == 1))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-46.172 -10.085   1.915   9.253  57.689 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 55.68676    2.17948  25.550  &lt; 2e-16 ***\nanios_esc    1.04814    0.10000  10.481  &lt; 2e-16 ***\ncasada      -3.56927    0.78057  -4.573 5.17e-06 ***\neda          0.11621    0.03904   2.977  0.00296 ** \nly          -9.82971    0.60406 -16.273  &lt; 2e-16 ***\nimr         -3.94669    3.62684  -1.088  0.27667    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.86 on 1693 degrees of freedom\nMultiple R-squared:  0.1563, Adjusted R-squared:  0.1539 \nF-statistic: 62.75 on 5 and 1693 DF,  p-value: &lt; 2.2e-16\n\n\nComparamos coeficientes (aquí stargazer lo hace mejor):\nstargazer(reg_heckit_mv, reg_heckit_se,\n          type=\"html\", \n          df=FALSE,\n          digits=4)\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nhrsocup\n\n\n\n\n\n\nHeckman\n\n\nOLS\n\n\n\n\n\n\nselection\n\n\n\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nanios_esc\n\n\n1.0482***\n\n\n1.0481***\n\n\n\n\n\n\n(0.0999)\n\n\n(0.1000)\n\n\n\n\n\n\n\n\n\n\n\n\ncasada\n\n\n-3.5886***\n\n\n-3.5693***\n\n\n\n\n\n\n(0.7797)\n\n\n(0.7806)\n\n\n\n\n\n\n\n\n\n\n\n\neda\n\n\n0.1161***\n\n\n0.1162***\n\n\n\n\n\n\n(0.0390)\n\n\n(0.0390)\n\n\n\n\n\n\n\n\n\n\n\n\nly\n\n\n-9.8342***\n\n\n-9.8297***\n\n\n\n\n\n\n(0.6039)\n\n\n(0.6041)\n\n\n\n\n\n\n\n\n\n\n\n\nimr\n\n\n\n\n-3.9467\n\n\n\n\n\n\n\n\n(3.6268)\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n55.6247***\n\n\n55.6868***\n\n\n\n\n\n\n(2.1766)\n\n\n(2.1795)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n2,625\n\n\n1,699\n\n\n\n\nR2\n\n\n0.1563\n\n\n\n\n\n\nAdjusted R2\n\n\n0.1539\n\n\n\n\n\n\nResidual Std. Error\n\n\n\n\n14.8614\n\n\n\n\nF Statistic\n\n\n\n\n62.7490***\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01\n\n\n\nLa magnitud de los coeficientes es práctiamente la misma entre el modelo estimado por máxima verosimilitud y con un procedimiento en dos etapas a mano. En este ejemplo las diferencias son sutiles, aunque recordemos que en general la estimación por MV es más eficiente si la verosimilitud está bien planteada."
  },
  {
    "objectID": "tareas/tarea-2-respuestas.html#footnotes",
    "href": "tareas/tarea-2-respuestas.html#footnotes",
    "title": "Respuestas a la tarea 2",
    "section": "Notas",
    "text": "Notas\n\n\nMroz, T. A. (1987). The sensitivity of an empirical model of married women’s hours of work to economic and statistical assumptions. Econometrica: Journal of the econometric society, 765-799.↩︎"
  },
  {
    "objectID": "tareas/tarea-3-respuestas.html#pregunta-1",
    "href": "tareas/tarea-3-respuestas.html#pregunta-1",
    "title": "Tarea 3",
    "section": "Pregunta 1",
    "text": "Pregunta 1\nEn este ejercicio usaremos los datos de Blackburn & Neumark (1992), que pueden accederse instalando el paquete wooldridge y luego llamando el objeto wooldridge::wage2. La pregunta de investigación de este trabajo es la misma que la del estudio de Card (1993) visto en clase, pero con otra estrategia de identificación. El propósito de este ejercicio es estimar los retornos a la educación.\n\n[3 puntos] Estime una regresión por MCO para explicar el log del salario (lwage) en función de la educación educ y los siguientes controles: experiencia (exper), tiempo en el empleo actual (tenure), edad (age), casado (married), número de hermanos (sibs) y urbano (urban). Reporte errores clásicos y errores robustos. ¿Qué problema encuentra en la estimación de esta relación? ¿El coeficiente sobre educ tiene una interpretación causal del efecto de la educación en el salario?\nEstimamos por MCO la relación entre salarios y educación, controlando por un conjunto de regresores:\n\nlibrary(wooldridge)\n\ndb &lt;- wooldridge::wage2\n\nregmco &lt;- lm(lwage ~ educ + exper + tenure + age + married + sibs + urban,\n            data = db)\n\nmodelsummary(models = list(regmco, regmco),\n          output = 'gt',\n          stars = c('***' = .01, '**' = .05, '*' = 0.1),\n          vcov = c(\"classical\", \"HC3\"),\n          fmt = 5,\n          coef_map = 'educ',\n          gof_map = \"nobs\")\n\n\n\n\n\n\n\n\n(1)\n(2)\n\n\n\n\neduc\n0.06730***\n0.06730***\n\n\n\n(0.00668)\n(0.00676)\n\n\nNum.Obs.\n935\n935\n\n\n\n* p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n\n\n\n\n\n\n\n\nHay una relación de 6.7% mayor ingreso por cada año de educación adicional. Sin embargo, esta no es una relación causal pues es muy probable que la educación no sea exógena en la ecuación de salarios. Esto puede deberse, por ejemplo, a una variable omitida de habilidad que afecta tanto al número de años de educación alcanzados como al desempeño en el mercado de trabajo.\n[3 puntos] Se propone usar como instrumento de los años de educación a la educación del padre. ¿Qué condiciones debe cumplir la variable propuesta para funcionar como instrumento válido?\nEl instrumento debe cumplir dos condiciones:\nExogeneidad: el instrumento no debe pertenecer a la ecuación de salarios. Es decir, la educación del padre no debe afectar el salario contemporáneo de forma directa.\nRelevancia: el instrumento debe estar correlacionado con la variable endógena. En este caso, la educación del padre debe estar correlacionada con el número de años de educación completados.\n[4 puntos] ¿Cómo juzga la propuesta de usar la variable antes descrita como instrumento?\nEste argumento fue usado por Blackburn y Neumark (1992) para estudiar los diferneciales en los salarios entre industrias en los Estados Unidos.\nNo hay una respuesta correcta o incorrecta. Quiero leer sus argumentos.\n[3 puntos] Estime la relación entre el logaritmo del salario y la educación usando la educación del padre, feduc, como instrumento. Emplee las mismas variables de control que en el modelo de MCO. Reporte errores clásicos y errores robustos.\n\nregvi &lt;- ivreg(lwage ~ educ + exper + tenure + age + married + sibs + urban  |\n                 feduc + exper + tenure + age + married + sibs + urban,\n               data=db)\n\nmodelsummary(models = list(regmco, regmco,regvi, regvi),\n          output = 'gt',\n          stars = c('***' = .01, '**' = .05, '*' = 0.1),\n          vcov = c(\"classical\", \"HC3\", \"classical\", \"HC3\"),\n          fmt = 5,\n          coef_map = 'educ',\n          gof_map = \"nobs\")\n\n\n\n\n\n\n\n\n(1)\n(2)\n(3)\n(4)\n\n\n\n\neduc\n0.06730***\n0.06730***\n0.13283***\n0.13283***\n\n\n\n(0.00668)\n(0.00676)\n(0.02413)\n(0.02411)\n\n\nNum.Obs.\n935\n935\n741\n741\n\n\n\n* p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n\n\n\n\n\n\n\n\n[4 puntos] Interprete la primera etapa en términos del coeficiente sobre el instrumento. Obtenga el estadístico \\(F\\) del instrumento excluido e interprete su magnitud.\nEn la primera etapa, cada año de educación del padre se asocia con 0.19 años de escolaridad acumulados. Este efecto estadísticamente significativo al 1%. El estadístico F es de una magnitud de 85, por encima de 10, la regla de dedo comúnmente empleada para juzgar la presencia de instrumentos débiles.\n\nregpe &lt;- lm(educ ~ feduc + exper + tenure + age + married + sibs + urban,\n            data=db)\n\n\nmodelsummary(models = list(regpe),\n          output = 'gt',\n          stars = c('***' = .01, '**' = .05, '*' = 0.1),\n          vcov = c(\"HC3\"),\n          fmt = 5,\n          coef_map = 'feduc',\n          gof_map = \"nobs\")\n\n\n\n\n\n\n\n\n(1)\n\n\n\n\nfeduc\n0.19398***\n\n\n\n(0.02093)\n\n\nNum.Obs.\n741\n\n\n\n* p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n\n\n\n\n\n\n\nlinearHypothesis(regpe, \"feduc=0\")\n\nLinear hypothesis test\n\nHypothesis:\nfeduc = 0\n\nModel 1: restricted model\nModel 2: educ ~ feduc + exper + tenure + age + married + sibs + urban\n\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1    734 2543.1                                  \n2    733 2278.2  1    264.91 85.235 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n[3 puntos] Interprete el coeficiente sobre la variable de educación en el modelo estructural. Compare la magnitud del efecto estimado con el resultado de MCO.\nEl coeficiente estimado sobre los años de educación indica que un año adicional de escolaridad incrementa en 13.3% el salario. Este efecto es casi el doble del estimado por MCO y estadísticamente significativo al 1%.\n\nmodelsummary(models = list(regmco, regvi),\n          output = 'gt',\n          stars = c('***' = .01, '**' = .05, '*' = 0.1),\n          vcov = c(\"HC3\", \"HC3\"),\n          fmt = 5,\n          coef_map = 'educ',\n          gof_map = \"nobs\")\n\n\n\n\n\n\n\n\n(1)\n(2)\n\n\n\n\neduc\n0.06730***\n0.13283***\n\n\n\n(0.00676)\n(0.02411)\n\n\nNum.Obs.\n935\n741\n\n\n\n* p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n\n\n\n\n\n\n\n\n[3 puntos] Realice ahora el siguiente procedimiento. Primero, estime la primera etapa usando una regresión por MCO. Obtenga los valores ajustados de educación y llámelos educ_hat. Luego, estime la segunda etapa empleando educ_hat como variable independiente, además del resto de variables de control. ¿Cómo cambian sus resultados en comparación con la parte d.?\nLa magnitud de los coeficientes estimados es la misma. Esto es lo que esperábamos pues sabemos que el estimador de MC2E puede entenderse como un procedimiento donde primero se estiman los valores ajustados de la variable endógena usando el instrumento y las variables de control y luego se usan estos valores ajustados en la ecuación estructural. En cambio, los errores estándar son algo distintos.\n\ndb &lt;- db %&gt;% \n  mutate(educ_hat = predict(regpe, .))\n\nreg2e &lt;- lm(lwage ~ educ_hat + exper + tenure + age + married + sibs + urban,\n            data=db)\n\n#Comparamos\n\nmodelsummary(models = list(regvi, reg2e),\n          output = 'gt',\n          stars = c('***' = .01, '**' = .05, '*' = 0.1),\n          vcov = c(\"HC3\", \"HC3\"),\n          fmt = 5,\n          coef_map = c('educ', 'educ_hat'),\n          gof_map = \"nobs\")\n\n\n\n\n\n\n\n\n(1)\n(2)\n\n\n\n\neduc\n0.13283***\n\n\n\n\n(0.02411)\n\n\n\neduc_hat\n\n0.13283***\n\n\n\n\n(0.02417)\n\n\nNum.Obs.\n741\n741\n\n\n\n* p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n\n\n\n\n\n\n\n\n[3 puntos] ¿A qué se deben las discrepancias que encuentra? ¿Cuál de las dos estrategias prefiere para estimar el modelo de variables instrumentales?\nLos coeficientes estimados son exactamente iguales, pero los errores estándar no. El problema es que nuestro procedimiento de MC2E a mano no toma en cuenta que en la ecuación estructural estamos usando valores ajustados de la variable endógena. Las funciones en la mayoría de los paquetes utilizados en econometría calculan los errores estándar de manera correcta. Preferimos usar las funciones previamente ya programadas cuando sea posible, aunque este ejercicio nos ayuda a reforzar la intuición del estimador de MC2E.\n[3 puntos] Reestime el modelo de variables instrumentales añadiendo un segundo instrumento, la educación de la madre, meduc, y reporte errores robustos (no es necesario usar gmm, puede seguir con ivreg, por lo que no estaría obteniendo el estimador de MGM óptimo). ¿Cómo cambian sus resultados para la ecuación estructural con respecto al caso exactamente identificado?\nEl efecto estimado es significativo al 1% y en magnitud se incrementa ligeramente hasta 0.137.\n\nregvi2 &lt;- ivreg(lwage ~ educ + exper + tenure + age + married + sibs + urban  |\n                 feduc + meduc +  exper + tenure + age + married + sibs + urban,\n               data=db)\n\n\nmodelsummary(models = list(regmco, regvi, regvi2),\n          output = 'gt',\n          stars = c('***' = .01, '**' = .05, '*' = 0.1),\n          vcov = c(\"HC3\", \"HC3\", \"HC3\"),\n          fmt = 5,\n          coef_map = c('educ'),\n          gof_map = \"nobs\")\n\n\n\n\n\n\n\n\n(1)\n(2)\n(3)\n\n\n\n\neduc\n0.06730***\n0.13283***\n0.13719***\n\n\n\n(0.00676)\n(0.02411)\n(0.02198)\n\n\nNum.Obs.\n935\n741\n722\n\n\n\n* p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n\n\n\n\n\n\n\n\n[3 puntos] Con el objeto que resulta de la estimación del modelo sobreidentificado, realice summary(OBJETO, vcov = sandwich, diagnostics = TRUE) para obtener las tres pruebas diagnóstico más usadas en variables instrumentales: prueba de instrumentos débiles, prueba de Hausman y prueba de Sargan. Interprete cada una de las pruebas.\n\nsummary(regvi2, vcov = sandwich, diagnostics = TRUE)\n\n\nCall:\nivreg(formula = lwage ~ educ + exper + tenure + age + married + \n    sibs + urban | feduc + meduc + exper + tenure + age + married + \n    sibs + urban, data = db)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.82265 -0.22364  0.01904  0.24623  1.29960 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.418980   0.268857  16.436  &lt; 2e-16 ***\neduc         0.137194   0.021691   6.325 4.47e-10 ***\nexper        0.035400   0.007952   4.452 9.88e-06 ***\ntenure       0.007832   0.003069   2.552   0.0109 *  \nage         -0.008260   0.007499  -1.102   0.2710    \nmarried      0.210486   0.048810   4.312 1.84e-05 ***\nsibs         0.003731   0.007297   0.511   0.6093    \nurban        0.170560   0.032662   5.222 2.32e-07 ***\n\nDiagnostic tests:\n                 df1 df2 statistic  p-value    \nWeak instruments   2 713    51.717  &lt; 2e-16 ***\nWu-Hausman         1 713    14.588 0.000145 ***\nSargan             1  NA     0.342 0.558820    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3949 on 714 degrees of freedom\nMultiple R-Squared: 0.1221,  Adjusted R-squared: 0.1134 \nWald test: 25.22 on 7 and 714 DF,  p-value: &lt; 2.2e-16 \n\n\nLa prueba de instrumentos débiles rechaza la \\(H_0\\) de que los instrumentos son débiles, por lo que tenemos primera etapa.\nLa prueba de Hausman rechaza al 10% que los estimadores de VI y de MCO sean iguales, por lo que se prefiere el de VI.\nLa prueba de Sargan no rechaza la \\(H_0\\) de que el modelo esté mal especificado.\n[4 puntos] Considere la primera etapa del modelo sobreidentificado. Compruebe que si realiza una prueba de significancia conjunta para los instrumentos obtiene la prueba de instrumentos débiles que se reporta en el resumen que obtuvo con summary.\nEn el apartado anterior, obtenemos un valor \\(p&lt;0.0000000000000002\\) para la prueba de instrumentos débiles. Noten que se especificó una matriz de varianzas robustas. Entonces, tenemos que usar la misma matriz en la prueba \\(F\\):\n\nregpe2 &lt;- lm(educ ~ feduc + meduc + exper + tenure + age + married + sibs + urban,\n            data=db)\n\nlinearHypothesis(regpe2, c(\"feduc=0\", \"meduc=0\"), white.adjust = \"hc0\")\n\nLinear hypothesis test\n\nHypothesis:\nfeduc = 0\nmeduc = 0\n\nModel 1: restricted model\nModel 2: educ ~ feduc + meduc + exper + tenure + age + married + sibs + \n    urban\n\nNote: Coefficient covariance matrix supplied.\n\n  Res.Df Df      F    Pr(&gt;F)    \n1    715                        \n2    713  2 51.717 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nObtenemos el mismo valor \\(p\\) y maginitud del estadístico \\(F\\).\n[4 puntos] Compruebe que si realiza el procedimiento de regresión auxiliar para la prueba de Hausman obtiene el mismo valor \\(p\\) que se reporta en el resumen que obtuvo con summary.\nDe la primera etapa, obtenemos los residuales:\n\ndb &lt;- db %&gt;% \n  filter(!is.na(lwage)) %&gt;% \n  mutate(vhat = educ - predict(regpe2, newdata=db))\n\nCorremos la regresión auxiliar con los residuales:\n\nregaux &lt;- lm(lwage ~ educ + exper + tenure + age + married + sibs + urban + vhat,\n               data=db)\n\n\nmodelsummary(models = list(regaux),\n          output = 'gt',\n          stars = c('***' = .01, '**' = .05, '*' = 0.1),\n          vcov = c(\"HC0\"),\n          statistic = c(\"p-val.={p.value}, e.e.={std.error}\"),\n          fmt = 6,\n          coef_map = c('vhat'),\n          gof_map = \"nobs\")\n\n\n\n\n\n\n\n\n(1)\n\n\n\n\nvhat\n-0.080976***\n\n\n\np-val.=0.000145, e.e.=0.021201\n\n\nNum.Obs.\n722\n\n\n\n* p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n\n\n\n\n\n\n\n\nEl valor \\(p\\) asociado al coeficiente de los residuales es idéntico al obtenido para la prueba de Hausman con summary."
  },
  {
    "objectID": "tareas/tarea-3-respuestas.html#pregunta-2",
    "href": "tareas/tarea-3-respuestas.html#pregunta-2",
    "title": "Tarea 3",
    "section": "Pregunta 2",
    "text": "Pregunta 2\nConsidere los datos comportamiento_wide.csv, que contienen información individual de niñas y niños, incluyendo su género, edad, raza e información de sus madres. Además, se incluye una medida auto reportada de autoestima (self) y una evaluación de comportamiento antisocial (anti). Se quiere conocer cómo influye la autoestima en el comportamiento antisocial. Para cada niño o niña hay tres observaciones en el tiempo. Se busca explicar el comportamiento antisocial en función de la autoestima y la condición de pobreza (pov):\n\\[anti_{it}=\\alpha_i+\\beta_1 self_{it}+\\beta_2 pov_{it}+\\varepsilon_{it}\\]\n\n[2 puntos] La base se encuentra en formato wide. Ponga la base en formato long, donde haya una columna para cada variable y donde las filas representen a un individuo en un periodo.\nHay muchas formas de hacer esto. Podemos usar las funciones pivot_longer y pivot_wider, por ejemplo.\n\ndata.comp &lt;-read_csv(\"../files/comportamiento_wide.csv\",\n                      locale = locale(encoding = \"latin1\")) %&gt;%\n  pivot_longer(c(anti90:anti94,self90:self94,pov90:pov94),\n               names_to = c(\"measure\", \"year\"),\n               names_pattern = \"(.*)(..)\")  %&gt;%\n  pivot_wider(names_from = measure,\n              values_from = value)\n\ncolnames(data.comp)\n\n [1] \"id\"       \"momage\"   \"gender\"   \"childage\" \"hispanic\" \"black\"   \n [7] \"momwork\"  \"married\"  \"year\"     \"anti\"     \"self\"     \"pov\"     \n\n\n[2 puntos] Estime la ecuación de comportamiento antisocial empleando MCO pooled. ¿Cuáles son los supuestos que se deben cumplir para que \\(\\hat{\\beta}_1^{MCO}\\) sea consistente?\n\nm.mco &lt;- plm( anti ~ self + pov,\n                      data=data.comp,\n                      model=\"pooling\",\n                      index = c(\"id\", \"year\"))\n\n\n\nmodelsummary(models = list(\"MCO\"=m.mco),\n          output = 'gt',\n          stars = c('***' = .01, '**' = .05, '*' = 0.1),\n          vcov = c(\"iid\"),\n          fmt = 5,\n          coef_map = c('self', 'pov'),\n          gof_map = \"nobs\")\n\n\n\n\n\n\n\n\nMCO\n\n\n\n\nself\n-0.06510***\n\n\n\n(0.01108)\n\n\npov\n0.51581***\n\n\n\n(0.07873)\n\n\nNum.Obs.\n1743\n\n\n\n* p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n\n\n\n\n\n\n\n\nLa variable self tiene un efecto negativo y estadísticamente significativo sobre anti. La variable pov tiene un efecto positivo y estadísticamente significativo. El estimador de MCO será consistente solo si las variables self y pov no están correlacionadas con el error. Además, para estimar este modelo, asumimos que la heterogeneidad no observada \\(\\alpha_i\\) puede escribirse simplemente como \\(\\alpha\\). Otra forma de pensar sobre este modelo es si el mismo modelo es válido para todos los periodos como para asumir una ordenada al origen y una pendiente común.\nEl modelo pooled ignora la naturaleza en panel de los datos. Sin embargo, como tenemos a los mismos individuos en varios puntos del tiempo, los errores están agrupados, así que se deben de estimar errores con esta estructura. En este caso, al tomar en cuenta esta correlación entre individuos, los errores estándar son más grandes, pero los resultados siguen siendo significativos. En muchos casos, no tomar en cuenta la estructura agrupada de los errores puede llevar a rechazar hipótesis nulas que son ciertas.\n\nmodelsummary(models = list(\"MCO\"=m.mco,\n                           \"MCO, errores agrupados\"=m.mco),\n          output = 'gt',\n          stars = c('***' = .01, '**' = .05, '*' = 0.1),\n          vcov=list(NULL, clubSandwich::vcovCR(m.mco, type='CR1', cluster=data.comp$id)),\n          fmt = 5,\n          coef_map = c('self', 'pov'),\n          gof_map = \"nobs\")\n\n\n\n\n\n\n\n\nMCO\nMCO, errores agrupados\n\n\n\n\nself\n-0.06510***\n-0.06510***\n\n\n\n(0.01108)\n(0.01369)\n\n\npov\n0.51581***\n0.51581***\n\n\n\n(0.07873)\n(0.10496)\n\n\nNum.Obs.\n1743\n1743\n\n\n\n* p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n\n\n\n\n\n\n\n\n[3 puntos] Estime la ecuación de comportamiento antisocial empleando el estimador within. ¿Cuáles son los supuestos que se deben cumplir para que \\(\\hat{\\beta}_1^{FE}\\) sea consistente?\nSi asumimos que la heterogeneidad no observada y el error están potencialmente correlacionados, entonces podemos usar un estimador de efectos fijos para deshacernos de la heterogeneidad no observada y estimar consistentemente los parámetros sobre self y pov.\n\nm.fe &lt;- plm( anti ~ self + pov,\n             data=data.comp,\n             model=\"within\",\n             index = c(\"id\", \"year\"))\n\nmodelsummary(models = list(\"MCO\"=m.mco,\n                           \"MCO, errores agrupados\"=m.mco,\n                           \"Efectos fijos\"=m.fe),\n          output = 'gt',\n          stars = c('***' = .01, '**' = .05, '*' = 0.1),\n          vcov=list(NULL, clubSandwich::vcovCR(m.mco, type='CR1', cluster=data.comp$id), clubSandwich::vcovCR(m.fe, type='CR1', cluster=data.comp$id)),\n          fmt = 5,\n          coef_map = c('self', 'pov'),\n          gof_map = \"nobs\")\n\n\n\n\n\n\n\n\nMCO\nMCO, errores agrupados\nEfectos fijos\n\n\n\n\nself\n-0.06510***\n-0.06510***\n-0.05150***\n\n\n\n(0.01108)\n(0.01369)\n(0.01131)\n\n\npov\n0.51581***\n0.51581***\n0.10490\n\n\n\n(0.07873)\n(0.10496)\n(0.09922)\n\n\nNum.Obs.\n1743\n1743\n1743\n\n\n\n* p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n\n\n\n\n\n\n\n\n[3 puntos] Estime la ecuación de comportamiento antisocial empleando efectos aleatorios. ¿Cuáles son los supuestos que se deben cumplir para que \\(\\hat{\\beta}_1^{RE}\\) sea consistente?\nSi estamos dispuestos a asumir que la heterogeneidad no observada y el error son independientes, podemos emplear el estimador de efectos aleatorios. MCO pooled también es consistente pero no es eficiente.\n\nm.re &lt;- plm( anti ~ self + pov,\n             data=data.comp,\n             model=\"random\",\n             index = c(\"id\", \"year\"))\n\n\nmodelsummary(models = list(\"MCO\"=m.mco,\n                           \"MCO, errores agrupados\"=m.mco,\n                           \"Efectos fijos\"=m.fe,\n                           \"Efectos aleatorios\"=m.re),\n          output = 'gt',\n          stars = c('***' = .01, '**' = .05, '*' = 0.1),\n          vcov=list(NULL,\n                    clubSandwich::vcovCR(m.mco, type='CR1',cluster=data.comp$id),\n                    clubSandwich::vcovCR(m.fe, type='CR1', cluster=data.comp$id),\n                    clubSandwich::vcovCR(m.re, type='CR1', cluster=data.comp$id)),\n          fmt = 5,\n          coef_map = c('self', 'pov'),\n          gof_map = \"nobs\")\n\n\n\n\n\n\n\n\nMCO\nMCO, errores agrupados\nEfectos fijos\nEfectos aleatorios\n\n\n\n\nself\n-0.06510***\n-0.06510***\n-0.05150***\n-0.05673***\n\n\n\n(0.01108)\n(0.01369)\n(0.01131)\n(0.01022)\n\n\npov\n0.51581***\n0.51581***\n0.10490\n0.29241***\n\n\n\n(0.07873)\n(0.10496)\n(0.09922)\n(0.08196)\n\n\nNum.Obs.\n1743\n1743\n1743\n1743\n\n\n\n* p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n\n\n\n\n\n\n\n\n[3 puntos] Se desea incorporar en el análisis el género (gender) y una variable dicotómica para los hispanos (hispanic). Indique qué modelo usaría y estime dicho modelo.\nNo es posible estimar los coeficientes sobre variables que no varían en el tiempo usando efectos fijos, por lo que este modelo queda descartado. Podríamos usar MCO pooled, que impone supuestos muy fuertes. La otra alternativa es un modelo de efectos aleatorios, que asume que la heterogeneidad no observada y el error no están correlacionados.\n\nm.sex &lt;- plm( anti ~ self + pov + gender,\n              data=data.comp,\n              model=\"random\",\n              index = c(\"id\", \"year\"))\n\n\nmodelsummary(models = list(\"MCO\"=m.mco,\n                           \"MCO, errores agrupados\"=m.mco,\n                           \"Efectos fijos\"=m.fe,\n                           \"Efectos aleatorios\"=m.re,\n                           \"Efectos aleatorios con género\"=m.sex),\n          output = 'gt',\n          stars = c('***' = .01, '**' = .05, '*' = 0.1),\n          vcov=list(NULL,\n                    clubSandwich::vcovCR(m.mco, type='CR1',cluster=data.comp$id),\n                    clubSandwich::vcovCR(m.fe, type='CR1', cluster=data.comp$id),\n                    clubSandwich::vcovCR(m.re, type='CR1', cluster=data.comp$id),\n                    clubSandwich::vcovCR(m.sex, type='CR1', cluster=data.comp$id)),\n          fmt = 5,\n          coef_map = c('self', 'pov', 'gender'),\n          gof_map = \"nobs\")\n\n\n\n\n\n\n\n\nMCO\nMCO, errores agrupados\nEfectos fijos\nEfectos aleatorios\nEfectos aleatorios con género\n\n\n\n\nself\n-0.06510***\n-0.06510***\n-0.05150***\n-0.05673***\n-0.05856***\n\n\n\n(0.01108)\n(0.01369)\n(0.01131)\n(0.01022)\n(0.01022)\n\n\npov\n0.51581***\n0.51581***\n0.10490\n0.29241***\n0.30500***\n\n\n\n(0.07873)\n(0.10496)\n(0.09922)\n(0.08196)\n(0.08146)\n\n\ngender\n\n\n\n\n-0.48047***\n\n\n\n\n\n\n\n(0.10710)\n\n\nNum.Obs.\n1743\n1743\n1743\n1743\n1743\n\n\n\n* p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n\n\n\n\n\n\n\n\n[2 puntos] Regrese al modelo que incluye solo la autoestima y el estado de pobreza como covariables. Realice una prueba de Hausman para determinar si se prefiere un modelo de efectos fijos o uno de efectos aleatorios.\nLa implementación de la prueba de Hausman indica que se rechaza la H0 de que los coeficientes estimados son iguales (y que el modelo de efectos aleatorios es el adecuado). Hay evidencia de que se prefiere un modelo de efectos fijos, aunque tendremos que vivir con el hecho de no poder estimar el coeficiente asociado a las variables que no varían en el tiempo en este caso.\n\nphtest(m.fe, m.re)\n\n\n Hausman Test\n\ndata:  anti ~ self + pov\nchisq = 13.578, df = 2, p-value = 0.001126\nalternative hypothesis: one model is inconsistent"
  },
  {
    "objectID": "tareas/tarea-3-respuestas.html#pregunta-3",
    "href": "tareas/tarea-3-respuestas.html#pregunta-3",
    "title": "Tarea 3",
    "section": "Pregunta 3",
    "text": "Pregunta 3\nRetome los datos de la pregunta 2 y el modelo del comportamiento antisocial en función de la autoestima y la pobreza. En esta pregunta mostraremos la equivalencia del estimador within con otros estimadores.\n\n[3 puntos] Compruebe que el estimador de efectos fijos es equivalente a MCO con dummies de individuos.\nComprobamos:\n\nm.fe &lt;- plm( anti ~ self + pov,\n             data=data.comp,\n             model=\"within\",\n             index = c(\"id\", \"year\"))\n\nm.dummy &lt;- lm(anti ~ self + pov + factor(id),\n              data=data.comp)\n\n\nmodelsummary(models = list(\"MCO\"=m.fe,\n                           \"MCO con dummies\"=m.dummy),\n          output = 'gt',\n          stars = c('***' = .01, '**' = .05, '*' = 0.1),\n          vcov=list(clubSandwich::vcovCR(m.fe, type='CR1',cluster=data.comp$id),\n                    clubSandwich::vcovCR(m.dummy, type='CR1',cluster=data.comp$id)),\n          fmt = 5,\n          coef_map = c('self', 'pov'),\n          gof_map = \"nobs\")\n\n\n\n\n\n\n\n\nMCO\nMCO con dummies\n\n\n\n\nself\n-0.05150***\n-0.05150***\n\n\n\n(0.01131)\n(0.01131)\n\n\npov\n0.10490\n0.10490\n\n\n\n(0.09922)\n(0.09922)\n\n\nNum.Obs.\n1743\n1743\n\n\n\n* p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n\n\n\n\n\n\n\n\n[2 puntos] Compruebe que en un modelo de efectos fijos las características que no varían en el tiempo no pueden ser identificadas. Añada la variable black para comprobarlo.\nComprobamos que la variable simplemente es omitida del análisis:\n\nsummary(plm( anti ~ self + pov + black,\n             data=data.comp,\n             model=\"within\",\n             index = c(\"id\", \"year\")))\n\nOneway (individual) effect Within Model\n\nCall:\nplm(formula = anti ~ self + pov + black, data = data.comp, model = \"within\", \n    index = c(\"id\", \"year\"))\n\nBalanced Panel: n = 581, T = 3, N = 1743\n\nResiduals:\n      Min.    1st Qu.     Median    3rd Qu.       Max. \n-3.7868224 -0.4706542 -0.0012721  0.4534891  3.2646729 \n\nCoefficients:\n      Estimate Std. Error t-value  Pr(&gt;|t|)    \nself -0.051495   0.010530 -4.8902 1.149e-06 ***\npov   0.104899   0.093880  1.1174    0.2641    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    1190.7\nResidual Sum of Squares: 1165.4\nR-Squared:      0.021182\nAdj. R-Squared: -0.46991\nF-statistic: 12.5514 on 2 and 1160 DF, p-value: 4.0471e-06\n\n\n[5 puntos] Compruebe que el estimador de efectos fijos es equivalente a MCO sobre el modelo en diferencias con respecto a la media. Para esto, conserve dos periodos consecutivos de datos y solo observaciones que tengan datos para las variables dependientes e independientes en los dos años que elija. Luego estime por MCO el modelo con variables transformadas.\nNos quedamos con un subconjunto de datos:\n\ndata.comp.sub &lt;- data.comp %&gt;% \n  dplyr::select(id, year, anti, self, pov) %&gt;% \n  filter(year==90 | year==92)\n\n#Nos quedamos con los que no son NA\ndata.comp.sub &lt;- data.comp.sub[complete.cases(data.comp.sub), ]\n\nCreamos las variables como diferencias respecto a la media y estimamos el modelo within y el modelo de MCO en las variables transformadas:\n\ndata.comp.sub &lt;- data.comp.sub %&gt;%\n  group_by(id) %&gt;% \n  mutate(m.anti = mean(anti),\n         m.self = mean(self),\n         m.pov = mean(pov)) %&gt;% \n  mutate(dm.anti = anti - m.anti,\n         dm.self = self - m.self,\n         dm.pov = pov - m.pov)\n\nm.fe.sub &lt;- plm( anti ~ self + pov,\n                 data=data.comp.sub,\n                 model=\"within\",\n                 index = c(\"id\", \"year\"))\n\n\nm.demean &lt;- lm(dm.anti ~ dm.self + dm.pov,\n               data.comp.sub)\n\n\nmodelsummary(models = list(\"Efectos fijos\"=m.fe.sub,\n                           \"MCO demean\"=m.demean),\n          output = 'gt',\n          stars = c('***' = .01, '**' = .05, '*' = 0.1),\n          vcov=list(clubSandwich::vcovCR(m.fe.sub, type='CR1',cluster=data.comp.sub$id),\n                    clubSandwich::vcovCR(m.demean, type='CR1',cluster=data.comp.sub$id)),\n          fmt = 5,\n          coef_map = c('self', 'pov', 'dm.self', 'dm.pov'),\n          gof_map = \"nobs\")\n\n\n\n\n\n\n\n\nEfectos fijos\nMCO demean\n\n\n\n\nself\n-0.03835**\n\n\n\n\n(0.01487)\n\n\n\npov\n0.19469\n\n\n\n\n(0.14506)\n\n\n\ndm.self\n\n-0.03835**\n\n\n\n\n(0.01487)\n\n\ndm.pov\n\n0.19469\n\n\n\n\n(0.14506)\n\n\nNum.Obs.\n1162\n1162\n\n\n\n* p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n\n\n\n\n\n\n\n\n[5 puntos] Compruebe que el estimador de efectos fijos es equivalente a MCO sobre el modelo en primeras diferencias. Parta de la muestra con dos años de la parte d. para estimar por MCO el modelo con variables transformadas.\nUsando el mismo subconjunto, calculamos ahora las primeras diferencias y estimamos:\n\ndata.comp.sub &lt;- data.comp.sub %&gt;%\n  group_by(id) %&gt;% \n  mutate(d.anti = anti-dplyr::lag(anti, order_by = year),\n         d.self = self-dplyr::lag(self, order_by = year),\n         d.pov = pov-dplyr::lag(pov, order_by = year)) %&gt;% \n  ungroup()\n\n\nm.difs &lt;- lm(d.anti ~ -1 + d.self + d.pov,\n             data=data.comp.sub)\n\n\n\nmodelsummary(models = list(\"Efectos fijos\"=m.fe.sub,\n                           \"MCO demean\"=m.demean,\n                           \"MCO primeras diferencias\"=m.difs),\n          output = 'gt',\n          stars = c('***' = .01, '**' = .05, '*' = 0.1),\n          vcov=list(clubSandwich::vcovCR(m.fe.sub, type='CR1',cluster=data.comp.sub$id),\n                    clubSandwich::vcovCR(m.demean, type='CR1',cluster=data.comp.sub$id),\n                    clubSandwich::vcovCR(m.difs, type='CR1',cluster=data.comp.sub$id)),\n          fmt = 5,\n          coef_map = c('self', 'pov', 'dm.self', 'dm.pov', 'd.self', 'd.pov'),\n          gof_map = \"nobs\")\n\n\n\n\n\n\n\n\nEfectos fijos\nMCO demean\nMCO primeras diferencias\n\n\n\n\nself\n-0.03835**\n\n\n\n\n\n(0.01487)\n\n\n\n\npov\n0.19469\n\n\n\n\n\n(0.14506)\n\n\n\n\ndm.self\n\n-0.03835**\n\n\n\n\n\n(0.01487)\n\n\n\ndm.pov\n\n0.19469\n\n\n\n\n\n(0.14506)\n\n\n\nd.self\n\n\n-0.03835**\n\n\n\n\n\n(0.01487)\n\n\nd.pov\n\n\n0.19469\n\n\n\n\n\n(0.14506)\n\n\nNum.Obs.\n1162\n1162\n581\n\n\n\n* p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01"
  },
  {
    "objectID": "tareas/tarea-3-respuestas.html#pregunta-4",
    "href": "tareas/tarea-3-respuestas.html#pregunta-4",
    "title": "Tarea 3",
    "section": "Pregunta 4",
    "text": "Pregunta 4\nConsidere los datos mlbook1.csv con información sobre 2287 estudiantes en 131 escuelas. Nos interesa la relación entre una medida de aptitud verbal, (iq_vert) y el resultado de un examen de inglés (langpost). Las variables schoolnr y pupilnr identifican a las escuelas y los estudiantes, respectivamente. El modelo a estimar es el siguiente:\n\\[langpost_{i}=\\alpha+\\beta iqvert_{i}+BX_{i}+\\varepsilon_{i}\\]\ndonde \\(i\\) indexa y \\(X_i\\) son tres características usadas como control: el sexo, sex, si el estudiante es de una población minoritaria, minority y el número de años repetidos, repeatgr.\n\n[3 puntos] ¿Por qué es posible que estemos frente a una situación de errores agrupados?\nLos datos están agrupados a nivel escuela. Los estudiantes en una misma escuela comparten características observadas y no observadas que hacen altamente probable que los factores no observables estén correlacionados entre los individuos, rompiendo el supuesto de independencia.\n[2 puntos] Estime la ecuación de calificación usando MCO ignorando la agrupación de datos. ¿Qué concluye respecto a la relación entre la aptitud verbal y la prueba de inglés?\nSe concluye que una hora más en la prueba de aptitud incrementa en 2.49 puntos la calificación del examen. El error estándar es 0.072.\n\ndata.examen&lt;-read_csv(\"../files/mlbook1.csv\",\n                      locale = locale(encoding = \"latin1\")) \n\nm.mco &lt;- lm(langpost ~ iq_verb + sex + minority + repeatgr,\n            data=data.examen)\n\n\n\nmodelsummary(models = list(\"MCO\"=m.mco),\n          output = 'gt',\n          stars = c('***' = .01, '**' = .05, '*' = 0.1),\n          vcov=list(NULL),\n          fmt = 5,\n          coef_map = c('iq_verb', 'sex', 'minority', 'repeatgr'),\n          gof_map = \"nobs\")\n\n\n\n\n\n\n\n\nMCO\n\n\n\n\niq_verb\n2.48635***\n\n\n\n(0.07233)\n\n\nsex\n2.42228***\n\n\n\n(0.28871)\n\n\nminority\n-0.03701\n\n\n\n(0.62762)\n\n\nrepeatgr\n-4.40860***\n\n\n\n(0.43222)\n\n\nNum.Obs.\n2287\n\n\n\n* p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n\n\n\n\n\n\n\n\n[3 puntos] Estime ahora los errores robustos a heteroscedasticidad del tipo HC1. ¿Qué cambia y por qué en la interpretación de la relación entre la prueba de aptitud y el examen?\nEl coeficiente estimado es el mismo. La fórmula empleada para calcular la varianza es una en forma de sándwich, que toma en cuenta la posible heterocedasticidad. El error estándar es apromximadamente 5% más grande, 0.076.\n\nmodelsummary(models = list(\"MCO\"=m.mco,\n                           \"MCO, errores robustos\"=m.mco),\n          output = 'gt',\n          stars = c('***' = .01, '**' = .05, '*' = 0.1),\n          vcov=list(NULL,\n                    \"HC1\"),\n          fmt = 5,\n          coef_map = c('iq_verb', 'sex', 'minority', 'repeatgr'),\n          gof_map = \"nobs\")\n\n\n\n\n\n\n\n\nMCO\nMCO, errores robustos\n\n\n\n\niq_verb\n2.48635***\n2.48635***\n\n\n\n(0.07233)\n(0.07587)\n\n\nsex\n2.42228***\n2.42228***\n\n\n\n(0.28871)\n(0.28852)\n\n\nminority\n-0.03701\n-0.03701\n\n\n\n(0.62762)\n(0.61245)\n\n\nrepeatgr\n-4.40860***\n-4.40860***\n\n\n\n(0.43222)\n(0.44862)\n\n\nNum.Obs.\n2287\n2287\n\n\n\n* p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n\n\n\n\n\n\n\n\n[2 puntos] Estime la ecuación de calificación usando MCO y efectos fijos de escuela. ¿Qué resuelve este procedimiento?\nAl incluir efectos fijos a nivel escuela controlamos por características no observadas a nivel escuela. Estas diferencias se incorporan en el modelo como desplazamientos de la ordenada al origen. Este procedimiento no tiene nada que ver con la agrupación de errores.\n\nm.mco.ef &lt;- lm(langpost ~ iq_verb + sex + minority + repeatgr + factor(schoolnr),\n               data=data.examen)\n\nmodelsummary(models = list(\"MCO\"=m.mco,\n                           \"MCO, errores robustos\"=m.mco,\n                           \"Efectos fijos\"=m.mco.ef),\n          output = 'gt',\n          stars = c('***' = .01, '**' = .05, '*' = 0.1),\n          vcov=list(NULL,\n                    \"HC1\",\n                    \"HC1\"),\n          fmt = 5,\n          coef_map = c('iq_verb', 'sex', 'minority', 'repeatgr'),\n          gof_map = \"nobs\")\n\n\n\n\n\n\n\n\nMCO\nMCO, errores robustos\nEfectos fijos\n\n\n\n\niq_verb\n2.48635***\n2.48635***\n2.25997***\n\n\n\n(0.07233)\n(0.07587)\n(0.07310)\n\n\nsex\n2.42228***\n2.42228***\n2.41900***\n\n\n\n(0.28871)\n(0.28852)\n(0.26990)\n\n\nminority\n-0.03701\n-0.03701\n0.23262\n\n\n\n(0.62762)\n(0.61245)\n(0.70803)\n\n\nrepeatgr\n-4.40860***\n-4.40860***\n-4.43503***\n\n\n\n(0.43222)\n(0.44862)\n(0.42731)\n\n\nNum.Obs.\n2287\n2287\n2287\n\n\n\n* p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n\n\n\n\n\n\n\n\n[5 puntos] Estime la ecuación de calificación usando MCO y con errores agrupados a nivel escuela (sin efectos fijos de escuela). ¿Qué resuelve este procedimiento?\nAl estimar los errores agrupados y robustos a heterocedasticidad se toma en cuenta la correlación que existe en los errores dentro de cada escuela. Los errores agrupados estimados con la opción cluster asumen correlación de errores dentro del grupo, pero no entre grupos. Con respecto a las partes b. y c., el error estándar asociado al tiempo dedicado a la tarea es aproximadamente 20% mayor. Este es un ejemplo típico en el que los errores agrupados se inflan con respecto a los errores de MCO clásicos y los errores robustos.\nNota: es posible que los errores agrupados sean menores que los errores de MCO. Para ver eso, considere un modelo simple con datos agrupados de la forma siguiente: \\[y_{ig=\\alpha+\\beta x_{ig}+u_{ig}}\\] donde \\(x_{ig}\\) es un regresor escalar.\nSe asume que el tamaño promedio de los grupos es \\(\\bar{N}_g\\). Moulton (1990) muestra que el error estándar de MCO esta sesgado hacia abajo por una cantidad igual a la raíz de \\(\\tau \\approx 1 +\\rho_x \\rho_u (\\bar{N}_g-1)\\), donde \\(\\rho_x\\) es la correlación dentro de los grupos de \\(x\\) y \\(\\rho_u\\) es la correlación dentro de los grupos de los errores. Esto implica que para obtener el error correcto que toma en cuenta la agrupación hay que multiplicar el error de MCO por la raíz de \\(\\tau\\). Sin embargo, note que dependiendo del signo y la magnitud de \\(\\rho_x\\) y \\(\\rho_u\\), la raíz de \\(\\tau\\) puede llegar a ser menor que 1 y, por tanto, el error agrupado puede llegar a ser menor que el de MCO. \\(\\tau\\) se conoce como el factor de Moulton y puede ser extendido para un modelo más complicado. La intuición funciona de manera similar para un modelo más complicado: todo depende de las correlaciones entre grupos de los regresores y la correlación de los errores.\n\nmodelsummary(models = list(\"MCO\"=m.mco,\n                           \"MCO, errores robustos\"=m.mco,\n                           \"Efectos fijos\"=m.mco.ef,\n                           \"MCO, errores agrupados\"=m.mco),\n          output = 'gt',\n          stars = c('***' = .01, '**' = .05, '*' = 0.1),\n          vcov=list(NULL,\n                    \"HC1\",\n                    \"HC1\",\n                    clubSandwich::vcovCR(m.mco, type='CR1',cluster=data.examen$schoolnr)),\n          fmt = 5,\n          coef_map = c('iq_verb', 'sex', 'minority', 'repeatgr'),\n          gof_map = \"nobs\")\n\n\n\n\n\n\n\n\nMCO\nMCO, errores robustos\nEfectos fijos\nMCO, errores agrupados\n\n\n\n\niq_verb\n2.48635***\n2.48635***\n2.25997***\n2.48635***\n\n\n\n(0.07233)\n(0.07587)\n(0.07310)\n(0.08986)\n\n\nsex\n2.42228***\n2.42228***\n2.41900***\n2.42228***\n\n\n\n(0.28871)\n(0.28852)\n(0.26990)\n(0.28436)\n\n\nminority\n-0.03701\n-0.03701\n0.23262\n-0.03701\n\n\n\n(0.62762)\n(0.61245)\n(0.70803)\n(0.85396)\n\n\nrepeatgr\n-4.40860***\n-4.40860***\n-4.43503***\n-4.40860***\n\n\n\n(0.43222)\n(0.44862)\n(0.42731)\n(0.39547)\n\n\nNum.Obs.\n2287\n2287\n2287\n2287\n\n\n\n* p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n\n\n\n\n\n\n\n\n[5 puntos] Estime la ecuación de calificación usando MCO, efectos fijos de escuela y con errores agrupados a nivel escuela. ¿Qué resuelve este procedimiento?\nAl controlar por características no observadas de las escuelas empleando efectos fijos por escuela y además estimando los errores que toman en cuenta la estructura agrupada de los errores obtenemos un coeficiente estimado de 2.26, pero con un error estándar mayor, 0.08879.\n\nmodelsummary(models = list(\"MCO\"=m.mco,\n                           \"MCO, errores robustos\"=m.mco,\n                           \"Efectos fijos\"=m.mco.ef,\n                           \"MCO, errores agrupados\"=m.mco,\n                           \"Efectos fijos, errores agrupados\"=m.mco.ef),\n          output = 'gt',\n          stars = c('***' = .01, '**' = .05, '*' = 0.1),\n          vcov=list(NULL,\n                    \"HC1\",\n                    \"HC1\",\n                    clubSandwich::vcovCR(m.mco, type='CR1',cluster=data.examen$schoolnr),\n                    clubSandwich::vcovCR(m.mco.ef, type='CR1',cluster=data.examen$schoolnr)),\n          fmt = 5,\n          coef_map = c('iq_verb', 'sex', 'minority', 'repeatgr'),\n          gof_map = \"nobs\")\n\n\n\n\n\n\n\n\nMCO\nMCO, errores robustos\nEfectos fijos\nMCO, errores agrupados\nEfectos fijos, errores agrupados\n\n\n\n\niq_verb\n2.48635***\n2.48635***\n2.25997***\n2.48635***\n2.25997***\n\n\n\n(0.07233)\n(0.07587)\n(0.07310)\n(0.08986)\n(0.08879)\n\n\nsex\n2.42228***\n2.42228***\n2.41900***\n2.42228***\n2.41900***\n\n\n\n(0.28871)\n(0.28852)\n(0.26990)\n(0.28436)\n(0.27519)\n\n\nminority\n-0.03701\n-0.03701\n0.23262\n-0.03701\n0.23262\n\n\n\n(0.62762)\n(0.61245)\n(0.70803)\n(0.85396)\n(0.69585)\n\n\nrepeatgr\n-4.40860***\n-4.40860***\n-4.43503***\n-4.40860***\n-4.43503***\n\n\n\n(0.43222)\n(0.44862)\n(0.42731)\n(0.39547)\n(0.39617)\n\n\nNum.Obs.\n2287\n2287\n2287\n2287\n2287\n\n\n\n* p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01"
  },
  {
    "objectID": "tareas/tarea-4-respuestas.html",
    "href": "tareas/tarea-4-respuestas.html",
    "title": "Respuestas a la tarea 4",
    "section": "",
    "text": "Considere los datos en el archivo capital_trabajo.csv. Con una función de producción Cobb-Douglas las participaciones del capital y el trabajo en el valor de la producción se pueden estimar usando una regresión lineal. En algunas aplicaciones es de interés conocer el cociente de las participaciones estimadas.\n\n[10 puntos] Usando 500 repeticiones bootstrap estime el error estándar del cociente capital-trabajo. Para ello realice el siguiente procedimiento:\n\nGenere una matriz vacía de 500 filas para coleccionar sus relaciones estimadas.\nEn cada una de las repeticiones obtenga una muestra con remplazo a partir de la muestra original.\nEstime por MCO los coeficientes sobre el log del capital y el log del trabajo. La variable dependiente es el log del valor de la producción. Calcule el cociente de los coeficientes estimados. Guarde el cociente en la matriz.\nRepita ii. y iii. 500 veces.\nCalcule la desviación estándar de los cocientes estimados.\n\nEn cada repetición bootstrap debemos estimar el siguiente modelo y obtener el ratio de los coeficientes:\n\ndata.kl &lt;- read_csv(\"../files/capital_trabajo.csv\") \n\nsummary(m1 &lt;- lm(lvalor ~ lcapital + ltrabajo, data=data.kl))\n\n\nCall:\nlm(formula = lvalor ~ lcapital + ltrabajo, data = data.kl)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.53523 -0.25678  0.03835  0.26003  0.49631 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 10.56478    0.05303  199.24   &lt;2e-16 ***\nlcapital     0.38502    0.03072   12.53   &lt;2e-16 ***\nltrabajo     0.66108    0.02813   23.50   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2999 on 97 degrees of freedom\nMultiple R-squared:  0.8768, Adjusted R-squared:  0.8742 \nF-statistic: 345.1 on 2 and 97 DF,  p-value: &lt; 2.2e-16\n\n\nFijamos una semilla y los parámetros de la rutina:\n\nset.seed(120)\nB=500\nobs &lt;- nrow(data.kl)\nbeta &lt;- data.frame(beta=matrix(ncol = 1, nrow = B))\n\nRealizamos la regresión y el cálculo del cociente en cada una de las \\(b\\) repeticiones:\n\nfor (i in 1:B)\n{\n  data.b &lt;-data.kl[sample(nrow(data.kl),obs, replace = TRUE),]\n\n  #Corremos regresión\n\n  m&lt;-lm(lvalor ~ lcapital + ltrabajo,\n        data=data.b)\n\n  #Guardamos en cada entrada el ratio estimado\n  beta[i,1] &lt;- as.numeric(m$coefficients[2] / m$coefficients[3])\n}\n\nEl error estimado es simplemente la desviación estándar de los B estadísticos estimados:\n\nsd(beta$beta)\n\n[1] 0.05090312\n\n\nEl error estándar estimado es de 0.0509.\n[10 puntos] Calcule ahora el error estándar jackknife, para lo que realizará \\(N\\) estimaciones de la ecuación del valor de la producción y en cada una de ellas calculará el cociente de interés. En cada una de las \\(i=1,\\ldots,N\\) repeticiones, eliminará de la muestra la observación \\(i\\), por lo que cada regresión será estimada con \\(N-1\\) observaciones. Obtenga la desviación estándar de los \\(N\\) cocientes estimados.\nFijamo utina:\n\nbeta.jackknife &lt;- data.frame(beta.jackknife=matrix(ncol = 1, nrow = nrow(data.kl)))\n\nEn cada repetición eliminamos la \\(i\\)-ésima observación, estimamos la regresión y calculamos el cociente de interés:\n\nfor (i in 1:nrow(data.kl))\n{\n  data.j &lt;- data.kl %&gt;%\n    filter(!row_number() == i )\n\n  #Corremos regresión\n\n  m &lt;-lm(lvalor ~ lcapital + ltrabajo,\n         data=data.j)\n\n  #Guardamos en cada entrada el ratio estimado\n  beta.jackknife[i,1] &lt;- as.numeric(m$coefficients[2] / m$coefficients[3])\n}\n\nObtenemos el error estándar. En clase quizás nos faltó ver la fórmula del error estándar, pero es la siguiente:\n\\[ee(\\hat{\\theta}_{jackknife})=\\sqrt{\\frac{N-1}{N}\\sum_{i=1}^{N}(\\hat\\theta_{-i}-\\bar{\\hat\\theta})^2}\\]\ndonde \\(\\hat\\theta_{-i}\\) es el estadístico estimado usando la muestra que omite la \\(i\\)-ésima observación y \\(\\bar{\\hat\\theta}\\) es la media de los \\(N\\) estadísticos estimados.\n\nbeta.jackknife &lt;- beta.jackknife %&gt;% \n  mutate(mean.beta = mean(beta.jackknife),\n         sq.desv = (beta.jackknife - mean.beta)^2)\n\nsqrt(((nrow(data.kl)-1) / nrow(data.kl))*sum(beta.jackknife$sq.desv))\n\n[1] 0.04807774\n\n\nEl error jackknife resulta ser 0.048.\n[10 puntos] Compruebe que sus cálculos aproximan el error estándar obtenido con el Método Delta. Para ello, después de estimar la ecuación del valor de la producción con la muestra original, use la función deltaMethod del paquete car.\nSi usamos el método Delta para calcular el error estándar de la combinación no lineal, obtenemos algo muy parecido, 0.052\n\ndeltaMethod(m1, \"lcapital/ltrabajo\")\n\n                  Estimate       SE    2.5 % 97.5 %\nlcapital/ltrabajo 0.582406 0.051923 0.480639 0.6842\n\n\n\n\n\n\nConsidere los datos en MunichRent.rda. Estos archivos contienen información sobre rentas en la ciudad de Munich, rent. Se desea explicar la renta en función de la antiguedad de los edificios en renta, controlando por el área, area. La variable yearc indica cuándo fue construido el edificio. Construya la antiguedad como antiguedad=2023-yearc. Para leer los datos basta con ejecutar load(“MunichRent.rda”).\n\n[10 puntos] Estime por MCO la relación entre la renta, rent y la antiguedad del edificio, controlando por area y efectos fijos de bath y kitchen. Interprete el coeficiente sobre la antiguedad.\nPrimero por MCO obtenemos una relación positiva entre la renta y el área y una relación negativa entre la renta y la antiguedad, como era de esperarse. Ambos coeficientes estimados son estadísticamente significativos.\n\nload(\"../files/MunichRent.rda\")\n\nMunichRent &lt;- MunichRent %&gt;% \n  mutate(antiguedad=2023-yearc)\n\n#Por MCO\nsummary(r.mco &lt;- lm(rent  ~ area + antiguedad,\n                    data=MunichRent))\n\n\nCall:\nlm(formula = rent ~ area + antiguedad, data = MunichRent)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-734.76  -94.75  -10.87   82.55 1063.17 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 264.3407    10.3561   25.52   &lt;2e-16 ***\narea          5.3618     0.1165   46.01   &lt;2e-16 ***\nantiguedad   -2.4913     0.1239  -20.11   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 149.3 on 3079 degrees of freedom\nMultiple R-squared:  0.4181, Adjusted R-squared:  0.4177 \nF-statistic:  1106 on 2 and 3079 DF,  p-value: &lt; 2.2e-16\n\n\n[10 puntos] Estime la misma relación que en la parte a., pero con una regresión mediana. Interprete el coeficiente sobre la antiguedad.\nAhora realizamos un modelo LAD:\n\nsummary(r.q50 &lt;- rq(rent  ~ area + antiguedad,\n                    data=MunichRent,\n                    tau=0.5))\n\n\nCall: rq(formula = rent ~ area + antiguedad, tau = 0.5, data = MunichRent)\n\ntau: [1] 0.5\n\nCoefficients:\n            Value     Std. Error t value   Pr(&gt;|t|) \n(Intercept) 310.17202  11.60978   26.71643   0.00000\narea          4.97688   0.14688   33.88284   0.00000\nantiguedad   -3.02031   0.14599  -20.68795   0.00000\n\n\nLos coeficientes estimados son de una magnitud similar a los de MCO.\n[10 puntos] Estime ahora una regresión cuantil para cada uno de los deciles de la distribución condicional de la renta y represente en una gráfica los coeficientes por regresión cuantil junto con el coeficiente de MCO para las variables del área y la antiguedad. ¿Concluye que vale la pena modelar la relación de las rentas en función del área y la antiguedad usando regresión cuantil?\nRegresión cuantil para cada decil:\n\nr.q1_9 &lt;- rq(rent  ~ area + antiguedad,\n                    data=MunichRent,\n                    tau= 1:9/10)\n\nplot(summary(r.q1_9), parm=c(\"area\",\"antiguedad\"))\n\n\n\n\n\n\n\n\nLos efectos de la antiguedad en la distribución de precios son no lineales. Los efectos en los cuantiles superiores crecen más rápido con la antiguedad. Quizás esto sugiera una preferencia por edificios viejos. La regresión cuantil sí fue útil para revelar esta característica.\n[10 puntos] Suponga que no está dispuesto a imponer una relación lineal entre la antiguedad y la renta. Considere entonces el siguiente modelo:\n\\[rent_i=\\beta_0+\\beta_1 area + \\lambda(antiguedad_i)+\\varepsilon_i\\]\nUse el estimador de Robinson (1988) para estimar este modelo parcialmente lineal. Grafique sus resultados e interprételos.\nSeleccionamos el ancho de banda:\n\nbw &lt;- npplregbw(formula=rent ~ area | antiguedad,\n                data=MunichRent,\n                regtype=\"ll\")\n\nImplementamos el estimador de Robinson:\n\nmodel.pl &lt;- npplreg(bw)\nsummary(model.pl)\n\n\nPartially Linear Model\nRegression data: 3082 training points, in 2 variable(s)\nWith 1 linear parametric regressor(s), 1 nonparametric regressor(s)\n\n                  y(z)\nBandwidth(s): 2.368969\n\n                   x(z)\nBandwidth(s): 0.8672961\n\n                    area\nCoefficient(s): 5.143053\n\nKernel Regression Estimator: Local-Linear\nBandwidth Type: Fixed\n\nResidual standard error: 145.801\nR-squared: 0.4445272\n\nContinuous Kernel Type: Second-Order Gaussian\nNo. Continuous Explanatory Vars.: 1\n\n\nPara obtener el gráfico usamos npplot:\n\ng.robinson &lt;- npplot(bw,\n       perspective=F,\n       plot.errors.method=\"bootstrap\",\n       plot.errors.boot.num=5,\n       plot.behavior=\"plot-data\")\n\n\n\n\n\n\n\ng &lt;- fitted(g.robinson$plr2)\nse &lt;- g.robinson[[\"plr2\"]][[\"merr\"]]\nlci &lt;- g - se[,1]\nuci &lt;- g + se[,2]\n\n#Este objeto nos dicen dónde fueron evaluados\nantiguedad.eval &lt;- g.robinson[[\"plr2\"]][[\"evalz\"]][[\"V1\"]]\n\nfitted &lt;- data.frame(antiguedad.eval, g,lci,uci)\n\nggplot() + \n  geom_point(data=MunichRent, aes(antiguedad,rent), color='black', alpha=0.5) + \n  geom_line(data=fitted, aes(antiguedad.eval, g), linetype='solid')+\n  geom_line(data=fitted, aes(antiguedad.eval, uci), linetype='dashed')+\n  geom_line(data=fitted, aes(antiguedad.eval, lci), linetype='dashed')\n\n\n\n\n\n\n\n\n\n\n\n\nStevenson & Wolfers (2006) estudian los efectos de la introducción de leyes que permiten el divorcio unilateral. La librería bacondecomp incluye los datos usados en dicho artículo. Usaremos los datos de 1964 a 1996 para mostrar cómo impactan las leyes de divorcio express (unilateral) a la tasa de suicidios en mujeres. Comience llamando los datos:\n::: {.cell}\nwd &lt;- divorce %&gt;% \n  filter(year&gt;1964 & year&lt;1996 & sex==2) %&gt;% \n  mutate(suicide_rate=suicide*1000000/(stpop*fshare),\n         year=as.numeric(year))\n:::\n\n[5 puntos] Estime el efecto de diferencia en diferencias asumiendo tendencias paralelas y el estimador de two-way fixed effects. Obtenga los errores estándar primero asumiendo errores clásicos y luego usando una matriz agrupada a nivel estado.\n\nm.twfe &lt;- felm(suicide_rate ~ unilateral | factor(st) + factor(year),\n                data = wd)\n\nm.twfe.cl &lt;- felm(suicide_rate ~ unilateral | factor(st) + factor(year) | 0 | st,\n                data = wd)\n\nmodelsummary(models = list(\"TWFE\"=m.twfe,\n                           \"TWFE, agrupada\"=m.twfe.cl),\n             coef_map = c('unilateral'),\n          output = 'gt',\n          stars = c('***' = .01, '**' = .05, '*' = 0.1),\n          gof_map = \"nobs\")\n\n\n\n\n\n\n\n\nTWFE\nTWFE, agrupada\n\n\n\n\nunilateral\n-2.824**\n-2.824\n\n\n\n(1.108)\n(2.274)\n\n\nNum.Obs.\n1581\n1581\n\n\n\n* p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n\n\n\n\n\n\n\n\n[5 puntos] Realice la descomposición de Goodman-Bacon (2021). Construya un gráfico donde muestre en el eje el peso otorgado a cada comparación 2x2 que el estimador de TWFE realiza mecánicamente y en el eje el efecto estimado correspondiente a cada comparación. Interprete el gráfico obtenido.\nImplementamos la descomposición:\n\ndf_bacon &lt;- bacon(suicide_rate ~ unilateral,\n                  data = wd,\n                  id_var = \"st\",\n                  time_var = \"year\")\n\n                      type  weight  avg_est\n1 Earlier vs Later Treated 0.10544  1.70904\n2  Later vs Always Treated 0.39872 -6.72231\n3 Later vs Earlier Treated 0.27432  3.31462\n4     Treated vs Untreated 0.22151 -5.56686\n\n\nLa tabla anterior nos da los valores de los estimadores \\(2\\times 2\\) con sus correspondientes pesos. El promedio ponderado es exactamente el estimador de efectos fijos:\n\ncoef_bacon &lt;- sum(df_bacon$estimate * df_bacon$weight)\n\nprint(paste(\"Suma ponderada de la descomposición =\",\n            round(coef_bacon, 4)))\n\n[1] \"Suma ponderada de la descomposición = -2.824\"\n\n\nConstruimos el gráfico:\n\ndf_bacon %&gt;% \n  ggplot(aes(x=weight,\n             y=estimate,\n             shape=type,\n             color=type)) +\n  geom_point() +\n  geom_hline(yintercept = round(m.twfe$coefficients, 4))\n\n\n\n\n\n\n\n\nLas comparaciones que más pesan en el estimador de efectos fijos son las de estados tratados con los que siempre estuvieron tratados en el panel, recibiendo dos de esas comparaciones alrededor de 12.5 y el 6% del peso (los dos triángulos verdes más hacia la derecha). otra comparación que recibe alrededor de 6.5% del peso es la de los tratados con los nunca tratados (cruz morada más hacia la derecha). En total, las comparaciones con estados que iniciaron siendo tratados se llevan el 40% del peso. Las comparaciones entre los tratados tarde y los tratados temprano también reciben un peso alto de 27%.\n[10 puntos] Implemente el estimador de Callaway & Sant’Anna (2021) para estimar los efectos del tratamiento específicos para cada cohorte, usando el paquete did. Utilice como grupo de comparación los estados que nunca son tratados. La columna stid es un identificador numérico de los estados (lo requerirá cuando use att_gt del paquete did).\n\natts_nt &lt;- att_gt(yname = \"suicide_rate\",\n                      tname = \"year\",\n                      idname = \"stid\",\n                      gname = \"divyear\",\n                      data = wd,\n                      control_group = \"nevertreated\",\n                      est_method = 'reg',\n                      bstrap = TRUE,\n                      biters = 1000,\n                      print_details = FALSE,\n                      panel = TRUE)\n\n\nsummary(atts_nt)\n\n\nCall:\natt_gt(yname = \"suicide_rate\", tname = \"year\", idname = \"stid\", \n    gname = \"divyear\", data = wd, panel = TRUE, control_group = \"nevertreated\", \n    bstrap = TRUE, biters = 1000, est_method = \"reg\", print_details = FALSE)\n\nReference: Callaway, Brantly and Pedro H.C. Sant'Anna.  \"Difference-in-Differences with Multiple Time Periods.\" Journal of Econometrics, Vol. 225, No. 2, pp. 200-230, 2021. &lt;https://doi.org/10.1016/j.jeconom.2020.12.001&gt;, &lt;https://arxiv.org/abs/1803.09015&gt; \n\nGroup-Time Average Treatment Effects:\n Group Time ATT(g,t) Std. Error [95% Simult.  Conf. Band]  \n  1969 1966  -3.1857    12.6090      -50.6076     44.2362  \n  1969 1967  12.5043     8.8492      -20.7772     45.7858  \n  1969 1968   3.1310     7.6435      -25.6160     31.8780  \n  1969 1969   1.1566     6.5207      -23.3675     25.6808  \n  1969 1970  -3.5412    13.3245      -53.6543     46.5720  \n  1969 1971  -6.7647    11.7616      -50.9995     37.4702  \n  1969 1972   1.7696    11.2775      -40.6447     44.1838  \n  1969 1973   2.7611     7.6039      -25.8369     31.3591  \n  1969 1974  -0.3084     8.5271      -32.3787     31.7618  \n  1969 1975  -0.7421     5.0161      -19.6075     18.1234  \n  1969 1976 -10.6073     7.5492      -38.9994     17.7848  \n  1969 1977  -3.9320    12.0431      -49.2258     41.3619  \n  1969 1978 -14.0099    11.3649      -56.7528     28.7329  \n  1969 1979  -8.1059    13.5501      -59.0675     42.8556  \n  1969 1980 -10.6943     7.3175      -38.2150     16.8265  \n  1969 1981  -3.4755    12.1039      -48.9980     42.0469  \n  1969 1982  -7.0072     9.0613      -41.0864     27.0721  \n  1969 1983   5.1998    13.3936      -45.1733     55.5728  \n  1969 1984 -10.0771    13.3542      -60.3017     40.1474  \n  1969 1985   5.9598    17.2201      -58.8045     70.7240  \n  1969 1986  -8.5962    12.7169      -56.4240     39.2317  \n  1969 1987  -8.6897     7.7323      -37.7705     20.3911  \n  1969 1988 -11.9815     8.3038      -43.2119     19.2489  \n  1969 1989  -5.9781    15.2884      -63.4774     51.5213  \n  1969 1990  -7.7327    11.1956      -49.8389     34.3735  \n  1969 1991 -14.7659    11.4140      -57.6934     28.1616  \n  1969 1992  -6.6956     6.5026      -31.1515     17.7603  \n  1969 1993   0.5290    12.8492      -47.7963     48.8543  \n  1969 1994  -6.1575    16.2005      -67.0872     54.7721  \n  1969 1995  -6.7870    17.1863      -71.4242     57.8503  \n  1970 1966  -5.7213     9.5945      -41.8061     30.3635  \n  1970 1967  10.0224     8.7761      -22.9842     43.0289  \n  1970 1968  -5.3764     9.3571      -40.5682     29.8154  \n  1970 1969  10.5202     6.3007      -13.1766     34.2169  \n  1970 1970   5.6377     6.5355      -18.9419     30.2174  \n  1970 1971   1.3987     4.8790      -16.9511     19.7485  \n  1970 1972   1.3331     9.6135      -34.8230     37.4891  \n  1970 1973 -13.0254     3.8835      -27.6311      1.5803  \n  1970 1974 -12.7744     4.9304      -31.3173      5.7685  \n  1970 1975 -15.3434     4.8741      -33.6748      2.9880  \n  1970 1976 -19.8605     4.0152      -34.9616     -4.7594 *\n  1970 1977 -19.5551     6.0573      -42.3365      3.2263  \n  1970 1978 -33.3382     5.6337      -54.5265    -12.1499 *\n  1970 1979 -30.5087    13.5303      -81.3955     20.3781  \n  1970 1980 -44.6366     7.4756      -72.7523    -16.5210 *\n  1970 1981 -34.0557    11.3256      -76.6509      8.5395  \n  1970 1982 -38.7875    10.3941      -77.8796      0.3045  \n  1970 1983 -32.9234    17.1234      -97.3240     31.4771  \n  1970 1984 -34.0625    11.3245      -76.6535      8.5286  \n  1970 1985 -30.6346    19.1149     -102.5252     41.2560  \n  1970 1986 -37.0754    18.8397     -107.9311     33.7802  \n  1970 1987 -37.6630    11.4929      -80.8875      5.5615  \n  1970 1988 -43.0563    12.8519      -91.3921      5.2794  \n  1970 1989 -45.1314    14.6881     -100.3729     10.1101  \n  1970 1990 -43.1765     8.6367      -75.6588    -10.6942 *\n  1970 1991 -49.9116     9.3412      -85.0436    -14.7797 *\n  1970 1992 -50.9515    14.7946     -106.5937      4.6906  \n  1970 1993 -44.5526    12.3309      -90.9285      1.8233  \n  1970 1994 -51.5405    17.6950     -118.0909     15.0099  \n  1970 1995 -48.4108    17.3598     -113.7003     16.8788  \n  1971 1966 -12.4375     9.2619      -47.2712     22.3963  \n  1971 1967  17.0967     8.3386      -14.2643     48.4578  \n  1971 1968   2.6186     7.3020      -24.8440     30.0813  \n  1971 1969  -2.1268     5.6673      -23.4413     19.1876  \n  1971 1970   5.7625     6.9692      -20.4485     31.9734  \n  1971 1971  -9.3866     7.2642      -36.7071     17.9338  \n  1971 1972 -13.8393     7.9538      -43.7533     16.0746  \n  1971 1973 -12.4602     7.8160      -41.8558     16.9355  \n  1971 1974   0.1729     5.5446      -20.6803     21.0261  \n  1971 1975  -8.8785     7.9118      -38.6345     20.8775  \n  1971 1976  -7.7923     5.8002      -29.6065     14.0219  \n  1971 1977  -6.3192     8.1699      -37.0460     24.4077  \n  1971 1978 -17.4985    10.5786      -57.2843     22.2872  \n  1971 1979 -16.1999     5.9721      -38.6606      6.2609  \n  1971 1980 -22.5395     5.3023      -42.4812     -2.5978 *\n  1971 1981 -12.5894     8.1475      -43.2319     18.0532  \n  1971 1982 -20.6385    10.2477      -59.1799     17.9028  \n  1971 1983 -11.0888     5.8593      -33.1253     10.9477  \n  1971 1984 -12.7478     6.1768      -35.9787     10.4831  \n  1971 1985  -9.3683     8.7995      -42.4627     23.7262  \n  1971 1986 -16.9260     7.0962      -43.6145      9.7625  \n  1971 1987 -12.9962    10.8822      -53.9237     27.9313  \n  1971 1988 -14.6487     8.1038      -45.1267     15.8292  \n  1971 1989 -18.7126     9.6051      -54.8369     17.4118  \n  1971 1990 -17.6198     6.9975      -43.9371      8.6976  \n  1971 1991 -17.2789     8.8508      -50.5665     16.0088  \n  1971 1992 -22.1825     9.5658      -58.1592     13.7943  \n  1971 1993  -9.1278     9.4625      -44.7159     26.4603  \n  1971 1994 -13.7091     8.6761      -46.3396     18.9214  \n  1971 1995 -15.3270     7.1781      -42.3235     11.6695  \n  1972 1966  -5.2709    10.2345      -43.7627     33.2209  \n  1972 1967   8.3599     9.5882      -27.7009     44.4207  \n  1972 1968   5.3444     7.4435      -22.6504     33.3393  \n  1972 1969  -6.1540     5.3925      -26.4352     14.1272  \n  1972 1970   5.3754     5.4766      -15.2220     25.9729  \n  1972 1971  -0.7708     5.3490      -20.8882     19.3467  \n  1972 1972  -5.3078     4.8366      -23.4980     12.8824  \n  1972 1973  -7.1261     9.2918      -42.0722     27.8199  \n  1972 1974  -4.3635     4.5454      -21.4585     12.7315  \n  1972 1975 -10.7104     7.0424      -37.1965     15.7756  \n  1972 1976  -8.5659     7.7610      -37.7548     20.6229  \n  1972 1977  -1.7264     4.1491      -17.3312     13.8784  \n  1972 1978 -19.5471     7.8113      -48.9251      9.8310  \n  1972 1979  -9.7870    10.2059      -48.1712     28.5972  \n  1972 1980 -16.4149     6.0669      -39.2322      6.4024  \n  1972 1981  -4.6831     4.8897      -23.0731     13.7069  \n  1972 1982  -9.8129     8.9395      -43.4339     23.8082  \n  1972 1983  -5.7570     6.8767      -31.6201     20.1061  \n  1972 1984 -11.1817     5.6138      -32.2949      9.9314  \n  1972 1985  -8.5492     8.7787      -41.5657     24.4672  \n  1972 1986  -3.2909     6.2836      -26.9234     20.3416  \n  1972 1987 -14.5853     7.4985      -42.7867     13.6162  \n  1972 1988 -12.6795     6.3039      -36.3883     11.0293  \n  1972 1989 -10.9845     7.1656      -37.9340     15.9651  \n  1972 1990  -7.7794     7.3159      -35.2943     19.7355  \n  1972 1991 -13.7033     5.5948      -34.7451      7.3385  \n  1972 1992 -11.0100     8.8163      -44.1679     22.1480  \n  1972 1993 -17.3770     7.4747      -45.4890     10.7350  \n  1972 1994 -16.6543     7.9092      -46.4007     13.0920  \n  1972 1995 -16.0626     6.6801      -41.1862      9.0610  \n  1973 1966 -10.0893    10.0711      -47.9663     27.7876  \n  1973 1967  16.0570     9.7318      -20.5439     52.6580  \n  1973 1968  -4.8486     8.5848      -37.1357     27.4385  \n  1973 1969   3.3129     6.0442      -19.4192     26.0449  \n  1973 1970   5.5555     6.4672      -18.7676     29.8786  \n  1973 1971  -2.0683     7.2951      -29.5049     25.3683  \n  1973 1972  -0.9275     6.5936      -25.7258     23.8707  \n  1973 1973   4.4106     8.4384      -27.3258     36.1470  \n  1973 1974   3.2944     6.1162      -19.7086     26.2973  \n  1973 1975   0.0725    11.5704      -43.4432     43.5883  \n  1973 1976  -1.1180     6.2547      -24.6418     22.4059  \n  1973 1977   2.7203     5.8664      -19.3431     24.7837  \n  1973 1978 -10.3862     8.7687      -43.3649     22.5924  \n  1973 1979  -3.4722     7.2328      -30.6746     23.7303  \n  1973 1980 -11.7465     7.7766      -40.9940     17.5009  \n  1973 1981  -3.1322     7.4260      -31.0614     24.7969  \n  1973 1982  -7.8264    10.3198      -46.6387     30.9859  \n  1973 1983  -4.3281     5.9254      -26.6134     17.9572  \n  1973 1984 -10.3847     6.4512      -34.6474     13.8780  \n  1973 1985  -3.3503     7.6448      -32.1021     25.4014  \n  1973 1986  -9.9416     4.6451      -27.4118      7.5286  \n  1973 1987 -10.5611     8.6734      -43.1813     22.0591  \n  1973 1988 -13.3770     9.0506      -47.4158     20.6619  \n  1973 1989  -9.7072     6.9669      -35.9096     16.4952  \n  1973 1990 -12.5464     6.0032      -35.1243     10.0314  \n  1973 1991 -15.9396     7.1237      -42.7317     10.8526  \n  1973 1992 -17.9985     6.9073      -43.9765      7.9796  \n  1973 1993 -13.8426     7.8503      -43.3673     15.6821  \n  1973 1994  -9.0985     6.0048      -31.6824     13.4855  \n  1973 1995 -12.4104     4.0223      -27.5381      2.7173  \n  1974 1966  -3.7313    10.2818      -42.4010     34.9383  \n  1974 1967   9.6967     8.4315      -22.0140     41.4075  \n  1974 1968  -5.6736     8.2589      -36.7351     25.3879  \n  1974 1969   4.7497     5.0947      -14.4114     23.9108  \n  1974 1970   8.9528     5.5731      -12.0073     29.9129  \n  1974 1971  -8.1562     5.5379      -28.9842     12.6718  \n  1974 1972   5.1631     4.2741      -10.9115     21.2378  \n  1974 1973  -4.1358     7.3372      -31.7309     23.4592  \n  1974 1974  -1.5277     4.6118      -18.8727     15.8173  \n  1974 1975  -0.2490     4.8780      -18.5948     18.0968  \n  1974 1976  -4.6623     3.9210      -19.4090     10.0844  \n  1974 1977  -3.6545     7.2230      -30.8199     23.5110  \n  1974 1978  -9.4996     5.6922      -30.9080     11.9087  \n  1974 1979  -1.7953     9.8989      -39.0246     35.4340  \n  1974 1980 -10.9157     5.3459      -31.0215      9.1901  \n  1974 1981  -2.9438     7.4216      -30.8561     24.9686  \n  1974 1982  -6.1358     5.0386      -25.0859     12.8143  \n  1974 1983  -2.2347     7.6185      -30.8878     26.4184  \n  1974 1984  -7.2784     6.5728      -31.9986     17.4418  \n  1974 1985   0.9048    10.7007      -39.3403     41.1500  \n  1974 1986  -3.4953     8.9151      -37.0248     30.0342  \n  1974 1987  -9.3045     8.3304      -40.6350     22.0259  \n  1974 1988  -9.0434     8.4419      -40.7932     22.7063  \n  1974 1989  -6.4758     9.6869      -42.9080     29.9564  \n  1974 1990  -7.6369     8.1137      -38.1521     22.8783  \n  1974 1991 -14.7133     7.9173      -44.4900     15.0634  \n  1974 1992 -14.7711     7.9707      -44.7486     15.2064  \n  1974 1993 -11.2274     8.5024      -43.2045     20.7497  \n  1974 1994 -14.4350    10.7449      -54.8461     25.9761  \n  1974 1995 -13.4194     9.6698      -49.7871     22.9484  \n  1975 1966 -10.7092     9.4775      -46.3536     24.9352  \n  1975 1967   9.6526     8.5008      -22.3186     41.6237  \n  1975 1968   5.2146     7.9016      -24.5031     34.9324  \n  1975 1969  -2.1343     8.3120      -33.3954     29.1269  \n  1975 1970  -5.6035     9.4988      -41.3283     30.1214  \n  1975 1971   3.4917     9.6360      -32.7492     39.7325  \n  1975 1972 -15.0108     8.0545      -45.3036     15.2820  \n  1975 1973   9.7321     7.1463      -17.1450     36.6091  \n  1975 1974   0.5633     5.8727      -21.5236     22.6502  \n  1975 1975  -4.9361     5.2822      -24.8023     14.9300  \n  1975 1976  -1.1020     3.9490      -15.9540     13.7501  \n  1975 1977  -4.3766    10.2412      -42.8934     34.1401  \n  1975 1978  -8.3191     7.6221      -36.9856     20.3474  \n  1975 1979 -12.7262    16.1052      -73.2973     47.8450  \n  1975 1980 -12.3662     5.2244      -32.0149      7.2826  \n  1975 1981  -7.7023     6.5726      -32.4217     17.0171  \n  1975 1982  -9.7198     9.3131      -44.7463     25.3066  \n  1975 1983  -7.3926     6.3929      -31.4359     16.6507  \n  1975 1984   8.8507     9.4537      -26.7045     44.4058  \n  1975 1985  -5.6163     8.3174      -36.8977     25.6651  \n  1975 1986  -5.2183     4.7325      -23.0171     12.5805  \n  1975 1987  -0.8638     5.2844      -20.7382     19.0107  \n  1975 1988 -15.3668    14.3175      -69.2143     38.4808  \n  1975 1989  -5.4933     5.8675      -27.5609     16.5742  \n  1975 1990   8.5067     7.9168      -21.2680     38.2813  \n  1975 1991   0.7631     5.9418      -21.5839     23.1101  \n  1975 1992  -4.2255     6.0728      -27.0650     18.6140  \n  1975 1993   1.1200     2.5499       -8.4702     10.7101  \n  1975 1994  -8.3655     7.6252      -37.0437     20.3128  \n  1975 1995   1.8041     3.8506      -12.6781     16.2863  \n  1976 1966  -7.3839     9.4775      -43.0283     28.2605  \n  1976 1967   6.1953     8.5008      -25.7758     38.1665  \n  1976 1968  -3.4100     6.8387      -29.1299     22.3100  \n  1976 1969  -4.5734     4.4373      -21.2619     12.1150  \n  1976 1970  13.6356     4.7051       -4.0601     31.3313  \n  1976 1971   2.1590     5.5379      -18.6690     22.9870  \n  1976 1972  -1.4512     4.3507      -17.8142     14.9117  \n  1976 1973 -23.5440     6.8621      -49.3521      2.2641  \n  1976 1974  37.6970     4.2540       21.6977     53.6963 *\n  1976 1975  -5.8620     5.1960      -25.4041     13.6801  \n  1976 1976   7.3375     6.7834      -18.1745     32.8496  \n  1976 1977  35.4928     7.3478        7.8581     63.1274 *\n  1976 1978  -2.2158     3.3728      -14.9006     10.4690  \n  1976 1979   2.7599    11.1105      -39.0265     44.5463  \n  1976 1980 -10.1277     5.1657      -29.5559      9.3004  \n  1976 1981  -9.1249     8.4453      -40.8873     22.6375  \n  1976 1982  -9.7529     5.1867      -29.2600      9.7541  \n  1976 1983 -12.2793     7.8075      -41.6431     17.0844  \n  1976 1984 -20.1148     4.7838      -38.1066     -2.1229 *\n  1976 1985  -0.2053    11.6139      -43.8848     43.4742  \n  1976 1986 -27.7992     8.3181      -59.0832      3.4848  \n  1976 1987  -9.9985     3.6973      -23.9038      3.9069  \n  1976 1988 -22.8540     5.8685      -44.9254     -0.7826 *\n  1976 1989 -14.3020     6.6385      -39.2692     10.6652  \n  1976 1990 -16.7275     4.8656      -35.0267      1.5717  \n  1976 1991 -29.9838     5.5425      -50.8290     -9.1387 *\n  1976 1992 -35.9431     6.2168      -59.3241    -12.5620 *\n  1976 1993 -33.5630     6.2298      -56.9932    -10.1328 *\n  1976 1994 -19.1785    10.5829      -58.9804     20.6233  \n  1976 1995 -18.8943     9.0197      -52.8170     15.0284  \n  1977 1966 -17.8103    11.1251      -59.6516     24.0310  \n  1977 1967  22.0500     9.1659      -12.4225     56.5225  \n  1977 1968 -12.0551    13.0853      -61.2684     37.1582  \n  1977 1969  11.3299    12.0132      -33.8514     56.5113  \n  1977 1970   7.9543     7.6371      -20.7685     36.6771  \n  1977 1971   0.9514     9.1138      -33.3252     35.2281  \n  1977 1972  -1.0798     9.5351      -36.9411     34.7816  \n  1977 1973  -2.0153     9.3763      -37.2792     33.2485  \n  1977 1974 -10.4409     5.7751      -32.1610     11.2792  \n  1977 1975  -2.6782     7.1322      -29.5022     24.1458  \n  1977 1976   7.5869    18.0939      -60.4636     75.6373  \n  1977 1977  -0.3083    15.2383      -57.6192     57.0026  \n  1977 1978 -18.5991    18.2767      -87.3370     50.1389  \n  1977 1979  -8.3001    28.5726     -115.7608     99.1606  \n  1977 1980 -13.4381    23.5917     -102.1657     75.2895  \n  1977 1981 -12.1467    21.1072      -91.5302     67.2367  \n  1977 1982 -17.4639    16.5230      -79.6064     44.6786  \n  1977 1983   6.1362    30.8151     -109.7585    122.0309  \n  1977 1984  -2.9191    24.6110      -95.4801     89.6419  \n  1977 1985 -14.8995    20.0884      -90.4513     60.6523  \n  1977 1986 -12.3230    14.6294      -67.3438     42.6979  \n  1977 1987 -23.6769    27.9188     -128.6785     81.3247  \n  1977 1988 -25.5547    22.3801     -109.7256     58.6162  \n  1977 1989  -9.2602    30.0056     -122.1103    103.5898  \n  1977 1990 -13.7369    29.7268     -125.5385     98.0647  \n  1977 1991 -25.9731    24.8484     -119.4270     67.4807  \n  1977 1992 -29.9220    23.7594     -119.2802     59.4362  \n  1977 1993 -14.9531    27.8362     -119.6441     89.7380  \n  1977 1994 -11.6033    35.5769     -145.4068    122.2001  \n  1977 1995 -29.0098    21.5447     -110.0389     52.0193  \n  1980 1966  -9.4644     9.4775      -45.1088     26.1799  \n  1980 1967  12.2092     8.5008      -19.7619     44.1804  \n  1980 1968   3.0528     6.8387      -22.6672     28.7727  \n  1980 1969  -3.6543     4.4373      -20.3428     13.0341  \n  1980 1970  12.7725     4.7051       -4.9232     30.4682  \n  1980 1971 -11.0126     5.5379      -31.8406      9.8154  \n  1980 1972   5.2094     4.3507      -11.1536     21.5723  \n  1980 1973  -8.0378     6.8621      -33.8459     17.7703  \n  1980 1974   2.0132     4.2540      -13.9861     18.0125  \n  1980 1975  -3.8913     5.1960      -23.4335     15.6508  \n  1980 1976   4.9514     6.7834      -20.5607     30.4634  \n  1980 1977  -1.8244     3.6248      -15.4572     11.8085  \n  1980 1978  -6.3410     5.6068      -27.4279     14.7460  \n  1980 1979   5.7875    10.6480      -34.2594     45.8344  \n  1980 1980 -12.0465     7.0875      -38.7025     14.6095  \n  1980 1981  -7.2766     3.1780      -19.2288      4.6756  \n  1980 1982  -7.7888    11.4064      -50.6877     35.1102  \n  1980 1983   3.8986     3.7649      -10.2611     18.0583  \n  1980 1984  -2.3172     5.8716      -24.4002     19.7657  \n  1980 1985   4.4467     4.7848      -13.5490     22.4423  \n  1980 1986  -9.5981     7.0565      -36.1373     16.9410  \n  1980 1987 -10.0537    11.0181      -51.4923     31.3849  \n  1980 1988 -10.9789     9.1300      -45.3164     23.3587  \n  1980 1989  -6.9527     6.3603      -30.8735     16.9681  \n  1980 1990  -5.3312     6.2450      -28.8183     18.1560  \n  1980 1991  -8.8820     7.9543      -38.7981     21.0340  \n  1980 1992 -12.0023     8.8737      -45.3760     21.3714  \n  1980 1993  -8.3192     7.7315      -37.3969     20.7586  \n  1980 1994 -12.5474     7.1011      -39.2546     14.1597  \n  1980 1995  -6.7308     9.4076      -42.1124     28.6508  \n  1984 1966  -9.0520     9.4775      -44.6964     26.5924  \n  1984 1967  10.1591     8.5008      -21.8120     42.1303  \n  1984 1968  -3.1247     6.8387      -28.8447     22.5952  \n  1984 1969   4.0091     4.4373      -12.6793     20.6976  \n  1984 1970   4.9470     4.7051      -12.7487     22.6428  \n  1984 1971 -12.3840     5.5379      -33.2120      8.4440  \n  1984 1972   5.1482     4.3507      -11.2147     21.5112  \n  1984 1973  -6.4124     6.8621      -32.2205     19.3957  \n  1984 1974  -5.7358     4.2540      -21.7351     10.2635  \n  1984 1975  -4.0919     5.1960      -23.6340     15.4503  \n  1984 1976   8.7209     6.7834      -16.7911     34.2329  \n  1984 1977   2.6679     3.6248      -10.9650     16.3007  \n  1984 1978  -7.3955     5.6068      -28.4825     13.6914  \n  1984 1979   2.2253    10.6480      -37.8217     42.2722  \n  1984 1980  -8.3661     7.0875      -35.0220     18.2899  \n  1984 1981  10.0008     5.6947      -11.4168     31.4184  \n  1984 1982  -3.2324    10.2165      -41.6562     35.1914  \n  1984 1983  11.0979    10.1458      -27.0603     49.2560  \n  1984 1984  -1.6979     3.0237      -13.0697      9.6740  \n  1984 1985   4.0744     3.8064      -10.2414     18.3901  \n  1984 1986  -8.0341     2.9094      -18.9763      2.9081  \n  1984 1987 -10.4189     7.1120      -37.1667     16.3289  \n  1984 1988  -7.8277     5.4597      -28.3613     12.7059  \n  1984 1989  -9.2894     3.4826      -22.3874      3.8086  \n  1984 1990  -6.2529     3.0974      -17.9022      5.3963  \n  1984 1991 -10.1584     4.9176      -28.6534      8.3366  \n  1984 1992 -11.4694     4.6630      -29.0068      6.0680  \n  1984 1993  -9.2844     4.4284      -25.9395      7.3707  \n  1984 1994 -14.6630     3.4313      -27.5682     -1.7578 *\n  1984 1995 -12.5044     3.3329      -25.0393      0.0304  \n  1985 1966  29.3180     9.4775       -6.3263     64.9624  \n  1985 1967  -6.8558     8.5008      -38.8270     25.1153  \n  1985 1968  -5.9545     6.8387      -31.6745     19.7654  \n  1985 1969  12.1675     4.4373       -4.5209     28.8560  \n  1985 1970  12.3074     4.7051       -5.3883     30.0031  \n  1985 1971   4.9424     5.5379      -15.8856     25.7704  \n  1985 1972   5.1718     4.3507      -11.1911     21.5348  \n  1985 1973 -41.2443     6.8621      -67.0524    -15.4362 *\n  1985 1974   9.0064     4.2540       -6.9928     25.0057  \n  1985 1975  -9.1424     5.1960      -28.6845     10.3998  \n  1985 1976   2.8125     6.7834      -22.6995     28.3245  \n  1985 1977  -2.1400     3.6248      -15.7728     11.4929  \n  1985 1978   5.7781     5.6068      -15.3088     26.8651  \n  1985 1979  -3.8368    10.6480      -43.8837     36.2102  \n  1985 1980   1.5265     7.0875      -25.1295     28.1825  \n  1985 1981  -2.8105     5.6947      -24.2281     18.6071  \n  1985 1982  13.2242    10.2165      -25.1996     51.6479  \n  1985 1983  -5.5714    10.1458      -43.7295     32.5867  \n  1985 1984  -5.5493     3.0237      -16.9212      5.8225  \n  1985 1985  11.4728     6.8301      -14.2148     37.1605  \n  1985 1986  10.1715     3.4372       -2.7558     23.0989  \n  1985 1987  17.5291     4.9075       -0.9279     35.9861  \n  1985 1988  -9.6423     3.9206      -24.3875      5.1030  \n  1985 1989  19.8229     5.3527       -0.3085     39.9542  \n  1985 1990  26.2350     0.9371       22.7106     29.7594 *\n  1985 1991   6.2209     3.7879       -8.0253     20.4671  \n  1985 1992  18.3602     6.2606       -5.1856     41.9059  \n  1985 1993  23.0343     2.8095       12.4679     33.6008 *\n  1985 1994  15.2612     6.2622       -8.2907     38.8131  \n  1985 1995  15.4633     4.4738       -1.3625     32.2892  \n---\nSignif. codes: `*' confidence band does not cover 0\n\nControl Group:  Never Treated,  Anticipation Periods:  0\nEstimation Method:  Outcome Regression\n\nggdid(atts_nt)\n\n\n\n\n\n\n\n\n[10 puntos] Reporte los resultados agregados obtenidos a partir del estimador Callaway & Sant’Anna (2021), usando una agregación dinámica que muestre los efectos promedio para cada periodo antes y después del tratamiento. Grafique e interprete los resultados.\n\nagg.es &lt;- aggte(atts_nt,\n                type = \"dynamic\")\n\nsummary(agg.es)\n\n\nCall:\naggte(MP = atts_nt, type = \"dynamic\")\n\nReference: Callaway, Brantly and Pedro H.C. Sant'Anna.  \"Difference-in-Differences with Multiple Time Periods.\" Journal of Econometrics, Vol. 225, No. 2, pp. 200-230, 2021. &lt;https://doi.org/10.1016/j.jeconom.2020.12.001&gt;, &lt;https://arxiv.org/abs/1803.09015&gt; \n\n\nOverall summary of ATT's based on event-study/dynamic aggregation:  \n      ATT    Std. Error     [ 95%  Conf. Int.]  \n -11.4964        3.7288   -18.8047     -4.1881 *\n\n\nDynamic Effects:\n Event time Estimate Std. Error [95% Simult.  Conf. Band]  \n        -19  29.3180    10.1469        1.3095     57.3266 *\n        -18  -7.9539     1.8476      -13.0538     -2.8540 *\n        -17   2.1023     5.9725      -14.3837     18.5883  \n        -16   4.5214     5.6877      -11.1785     20.2213  \n        -15   8.1583     5.9371       -8.2299     24.5465  \n        -14   0.1417     4.8451      -13.2322     13.5156  \n        -13   1.6657     7.2660      -18.3906     21.7220  \n        -12 -11.0144    14.9396      -52.2524     30.2235  \n        -11  -9.0818     7.5219      -29.8445     11.6808  \n        -10   8.0943     6.4728       -9.7727     25.9614  \n         -9  -7.0756     5.2780      -21.6446      7.4934  \n         -8   4.2068     4.1602       -7.2766     15.6902  \n         -7  -2.2507     4.5544      -14.8223     10.3209  \n         -6   5.6460     3.7781       -4.7828     16.0748  \n         -5  -3.4392     3.4505      -12.9635      6.0851  \n         -4   5.1665     2.6344       -2.1052     12.4383  \n         -3  -0.3596     2.1774       -6.3699      5.6508  \n         -2   1.1409     2.5639       -5.9363      8.2182  \n         -1   1.9481     3.1449       -6.7330     10.6291  \n          0  -0.8065     2.8923       -8.7902      7.1773  \n          1  -2.7726     3.0902      -11.3025      5.7573  \n          2  -4.2845     4.3780      -16.3691      7.8001  \n          3  -3.9743     2.8756      -11.9117      3.9631  \n          4  -4.5620     3.5778      -14.4376      5.3137  \n          5  -7.4465     3.1521      -16.1472      1.2542  \n          6  -6.2541     3.9075      -17.0399      4.5317  \n          7 -10.6094     3.5133      -20.3071     -0.9117 *\n          8  -9.9746     3.5535      -19.7834     -0.1658 *\n          9 -10.1110     3.7334      -20.4164      0.1944  \n         10 -11.0023     3.9502      -21.9060     -0.0987 *\n         11 -13.3679     3.8009      -23.8595     -2.8763 *\n         12  -8.7828     4.8173      -22.0800      4.5144  \n         13 -12.1880     4.0827      -23.4575     -0.9185 *\n         14 -11.2760     4.5907      -23.9478      1.3958  \n         15 -14.8487     5.0057      -28.6658     -1.0315 *\n         16 -11.7709     5.1512      -25.9897      2.4479  \n         17 -14.3232     4.7228      -27.3595     -1.2869 *\n         18 -17.1010     4.6834      -30.0288     -4.1733 *\n         19 -17.6118     4.2504      -29.3443     -5.8793 *\n         20 -14.8085     5.0632      -28.7847     -0.8324 *\n         21 -16.5371     4.7367      -29.6117     -3.4624 *\n         22 -15.2723     4.6988      -28.2424     -2.3022 *\n         23 -17.6177     7.3821      -37.9947      2.7593  \n         24 -19.0284     8.8203      -43.3752      5.3184  \n         25 -27.2842    19.0460      -79.8569     25.2886  \n         26  -6.7870    16.2988      -51.7766     38.2026  \n---\nSignif. codes: `*' confidence band does not cover 0\n\nControl Group:  Never Treated,  Anticipation Periods:  0\nEstimation Method:  Outcome Regression\n\nggdid(agg.es)\n\n\n\n\n\n\n\n\nSe obtiene una reducción en la tasa de suicidios que es estadísticamente significativa a partir de 5 años después de la introducción de la legislación."
  },
  {
    "objectID": "tareas/tarea-4-respuestas.html#pregunta-1",
    "href": "tareas/tarea-4-respuestas.html#pregunta-1",
    "title": "Respuestas a la tarea 4",
    "section": "",
    "text": "Considere los datos en el archivo capital_trabajo.csv. Con una función de producción Cobb-Douglas las participaciones del capital y el trabajo en el valor de la producción se pueden estimar usando una regresión lineal. En algunas aplicaciones es de interés conocer el cociente de las participaciones estimadas.\n\n[10 puntos] Usando 500 repeticiones bootstrap estime el error estándar del cociente capital-trabajo. Para ello realice el siguiente procedimiento:\n\nGenere una matriz vacía de 500 filas para coleccionar sus relaciones estimadas.\nEn cada una de las repeticiones obtenga una muestra con remplazo a partir de la muestra original.\nEstime por MCO los coeficientes sobre el log del capital y el log del trabajo. La variable dependiente es el log del valor de la producción. Calcule el cociente de los coeficientes estimados. Guarde el cociente en la matriz.\nRepita ii. y iii. 500 veces.\nCalcule la desviación estándar de los cocientes estimados.\n\nEn cada repetición bootstrap debemos estimar el siguiente modelo y obtener el ratio de los coeficientes:\n\ndata.kl &lt;- read_csv(\"../files/capital_trabajo.csv\") \n\nsummary(m1 &lt;- lm(lvalor ~ lcapital + ltrabajo, data=data.kl))\n\n\nCall:\nlm(formula = lvalor ~ lcapital + ltrabajo, data = data.kl)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.53523 -0.25678  0.03835  0.26003  0.49631 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 10.56478    0.05303  199.24   &lt;2e-16 ***\nlcapital     0.38502    0.03072   12.53   &lt;2e-16 ***\nltrabajo     0.66108    0.02813   23.50   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2999 on 97 degrees of freedom\nMultiple R-squared:  0.8768, Adjusted R-squared:  0.8742 \nF-statistic: 345.1 on 2 and 97 DF,  p-value: &lt; 2.2e-16\n\n\nFijamos una semilla y los parámetros de la rutina:\n\nset.seed(120)\nB=500\nobs &lt;- nrow(data.kl)\nbeta &lt;- data.frame(beta=matrix(ncol = 1, nrow = B))\n\nRealizamos la regresión y el cálculo del cociente en cada una de las \\(b\\) repeticiones:\n\nfor (i in 1:B)\n{\n  data.b &lt;-data.kl[sample(nrow(data.kl),obs, replace = TRUE),]\n\n  #Corremos regresión\n\n  m&lt;-lm(lvalor ~ lcapital + ltrabajo,\n        data=data.b)\n\n  #Guardamos en cada entrada el ratio estimado\n  beta[i,1] &lt;- as.numeric(m$coefficients[2] / m$coefficients[3])\n}\n\nEl error estimado es simplemente la desviación estándar de los B estadísticos estimados:\n\nsd(beta$beta)\n\n[1] 0.05090312\n\n\nEl error estándar estimado es de 0.0509.\n[10 puntos] Calcule ahora el error estándar jackknife, para lo que realizará \\(N\\) estimaciones de la ecuación del valor de la producción y en cada una de ellas calculará el cociente de interés. En cada una de las \\(i=1,\\ldots,N\\) repeticiones, eliminará de la muestra la observación \\(i\\), por lo que cada regresión será estimada con \\(N-1\\) observaciones. Obtenga la desviación estándar de los \\(N\\) cocientes estimados.\nFijamo utina:\n\nbeta.jackknife &lt;- data.frame(beta.jackknife=matrix(ncol = 1, nrow = nrow(data.kl)))\n\nEn cada repetición eliminamos la \\(i\\)-ésima observación, estimamos la regresión y calculamos el cociente de interés:\n\nfor (i in 1:nrow(data.kl))\n{\n  data.j &lt;- data.kl %&gt;%\n    filter(!row_number() == i )\n\n  #Corremos regresión\n\n  m &lt;-lm(lvalor ~ lcapital + ltrabajo,\n         data=data.j)\n\n  #Guardamos en cada entrada el ratio estimado\n  beta.jackknife[i,1] &lt;- as.numeric(m$coefficients[2] / m$coefficients[3])\n}\n\nObtenemos el error estándar. En clase quizás nos faltó ver la fórmula del error estándar, pero es la siguiente:\n\\[ee(\\hat{\\theta}_{jackknife})=\\sqrt{\\frac{N-1}{N}\\sum_{i=1}^{N}(\\hat\\theta_{-i}-\\bar{\\hat\\theta})^2}\\]\ndonde \\(\\hat\\theta_{-i}\\) es el estadístico estimado usando la muestra que omite la \\(i\\)-ésima observación y \\(\\bar{\\hat\\theta}\\) es la media de los \\(N\\) estadísticos estimados.\n\nbeta.jackknife &lt;- beta.jackknife %&gt;% \n  mutate(mean.beta = mean(beta.jackknife),\n         sq.desv = (beta.jackknife - mean.beta)^2)\n\nsqrt(((nrow(data.kl)-1) / nrow(data.kl))*sum(beta.jackknife$sq.desv))\n\n[1] 0.04807774\n\n\nEl error jackknife resulta ser 0.048.\n[10 puntos] Compruebe que sus cálculos aproximan el error estándar obtenido con el Método Delta. Para ello, después de estimar la ecuación del valor de la producción con la muestra original, use la función deltaMethod del paquete car.\nSi usamos el método Delta para calcular el error estándar de la combinación no lineal, obtenemos algo muy parecido, 0.052\n\ndeltaMethod(m1, \"lcapital/ltrabajo\")\n\n                  Estimate       SE    2.5 % 97.5 %\nlcapital/ltrabajo 0.582406 0.051923 0.480639 0.6842"
  },
  {
    "objectID": "tareas/tarea-4-respuestas.html#pregunta-2",
    "href": "tareas/tarea-4-respuestas.html#pregunta-2",
    "title": "Respuestas a la tarea 4",
    "section": "",
    "text": "Considere los datos en MunichRent.rda. Estos archivos contienen información sobre rentas en la ciudad de Munich, rent. Se desea explicar la renta en función de la antiguedad de los edificios en renta, controlando por el área, area. La variable yearc indica cuándo fue construido el edificio. Construya la antiguedad como antiguedad=2023-yearc. Para leer los datos basta con ejecutar load(“MunichRent.rda”).\n\n[10 puntos] Estime por MCO la relación entre la renta, rent y la antiguedad del edificio, controlando por area y efectos fijos de bath y kitchen. Interprete el coeficiente sobre la antiguedad.\nPrimero por MCO obtenemos una relación positiva entre la renta y el área y una relación negativa entre la renta y la antiguedad, como era de esperarse. Ambos coeficientes estimados son estadísticamente significativos.\n\nload(\"../files/MunichRent.rda\")\n\nMunichRent &lt;- MunichRent %&gt;% \n  mutate(antiguedad=2023-yearc)\n\n#Por MCO\nsummary(r.mco &lt;- lm(rent  ~ area + antiguedad,\n                    data=MunichRent))\n\n\nCall:\nlm(formula = rent ~ area + antiguedad, data = MunichRent)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-734.76  -94.75  -10.87   82.55 1063.17 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 264.3407    10.3561   25.52   &lt;2e-16 ***\narea          5.3618     0.1165   46.01   &lt;2e-16 ***\nantiguedad   -2.4913     0.1239  -20.11   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 149.3 on 3079 degrees of freedom\nMultiple R-squared:  0.4181, Adjusted R-squared:  0.4177 \nF-statistic:  1106 on 2 and 3079 DF,  p-value: &lt; 2.2e-16\n\n\n[10 puntos] Estime la misma relación que en la parte a., pero con una regresión mediana. Interprete el coeficiente sobre la antiguedad.\nAhora realizamos un modelo LAD:\n\nsummary(r.q50 &lt;- rq(rent  ~ area + antiguedad,\n                    data=MunichRent,\n                    tau=0.5))\n\n\nCall: rq(formula = rent ~ area + antiguedad, tau = 0.5, data = MunichRent)\n\ntau: [1] 0.5\n\nCoefficients:\n            Value     Std. Error t value   Pr(&gt;|t|) \n(Intercept) 310.17202  11.60978   26.71643   0.00000\narea          4.97688   0.14688   33.88284   0.00000\nantiguedad   -3.02031   0.14599  -20.68795   0.00000\n\n\nLos coeficientes estimados son de una magnitud similar a los de MCO.\n[10 puntos] Estime ahora una regresión cuantil para cada uno de los deciles de la distribución condicional de la renta y represente en una gráfica los coeficientes por regresión cuantil junto con el coeficiente de MCO para las variables del área y la antiguedad. ¿Concluye que vale la pena modelar la relación de las rentas en función del área y la antiguedad usando regresión cuantil?\nRegresión cuantil para cada decil:\n\nr.q1_9 &lt;- rq(rent  ~ area + antiguedad,\n                    data=MunichRent,\n                    tau= 1:9/10)\n\nplot(summary(r.q1_9), parm=c(\"area\",\"antiguedad\"))\n\n\n\n\n\n\n\n\nLos efectos de la antiguedad en la distribución de precios son no lineales. Los efectos en los cuantiles superiores crecen más rápido con la antiguedad. Quizás esto sugiera una preferencia por edificios viejos. La regresión cuantil sí fue útil para revelar esta característica.\n[10 puntos] Suponga que no está dispuesto a imponer una relación lineal entre la antiguedad y la renta. Considere entonces el siguiente modelo:\n\\[rent_i=\\beta_0+\\beta_1 area + \\lambda(antiguedad_i)+\\varepsilon_i\\]\nUse el estimador de Robinson (1988) para estimar este modelo parcialmente lineal. Grafique sus resultados e interprételos.\nSeleccionamos el ancho de banda:\n\nbw &lt;- npplregbw(formula=rent ~ area | antiguedad,\n                data=MunichRent,\n                regtype=\"ll\")\n\nImplementamos el estimador de Robinson:\n\nmodel.pl &lt;- npplreg(bw)\nsummary(model.pl)\n\n\nPartially Linear Model\nRegression data: 3082 training points, in 2 variable(s)\nWith 1 linear parametric regressor(s), 1 nonparametric regressor(s)\n\n                  y(z)\nBandwidth(s): 2.368969\n\n                   x(z)\nBandwidth(s): 0.8672961\n\n                    area\nCoefficient(s): 5.143053\n\nKernel Regression Estimator: Local-Linear\nBandwidth Type: Fixed\n\nResidual standard error: 145.801\nR-squared: 0.4445272\n\nContinuous Kernel Type: Second-Order Gaussian\nNo. Continuous Explanatory Vars.: 1\n\n\nPara obtener el gráfico usamos npplot:\n\ng.robinson &lt;- npplot(bw,\n       perspective=F,\n       plot.errors.method=\"bootstrap\",\n       plot.errors.boot.num=5,\n       plot.behavior=\"plot-data\")\n\n\n\n\n\n\n\ng &lt;- fitted(g.robinson$plr2)\nse &lt;- g.robinson[[\"plr2\"]][[\"merr\"]]\nlci &lt;- g - se[,1]\nuci &lt;- g + se[,2]\n\n#Este objeto nos dicen dónde fueron evaluados\nantiguedad.eval &lt;- g.robinson[[\"plr2\"]][[\"evalz\"]][[\"V1\"]]\n\nfitted &lt;- data.frame(antiguedad.eval, g,lci,uci)\n\nggplot() + \n  geom_point(data=MunichRent, aes(antiguedad,rent), color='black', alpha=0.5) + \n  geom_line(data=fitted, aes(antiguedad.eval, g), linetype='solid')+\n  geom_line(data=fitted, aes(antiguedad.eval, uci), linetype='dashed')+\n  geom_line(data=fitted, aes(antiguedad.eval, lci), linetype='dashed')"
  },
  {
    "objectID": "tareas/tarea-4-respuestas.html#pregunta-3",
    "href": "tareas/tarea-4-respuestas.html#pregunta-3",
    "title": "Respuestas a la tarea 4",
    "section": "",
    "text": "Stevenson & Wolfers (2006) estudian los efectos de la introducción de leyes que permiten el divorcio unilateral. La librería bacondecomp incluye los datos usados en dicho artículo. Usaremos los datos de 1964 a 1996 para mostrar cómo impactan las leyes de divorcio express (unilateral) a la tasa de suicidios en mujeres. Comience llamando los datos:\n::: {.cell}\nwd &lt;- divorce %&gt;% \n  filter(year&gt;1964 & year&lt;1996 & sex==2) %&gt;% \n  mutate(suicide_rate=suicide*1000000/(stpop*fshare),\n         year=as.numeric(year))\n:::\n\n[5 puntos] Estime el efecto de diferencia en diferencias asumiendo tendencias paralelas y el estimador de two-way fixed effects. Obtenga los errores estándar primero asumiendo errores clásicos y luego usando una matriz agrupada a nivel estado.\n\nm.twfe &lt;- felm(suicide_rate ~ unilateral | factor(st) + factor(year),\n                data = wd)\n\nm.twfe.cl &lt;- felm(suicide_rate ~ unilateral | factor(st) + factor(year) | 0 | st,\n                data = wd)\n\nmodelsummary(models = list(\"TWFE\"=m.twfe,\n                           \"TWFE, agrupada\"=m.twfe.cl),\n             coef_map = c('unilateral'),\n          output = 'gt',\n          stars = c('***' = .01, '**' = .05, '*' = 0.1),\n          gof_map = \"nobs\")\n\n\n\n\n\n\n\n\nTWFE\nTWFE, agrupada\n\n\n\n\nunilateral\n-2.824**\n-2.824\n\n\n\n(1.108)\n(2.274)\n\n\nNum.Obs.\n1581\n1581\n\n\n\n* p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n\n\n\n\n\n\n\n\n[5 puntos] Realice la descomposición de Goodman-Bacon (2021). Construya un gráfico donde muestre en el eje el peso otorgado a cada comparación 2x2 que el estimador de TWFE realiza mecánicamente y en el eje el efecto estimado correspondiente a cada comparación. Interprete el gráfico obtenido.\nImplementamos la descomposición:\n\ndf_bacon &lt;- bacon(suicide_rate ~ unilateral,\n                  data = wd,\n                  id_var = \"st\",\n                  time_var = \"year\")\n\n                      type  weight  avg_est\n1 Earlier vs Later Treated 0.10544  1.70904\n2  Later vs Always Treated 0.39872 -6.72231\n3 Later vs Earlier Treated 0.27432  3.31462\n4     Treated vs Untreated 0.22151 -5.56686\n\n\nLa tabla anterior nos da los valores de los estimadores \\(2\\times 2\\) con sus correspondientes pesos. El promedio ponderado es exactamente el estimador de efectos fijos:\n\ncoef_bacon &lt;- sum(df_bacon$estimate * df_bacon$weight)\n\nprint(paste(\"Suma ponderada de la descomposición =\",\n            round(coef_bacon, 4)))\n\n[1] \"Suma ponderada de la descomposición = -2.824\"\n\n\nConstruimos el gráfico:\n\ndf_bacon %&gt;% \n  ggplot(aes(x=weight,\n             y=estimate,\n             shape=type,\n             color=type)) +\n  geom_point() +\n  geom_hline(yintercept = round(m.twfe$coefficients, 4))\n\n\n\n\n\n\n\n\nLas comparaciones que más pesan en el estimador de efectos fijos son las de estados tratados con los que siempre estuvieron tratados en el panel, recibiendo dos de esas comparaciones alrededor de 12.5 y el 6% del peso (los dos triángulos verdes más hacia la derecha). otra comparación que recibe alrededor de 6.5% del peso es la de los tratados con los nunca tratados (cruz morada más hacia la derecha). En total, las comparaciones con estados que iniciaron siendo tratados se llevan el 40% del peso. Las comparaciones entre los tratados tarde y los tratados temprano también reciben un peso alto de 27%.\n[10 puntos] Implemente el estimador de Callaway & Sant’Anna (2021) para estimar los efectos del tratamiento específicos para cada cohorte, usando el paquete did. Utilice como grupo de comparación los estados que nunca son tratados. La columna stid es un identificador numérico de los estados (lo requerirá cuando use att_gt del paquete did).\n\natts_nt &lt;- att_gt(yname = \"suicide_rate\",\n                      tname = \"year\",\n                      idname = \"stid\",\n                      gname = \"divyear\",\n                      data = wd,\n                      control_group = \"nevertreated\",\n                      est_method = 'reg',\n                      bstrap = TRUE,\n                      biters = 1000,\n                      print_details = FALSE,\n                      panel = TRUE)\n\n\nsummary(atts_nt)\n\n\nCall:\natt_gt(yname = \"suicide_rate\", tname = \"year\", idname = \"stid\", \n    gname = \"divyear\", data = wd, panel = TRUE, control_group = \"nevertreated\", \n    bstrap = TRUE, biters = 1000, est_method = \"reg\", print_details = FALSE)\n\nReference: Callaway, Brantly and Pedro H.C. Sant'Anna.  \"Difference-in-Differences with Multiple Time Periods.\" Journal of Econometrics, Vol. 225, No. 2, pp. 200-230, 2021. &lt;https://doi.org/10.1016/j.jeconom.2020.12.001&gt;, &lt;https://arxiv.org/abs/1803.09015&gt; \n\nGroup-Time Average Treatment Effects:\n Group Time ATT(g,t) Std. Error [95% Simult.  Conf. Band]  \n  1969 1966  -3.1857    12.6090      -50.6076     44.2362  \n  1969 1967  12.5043     8.8492      -20.7772     45.7858  \n  1969 1968   3.1310     7.6435      -25.6160     31.8780  \n  1969 1969   1.1566     6.5207      -23.3675     25.6808  \n  1969 1970  -3.5412    13.3245      -53.6543     46.5720  \n  1969 1971  -6.7647    11.7616      -50.9995     37.4702  \n  1969 1972   1.7696    11.2775      -40.6447     44.1838  \n  1969 1973   2.7611     7.6039      -25.8369     31.3591  \n  1969 1974  -0.3084     8.5271      -32.3787     31.7618  \n  1969 1975  -0.7421     5.0161      -19.6075     18.1234  \n  1969 1976 -10.6073     7.5492      -38.9994     17.7848  \n  1969 1977  -3.9320    12.0431      -49.2258     41.3619  \n  1969 1978 -14.0099    11.3649      -56.7528     28.7329  \n  1969 1979  -8.1059    13.5501      -59.0675     42.8556  \n  1969 1980 -10.6943     7.3175      -38.2150     16.8265  \n  1969 1981  -3.4755    12.1039      -48.9980     42.0469  \n  1969 1982  -7.0072     9.0613      -41.0864     27.0721  \n  1969 1983   5.1998    13.3936      -45.1733     55.5728  \n  1969 1984 -10.0771    13.3542      -60.3017     40.1474  \n  1969 1985   5.9598    17.2201      -58.8045     70.7240  \n  1969 1986  -8.5962    12.7169      -56.4240     39.2317  \n  1969 1987  -8.6897     7.7323      -37.7705     20.3911  \n  1969 1988 -11.9815     8.3038      -43.2119     19.2489  \n  1969 1989  -5.9781    15.2884      -63.4774     51.5213  \n  1969 1990  -7.7327    11.1956      -49.8389     34.3735  \n  1969 1991 -14.7659    11.4140      -57.6934     28.1616  \n  1969 1992  -6.6956     6.5026      -31.1515     17.7603  \n  1969 1993   0.5290    12.8492      -47.7963     48.8543  \n  1969 1994  -6.1575    16.2005      -67.0872     54.7721  \n  1969 1995  -6.7870    17.1863      -71.4242     57.8503  \n  1970 1966  -5.7213     9.5945      -41.8061     30.3635  \n  1970 1967  10.0224     8.7761      -22.9842     43.0289  \n  1970 1968  -5.3764     9.3571      -40.5682     29.8154  \n  1970 1969  10.5202     6.3007      -13.1766     34.2169  \n  1970 1970   5.6377     6.5355      -18.9419     30.2174  \n  1970 1971   1.3987     4.8790      -16.9511     19.7485  \n  1970 1972   1.3331     9.6135      -34.8230     37.4891  \n  1970 1973 -13.0254     3.8835      -27.6311      1.5803  \n  1970 1974 -12.7744     4.9304      -31.3173      5.7685  \n  1970 1975 -15.3434     4.8741      -33.6748      2.9880  \n  1970 1976 -19.8605     4.0152      -34.9616     -4.7594 *\n  1970 1977 -19.5551     6.0573      -42.3365      3.2263  \n  1970 1978 -33.3382     5.6337      -54.5265    -12.1499 *\n  1970 1979 -30.5087    13.5303      -81.3955     20.3781  \n  1970 1980 -44.6366     7.4756      -72.7523    -16.5210 *\n  1970 1981 -34.0557    11.3256      -76.6509      8.5395  \n  1970 1982 -38.7875    10.3941      -77.8796      0.3045  \n  1970 1983 -32.9234    17.1234      -97.3240     31.4771  \n  1970 1984 -34.0625    11.3245      -76.6535      8.5286  \n  1970 1985 -30.6346    19.1149     -102.5252     41.2560  \n  1970 1986 -37.0754    18.8397     -107.9311     33.7802  \n  1970 1987 -37.6630    11.4929      -80.8875      5.5615  \n  1970 1988 -43.0563    12.8519      -91.3921      5.2794  \n  1970 1989 -45.1314    14.6881     -100.3729     10.1101  \n  1970 1990 -43.1765     8.6367      -75.6588    -10.6942 *\n  1970 1991 -49.9116     9.3412      -85.0436    -14.7797 *\n  1970 1992 -50.9515    14.7946     -106.5937      4.6906  \n  1970 1993 -44.5526    12.3309      -90.9285      1.8233  \n  1970 1994 -51.5405    17.6950     -118.0909     15.0099  \n  1970 1995 -48.4108    17.3598     -113.7003     16.8788  \n  1971 1966 -12.4375     9.2619      -47.2712     22.3963  \n  1971 1967  17.0967     8.3386      -14.2643     48.4578  \n  1971 1968   2.6186     7.3020      -24.8440     30.0813  \n  1971 1969  -2.1268     5.6673      -23.4413     19.1876  \n  1971 1970   5.7625     6.9692      -20.4485     31.9734  \n  1971 1971  -9.3866     7.2642      -36.7071     17.9338  \n  1971 1972 -13.8393     7.9538      -43.7533     16.0746  \n  1971 1973 -12.4602     7.8160      -41.8558     16.9355  \n  1971 1974   0.1729     5.5446      -20.6803     21.0261  \n  1971 1975  -8.8785     7.9118      -38.6345     20.8775  \n  1971 1976  -7.7923     5.8002      -29.6065     14.0219  \n  1971 1977  -6.3192     8.1699      -37.0460     24.4077  \n  1971 1978 -17.4985    10.5786      -57.2843     22.2872  \n  1971 1979 -16.1999     5.9721      -38.6606      6.2609  \n  1971 1980 -22.5395     5.3023      -42.4812     -2.5978 *\n  1971 1981 -12.5894     8.1475      -43.2319     18.0532  \n  1971 1982 -20.6385    10.2477      -59.1799     17.9028  \n  1971 1983 -11.0888     5.8593      -33.1253     10.9477  \n  1971 1984 -12.7478     6.1768      -35.9787     10.4831  \n  1971 1985  -9.3683     8.7995      -42.4627     23.7262  \n  1971 1986 -16.9260     7.0962      -43.6145      9.7625  \n  1971 1987 -12.9962    10.8822      -53.9237     27.9313  \n  1971 1988 -14.6487     8.1038      -45.1267     15.8292  \n  1971 1989 -18.7126     9.6051      -54.8369     17.4118  \n  1971 1990 -17.6198     6.9975      -43.9371      8.6976  \n  1971 1991 -17.2789     8.8508      -50.5665     16.0088  \n  1971 1992 -22.1825     9.5658      -58.1592     13.7943  \n  1971 1993  -9.1278     9.4625      -44.7159     26.4603  \n  1971 1994 -13.7091     8.6761      -46.3396     18.9214  \n  1971 1995 -15.3270     7.1781      -42.3235     11.6695  \n  1972 1966  -5.2709    10.2345      -43.7627     33.2209  \n  1972 1967   8.3599     9.5882      -27.7009     44.4207  \n  1972 1968   5.3444     7.4435      -22.6504     33.3393  \n  1972 1969  -6.1540     5.3925      -26.4352     14.1272  \n  1972 1970   5.3754     5.4766      -15.2220     25.9729  \n  1972 1971  -0.7708     5.3490      -20.8882     19.3467  \n  1972 1972  -5.3078     4.8366      -23.4980     12.8824  \n  1972 1973  -7.1261     9.2918      -42.0722     27.8199  \n  1972 1974  -4.3635     4.5454      -21.4585     12.7315  \n  1972 1975 -10.7104     7.0424      -37.1965     15.7756  \n  1972 1976  -8.5659     7.7610      -37.7548     20.6229  \n  1972 1977  -1.7264     4.1491      -17.3312     13.8784  \n  1972 1978 -19.5471     7.8113      -48.9251      9.8310  \n  1972 1979  -9.7870    10.2059      -48.1712     28.5972  \n  1972 1980 -16.4149     6.0669      -39.2322      6.4024  \n  1972 1981  -4.6831     4.8897      -23.0731     13.7069  \n  1972 1982  -9.8129     8.9395      -43.4339     23.8082  \n  1972 1983  -5.7570     6.8767      -31.6201     20.1061  \n  1972 1984 -11.1817     5.6138      -32.2949      9.9314  \n  1972 1985  -8.5492     8.7787      -41.5657     24.4672  \n  1972 1986  -3.2909     6.2836      -26.9234     20.3416  \n  1972 1987 -14.5853     7.4985      -42.7867     13.6162  \n  1972 1988 -12.6795     6.3039      -36.3883     11.0293  \n  1972 1989 -10.9845     7.1656      -37.9340     15.9651  \n  1972 1990  -7.7794     7.3159      -35.2943     19.7355  \n  1972 1991 -13.7033     5.5948      -34.7451      7.3385  \n  1972 1992 -11.0100     8.8163      -44.1679     22.1480  \n  1972 1993 -17.3770     7.4747      -45.4890     10.7350  \n  1972 1994 -16.6543     7.9092      -46.4007     13.0920  \n  1972 1995 -16.0626     6.6801      -41.1862      9.0610  \n  1973 1966 -10.0893    10.0711      -47.9663     27.7876  \n  1973 1967  16.0570     9.7318      -20.5439     52.6580  \n  1973 1968  -4.8486     8.5848      -37.1357     27.4385  \n  1973 1969   3.3129     6.0442      -19.4192     26.0449  \n  1973 1970   5.5555     6.4672      -18.7676     29.8786  \n  1973 1971  -2.0683     7.2951      -29.5049     25.3683  \n  1973 1972  -0.9275     6.5936      -25.7258     23.8707  \n  1973 1973   4.4106     8.4384      -27.3258     36.1470  \n  1973 1974   3.2944     6.1162      -19.7086     26.2973  \n  1973 1975   0.0725    11.5704      -43.4432     43.5883  \n  1973 1976  -1.1180     6.2547      -24.6418     22.4059  \n  1973 1977   2.7203     5.8664      -19.3431     24.7837  \n  1973 1978 -10.3862     8.7687      -43.3649     22.5924  \n  1973 1979  -3.4722     7.2328      -30.6746     23.7303  \n  1973 1980 -11.7465     7.7766      -40.9940     17.5009  \n  1973 1981  -3.1322     7.4260      -31.0614     24.7969  \n  1973 1982  -7.8264    10.3198      -46.6387     30.9859  \n  1973 1983  -4.3281     5.9254      -26.6134     17.9572  \n  1973 1984 -10.3847     6.4512      -34.6474     13.8780  \n  1973 1985  -3.3503     7.6448      -32.1021     25.4014  \n  1973 1986  -9.9416     4.6451      -27.4118      7.5286  \n  1973 1987 -10.5611     8.6734      -43.1813     22.0591  \n  1973 1988 -13.3770     9.0506      -47.4158     20.6619  \n  1973 1989  -9.7072     6.9669      -35.9096     16.4952  \n  1973 1990 -12.5464     6.0032      -35.1243     10.0314  \n  1973 1991 -15.9396     7.1237      -42.7317     10.8526  \n  1973 1992 -17.9985     6.9073      -43.9765      7.9796  \n  1973 1993 -13.8426     7.8503      -43.3673     15.6821  \n  1973 1994  -9.0985     6.0048      -31.6824     13.4855  \n  1973 1995 -12.4104     4.0223      -27.5381      2.7173  \n  1974 1966  -3.7313    10.2818      -42.4010     34.9383  \n  1974 1967   9.6967     8.4315      -22.0140     41.4075  \n  1974 1968  -5.6736     8.2589      -36.7351     25.3879  \n  1974 1969   4.7497     5.0947      -14.4114     23.9108  \n  1974 1970   8.9528     5.5731      -12.0073     29.9129  \n  1974 1971  -8.1562     5.5379      -28.9842     12.6718  \n  1974 1972   5.1631     4.2741      -10.9115     21.2378  \n  1974 1973  -4.1358     7.3372      -31.7309     23.4592  \n  1974 1974  -1.5277     4.6118      -18.8727     15.8173  \n  1974 1975  -0.2490     4.8780      -18.5948     18.0968  \n  1974 1976  -4.6623     3.9210      -19.4090     10.0844  \n  1974 1977  -3.6545     7.2230      -30.8199     23.5110  \n  1974 1978  -9.4996     5.6922      -30.9080     11.9087  \n  1974 1979  -1.7953     9.8989      -39.0246     35.4340  \n  1974 1980 -10.9157     5.3459      -31.0215      9.1901  \n  1974 1981  -2.9438     7.4216      -30.8561     24.9686  \n  1974 1982  -6.1358     5.0386      -25.0859     12.8143  \n  1974 1983  -2.2347     7.6185      -30.8878     26.4184  \n  1974 1984  -7.2784     6.5728      -31.9986     17.4418  \n  1974 1985   0.9048    10.7007      -39.3403     41.1500  \n  1974 1986  -3.4953     8.9151      -37.0248     30.0342  \n  1974 1987  -9.3045     8.3304      -40.6350     22.0259  \n  1974 1988  -9.0434     8.4419      -40.7932     22.7063  \n  1974 1989  -6.4758     9.6869      -42.9080     29.9564  \n  1974 1990  -7.6369     8.1137      -38.1521     22.8783  \n  1974 1991 -14.7133     7.9173      -44.4900     15.0634  \n  1974 1992 -14.7711     7.9707      -44.7486     15.2064  \n  1974 1993 -11.2274     8.5024      -43.2045     20.7497  \n  1974 1994 -14.4350    10.7449      -54.8461     25.9761  \n  1974 1995 -13.4194     9.6698      -49.7871     22.9484  \n  1975 1966 -10.7092     9.4775      -46.3536     24.9352  \n  1975 1967   9.6526     8.5008      -22.3186     41.6237  \n  1975 1968   5.2146     7.9016      -24.5031     34.9324  \n  1975 1969  -2.1343     8.3120      -33.3954     29.1269  \n  1975 1970  -5.6035     9.4988      -41.3283     30.1214  \n  1975 1971   3.4917     9.6360      -32.7492     39.7325  \n  1975 1972 -15.0108     8.0545      -45.3036     15.2820  \n  1975 1973   9.7321     7.1463      -17.1450     36.6091  \n  1975 1974   0.5633     5.8727      -21.5236     22.6502  \n  1975 1975  -4.9361     5.2822      -24.8023     14.9300  \n  1975 1976  -1.1020     3.9490      -15.9540     13.7501  \n  1975 1977  -4.3766    10.2412      -42.8934     34.1401  \n  1975 1978  -8.3191     7.6221      -36.9856     20.3474  \n  1975 1979 -12.7262    16.1052      -73.2973     47.8450  \n  1975 1980 -12.3662     5.2244      -32.0149      7.2826  \n  1975 1981  -7.7023     6.5726      -32.4217     17.0171  \n  1975 1982  -9.7198     9.3131      -44.7463     25.3066  \n  1975 1983  -7.3926     6.3929      -31.4359     16.6507  \n  1975 1984   8.8507     9.4537      -26.7045     44.4058  \n  1975 1985  -5.6163     8.3174      -36.8977     25.6651  \n  1975 1986  -5.2183     4.7325      -23.0171     12.5805  \n  1975 1987  -0.8638     5.2844      -20.7382     19.0107  \n  1975 1988 -15.3668    14.3175      -69.2143     38.4808  \n  1975 1989  -5.4933     5.8675      -27.5609     16.5742  \n  1975 1990   8.5067     7.9168      -21.2680     38.2813  \n  1975 1991   0.7631     5.9418      -21.5839     23.1101  \n  1975 1992  -4.2255     6.0728      -27.0650     18.6140  \n  1975 1993   1.1200     2.5499       -8.4702     10.7101  \n  1975 1994  -8.3655     7.6252      -37.0437     20.3128  \n  1975 1995   1.8041     3.8506      -12.6781     16.2863  \n  1976 1966  -7.3839     9.4775      -43.0283     28.2605  \n  1976 1967   6.1953     8.5008      -25.7758     38.1665  \n  1976 1968  -3.4100     6.8387      -29.1299     22.3100  \n  1976 1969  -4.5734     4.4373      -21.2619     12.1150  \n  1976 1970  13.6356     4.7051       -4.0601     31.3313  \n  1976 1971   2.1590     5.5379      -18.6690     22.9870  \n  1976 1972  -1.4512     4.3507      -17.8142     14.9117  \n  1976 1973 -23.5440     6.8621      -49.3521      2.2641  \n  1976 1974  37.6970     4.2540       21.6977     53.6963 *\n  1976 1975  -5.8620     5.1960      -25.4041     13.6801  \n  1976 1976   7.3375     6.7834      -18.1745     32.8496  \n  1976 1977  35.4928     7.3478        7.8581     63.1274 *\n  1976 1978  -2.2158     3.3728      -14.9006     10.4690  \n  1976 1979   2.7599    11.1105      -39.0265     44.5463  \n  1976 1980 -10.1277     5.1657      -29.5559      9.3004  \n  1976 1981  -9.1249     8.4453      -40.8873     22.6375  \n  1976 1982  -9.7529     5.1867      -29.2600      9.7541  \n  1976 1983 -12.2793     7.8075      -41.6431     17.0844  \n  1976 1984 -20.1148     4.7838      -38.1066     -2.1229 *\n  1976 1985  -0.2053    11.6139      -43.8848     43.4742  \n  1976 1986 -27.7992     8.3181      -59.0832      3.4848  \n  1976 1987  -9.9985     3.6973      -23.9038      3.9069  \n  1976 1988 -22.8540     5.8685      -44.9254     -0.7826 *\n  1976 1989 -14.3020     6.6385      -39.2692     10.6652  \n  1976 1990 -16.7275     4.8656      -35.0267      1.5717  \n  1976 1991 -29.9838     5.5425      -50.8290     -9.1387 *\n  1976 1992 -35.9431     6.2168      -59.3241    -12.5620 *\n  1976 1993 -33.5630     6.2298      -56.9932    -10.1328 *\n  1976 1994 -19.1785    10.5829      -58.9804     20.6233  \n  1976 1995 -18.8943     9.0197      -52.8170     15.0284  \n  1977 1966 -17.8103    11.1251      -59.6516     24.0310  \n  1977 1967  22.0500     9.1659      -12.4225     56.5225  \n  1977 1968 -12.0551    13.0853      -61.2684     37.1582  \n  1977 1969  11.3299    12.0132      -33.8514     56.5113  \n  1977 1970   7.9543     7.6371      -20.7685     36.6771  \n  1977 1971   0.9514     9.1138      -33.3252     35.2281  \n  1977 1972  -1.0798     9.5351      -36.9411     34.7816  \n  1977 1973  -2.0153     9.3763      -37.2792     33.2485  \n  1977 1974 -10.4409     5.7751      -32.1610     11.2792  \n  1977 1975  -2.6782     7.1322      -29.5022     24.1458  \n  1977 1976   7.5869    18.0939      -60.4636     75.6373  \n  1977 1977  -0.3083    15.2383      -57.6192     57.0026  \n  1977 1978 -18.5991    18.2767      -87.3370     50.1389  \n  1977 1979  -8.3001    28.5726     -115.7608     99.1606  \n  1977 1980 -13.4381    23.5917     -102.1657     75.2895  \n  1977 1981 -12.1467    21.1072      -91.5302     67.2367  \n  1977 1982 -17.4639    16.5230      -79.6064     44.6786  \n  1977 1983   6.1362    30.8151     -109.7585    122.0309  \n  1977 1984  -2.9191    24.6110      -95.4801     89.6419  \n  1977 1985 -14.8995    20.0884      -90.4513     60.6523  \n  1977 1986 -12.3230    14.6294      -67.3438     42.6979  \n  1977 1987 -23.6769    27.9188     -128.6785     81.3247  \n  1977 1988 -25.5547    22.3801     -109.7256     58.6162  \n  1977 1989  -9.2602    30.0056     -122.1103    103.5898  \n  1977 1990 -13.7369    29.7268     -125.5385     98.0647  \n  1977 1991 -25.9731    24.8484     -119.4270     67.4807  \n  1977 1992 -29.9220    23.7594     -119.2802     59.4362  \n  1977 1993 -14.9531    27.8362     -119.6441     89.7380  \n  1977 1994 -11.6033    35.5769     -145.4068    122.2001  \n  1977 1995 -29.0098    21.5447     -110.0389     52.0193  \n  1980 1966  -9.4644     9.4775      -45.1088     26.1799  \n  1980 1967  12.2092     8.5008      -19.7619     44.1804  \n  1980 1968   3.0528     6.8387      -22.6672     28.7727  \n  1980 1969  -3.6543     4.4373      -20.3428     13.0341  \n  1980 1970  12.7725     4.7051       -4.9232     30.4682  \n  1980 1971 -11.0126     5.5379      -31.8406      9.8154  \n  1980 1972   5.2094     4.3507      -11.1536     21.5723  \n  1980 1973  -8.0378     6.8621      -33.8459     17.7703  \n  1980 1974   2.0132     4.2540      -13.9861     18.0125  \n  1980 1975  -3.8913     5.1960      -23.4335     15.6508  \n  1980 1976   4.9514     6.7834      -20.5607     30.4634  \n  1980 1977  -1.8244     3.6248      -15.4572     11.8085  \n  1980 1978  -6.3410     5.6068      -27.4279     14.7460  \n  1980 1979   5.7875    10.6480      -34.2594     45.8344  \n  1980 1980 -12.0465     7.0875      -38.7025     14.6095  \n  1980 1981  -7.2766     3.1780      -19.2288      4.6756  \n  1980 1982  -7.7888    11.4064      -50.6877     35.1102  \n  1980 1983   3.8986     3.7649      -10.2611     18.0583  \n  1980 1984  -2.3172     5.8716      -24.4002     19.7657  \n  1980 1985   4.4467     4.7848      -13.5490     22.4423  \n  1980 1986  -9.5981     7.0565      -36.1373     16.9410  \n  1980 1987 -10.0537    11.0181      -51.4923     31.3849  \n  1980 1988 -10.9789     9.1300      -45.3164     23.3587  \n  1980 1989  -6.9527     6.3603      -30.8735     16.9681  \n  1980 1990  -5.3312     6.2450      -28.8183     18.1560  \n  1980 1991  -8.8820     7.9543      -38.7981     21.0340  \n  1980 1992 -12.0023     8.8737      -45.3760     21.3714  \n  1980 1993  -8.3192     7.7315      -37.3969     20.7586  \n  1980 1994 -12.5474     7.1011      -39.2546     14.1597  \n  1980 1995  -6.7308     9.4076      -42.1124     28.6508  \n  1984 1966  -9.0520     9.4775      -44.6964     26.5924  \n  1984 1967  10.1591     8.5008      -21.8120     42.1303  \n  1984 1968  -3.1247     6.8387      -28.8447     22.5952  \n  1984 1969   4.0091     4.4373      -12.6793     20.6976  \n  1984 1970   4.9470     4.7051      -12.7487     22.6428  \n  1984 1971 -12.3840     5.5379      -33.2120      8.4440  \n  1984 1972   5.1482     4.3507      -11.2147     21.5112  \n  1984 1973  -6.4124     6.8621      -32.2205     19.3957  \n  1984 1974  -5.7358     4.2540      -21.7351     10.2635  \n  1984 1975  -4.0919     5.1960      -23.6340     15.4503  \n  1984 1976   8.7209     6.7834      -16.7911     34.2329  \n  1984 1977   2.6679     3.6248      -10.9650     16.3007  \n  1984 1978  -7.3955     5.6068      -28.4825     13.6914  \n  1984 1979   2.2253    10.6480      -37.8217     42.2722  \n  1984 1980  -8.3661     7.0875      -35.0220     18.2899  \n  1984 1981  10.0008     5.6947      -11.4168     31.4184  \n  1984 1982  -3.2324    10.2165      -41.6562     35.1914  \n  1984 1983  11.0979    10.1458      -27.0603     49.2560  \n  1984 1984  -1.6979     3.0237      -13.0697      9.6740  \n  1984 1985   4.0744     3.8064      -10.2414     18.3901  \n  1984 1986  -8.0341     2.9094      -18.9763      2.9081  \n  1984 1987 -10.4189     7.1120      -37.1667     16.3289  \n  1984 1988  -7.8277     5.4597      -28.3613     12.7059  \n  1984 1989  -9.2894     3.4826      -22.3874      3.8086  \n  1984 1990  -6.2529     3.0974      -17.9022      5.3963  \n  1984 1991 -10.1584     4.9176      -28.6534      8.3366  \n  1984 1992 -11.4694     4.6630      -29.0068      6.0680  \n  1984 1993  -9.2844     4.4284      -25.9395      7.3707  \n  1984 1994 -14.6630     3.4313      -27.5682     -1.7578 *\n  1984 1995 -12.5044     3.3329      -25.0393      0.0304  \n  1985 1966  29.3180     9.4775       -6.3263     64.9624  \n  1985 1967  -6.8558     8.5008      -38.8270     25.1153  \n  1985 1968  -5.9545     6.8387      -31.6745     19.7654  \n  1985 1969  12.1675     4.4373       -4.5209     28.8560  \n  1985 1970  12.3074     4.7051       -5.3883     30.0031  \n  1985 1971   4.9424     5.5379      -15.8856     25.7704  \n  1985 1972   5.1718     4.3507      -11.1911     21.5348  \n  1985 1973 -41.2443     6.8621      -67.0524    -15.4362 *\n  1985 1974   9.0064     4.2540       -6.9928     25.0057  \n  1985 1975  -9.1424     5.1960      -28.6845     10.3998  \n  1985 1976   2.8125     6.7834      -22.6995     28.3245  \n  1985 1977  -2.1400     3.6248      -15.7728     11.4929  \n  1985 1978   5.7781     5.6068      -15.3088     26.8651  \n  1985 1979  -3.8368    10.6480      -43.8837     36.2102  \n  1985 1980   1.5265     7.0875      -25.1295     28.1825  \n  1985 1981  -2.8105     5.6947      -24.2281     18.6071  \n  1985 1982  13.2242    10.2165      -25.1996     51.6479  \n  1985 1983  -5.5714    10.1458      -43.7295     32.5867  \n  1985 1984  -5.5493     3.0237      -16.9212      5.8225  \n  1985 1985  11.4728     6.8301      -14.2148     37.1605  \n  1985 1986  10.1715     3.4372       -2.7558     23.0989  \n  1985 1987  17.5291     4.9075       -0.9279     35.9861  \n  1985 1988  -9.6423     3.9206      -24.3875      5.1030  \n  1985 1989  19.8229     5.3527       -0.3085     39.9542  \n  1985 1990  26.2350     0.9371       22.7106     29.7594 *\n  1985 1991   6.2209     3.7879       -8.0253     20.4671  \n  1985 1992  18.3602     6.2606       -5.1856     41.9059  \n  1985 1993  23.0343     2.8095       12.4679     33.6008 *\n  1985 1994  15.2612     6.2622       -8.2907     38.8131  \n  1985 1995  15.4633     4.4738       -1.3625     32.2892  \n---\nSignif. codes: `*' confidence band does not cover 0\n\nControl Group:  Never Treated,  Anticipation Periods:  0\nEstimation Method:  Outcome Regression\n\nggdid(atts_nt)\n\n\n\n\n\n\n\n\n[10 puntos] Reporte los resultados agregados obtenidos a partir del estimador Callaway & Sant’Anna (2021), usando una agregación dinámica que muestre los efectos promedio para cada periodo antes y después del tratamiento. Grafique e interprete los resultados.\n\nagg.es &lt;- aggte(atts_nt,\n                type = \"dynamic\")\n\nsummary(agg.es)\n\n\nCall:\naggte(MP = atts_nt, type = \"dynamic\")\n\nReference: Callaway, Brantly and Pedro H.C. Sant'Anna.  \"Difference-in-Differences with Multiple Time Periods.\" Journal of Econometrics, Vol. 225, No. 2, pp. 200-230, 2021. &lt;https://doi.org/10.1016/j.jeconom.2020.12.001&gt;, &lt;https://arxiv.org/abs/1803.09015&gt; \n\n\nOverall summary of ATT's based on event-study/dynamic aggregation:  \n      ATT    Std. Error     [ 95%  Conf. Int.]  \n -11.4964        3.7288   -18.8047     -4.1881 *\n\n\nDynamic Effects:\n Event time Estimate Std. Error [95% Simult.  Conf. Band]  \n        -19  29.3180    10.1469        1.3095     57.3266 *\n        -18  -7.9539     1.8476      -13.0538     -2.8540 *\n        -17   2.1023     5.9725      -14.3837     18.5883  \n        -16   4.5214     5.6877      -11.1785     20.2213  \n        -15   8.1583     5.9371       -8.2299     24.5465  \n        -14   0.1417     4.8451      -13.2322     13.5156  \n        -13   1.6657     7.2660      -18.3906     21.7220  \n        -12 -11.0144    14.9396      -52.2524     30.2235  \n        -11  -9.0818     7.5219      -29.8445     11.6808  \n        -10   8.0943     6.4728       -9.7727     25.9614  \n         -9  -7.0756     5.2780      -21.6446      7.4934  \n         -8   4.2068     4.1602       -7.2766     15.6902  \n         -7  -2.2507     4.5544      -14.8223     10.3209  \n         -6   5.6460     3.7781       -4.7828     16.0748  \n         -5  -3.4392     3.4505      -12.9635      6.0851  \n         -4   5.1665     2.6344       -2.1052     12.4383  \n         -3  -0.3596     2.1774       -6.3699      5.6508  \n         -2   1.1409     2.5639       -5.9363      8.2182  \n         -1   1.9481     3.1449       -6.7330     10.6291  \n          0  -0.8065     2.8923       -8.7902      7.1773  \n          1  -2.7726     3.0902      -11.3025      5.7573  \n          2  -4.2845     4.3780      -16.3691      7.8001  \n          3  -3.9743     2.8756      -11.9117      3.9631  \n          4  -4.5620     3.5778      -14.4376      5.3137  \n          5  -7.4465     3.1521      -16.1472      1.2542  \n          6  -6.2541     3.9075      -17.0399      4.5317  \n          7 -10.6094     3.5133      -20.3071     -0.9117 *\n          8  -9.9746     3.5535      -19.7834     -0.1658 *\n          9 -10.1110     3.7334      -20.4164      0.1944  \n         10 -11.0023     3.9502      -21.9060     -0.0987 *\n         11 -13.3679     3.8009      -23.8595     -2.8763 *\n         12  -8.7828     4.8173      -22.0800      4.5144  \n         13 -12.1880     4.0827      -23.4575     -0.9185 *\n         14 -11.2760     4.5907      -23.9478      1.3958  \n         15 -14.8487     5.0057      -28.6658     -1.0315 *\n         16 -11.7709     5.1512      -25.9897      2.4479  \n         17 -14.3232     4.7228      -27.3595     -1.2869 *\n         18 -17.1010     4.6834      -30.0288     -4.1733 *\n         19 -17.6118     4.2504      -29.3443     -5.8793 *\n         20 -14.8085     5.0632      -28.7847     -0.8324 *\n         21 -16.5371     4.7367      -29.6117     -3.4624 *\n         22 -15.2723     4.6988      -28.2424     -2.3022 *\n         23 -17.6177     7.3821      -37.9947      2.7593  \n         24 -19.0284     8.8203      -43.3752      5.3184  \n         25 -27.2842    19.0460      -79.8569     25.2886  \n         26  -6.7870    16.2988      -51.7766     38.2026  \n---\nSignif. codes: `*' confidence band does not cover 0\n\nControl Group:  Never Treated,  Anticipation Periods:  0\nEstimation Method:  Outcome Regression\n\nggdid(agg.es)\n\n\n\n\n\n\n\n\nSe obtiene una reducción en la tasa de suicidios que es estadísticamente significativa a partir de 5 años después de la introducción de la legislación."
  }
]